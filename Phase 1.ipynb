{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0240e2f",
   "metadata": {},
   "source": [
    "# ==============================================================================\n",
    "# Phase 1: Data Preparation\n",
    "# This script handles loading, preprocessing, and exporting gene expression\n",
    "# and phenotype data.\n",
    "#\n",
    "# Before running:\n",
    "# 1. Ensure you have pandas and numpy installed: pip install pandas numpy\n",
    "# 2. Update the 'raw_expr_file' and 'raw_pheno_file' paths below\n",
    "#    to point to your actual raw data files.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30476386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_data(expr_path, pheno_path):\n",
    "    \"\"\"\n",
    "    Loads gene expression and phenotype data from specified file paths.\n",
    "\n",
    "    Args:\n",
    "        expr_path (str): Path to the gene expression data file.\n",
    "        pheno_path (str): Path to the phenotype data file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - expr (pd.DataFrame): Gene expression DataFrame (genes x samples).\n",
    "            - pheno (pd.DataFrame): Phenotype DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Data Preparation: Loading Data ---\")\n",
    "    print(f\"Loading gene expression data from: {expr_path}\")\n",
    "    expr = pd.read_csv(expr_path, sep=\"\\t\", index_col=0)\n",
    "    print(f\"Gene expression data shape: {expr.shape}\")\n",
    "\n",
    "    print(f\"Loading phenotype data from: {pheno_path}\")\n",
    "    pheno = pd.read_csv(pheno_path, sep=\"\\t\", index_col=0)\n",
    "    print(f\"Phenotype data shape: {pheno.shape}\")\n",
    "\n",
    "    return expr, pheno\n",
    "\n",
    "def preprocess_data(expr, pheno):\n",
    "    \"\"\"\n",
    "    Transposes gene expression data and matches samples with phenotype data.\n",
    "\n",
    "    Args:\n",
    "        expr (pd.DataFrame): Gene expression DataFrame (genes x samples).\n",
    "        pheno (pd.DataFrame): Phenotype DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - expr_T (pd.DataFrame): Transposed gene expression DataFrame (samples x genes).\n",
    "            - pheno_matched (pd.DataFrame): Matched phenotype DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Data Preparation: Preprocessing Data ---\")\n",
    "    # Transpose gene expression data to (samples x genes)\n",
    "    print(\"Transposing gene expression data...\")\n",
    "    expr_T = expr.T\n",
    "    expr_T.index.name = 'sample_id'\n",
    "    print(f\"Transposed gene expression data shape: {expr_T.shape}\")\n",
    "\n",
    "    # Match samples in both files\n",
    "    print(\"Matching samples between expression and phenotype data...\")\n",
    "    common_samples = expr_T.index.intersection(pheno.index)\n",
    "    expr_matched = expr_T.loc[common_samples]\n",
    "    pheno_matched = pheno.loc[common_samples]\n",
    "    print(f\"Number of matched samples: {len(common_samples)}\")\n",
    "    print(f\"Matched expression data shape: {expr_matched.shape}\")\n",
    "    print(f\"Matched phenotype data shape: {pheno_matched.shape}\")\n",
    "\n",
    "    return expr_matched, pheno_matched\n",
    "\n",
    "def export_data(dataframe, output_path, file_format='csv'):\n",
    "    \"\"\"\n",
    "    Exports a DataFrame to a specified file format.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The DataFrame to export.\n",
    "        output_path (str): The path to save the exported file.\n",
    "        file_format (str): The format to save the file ('csv', 'tsv', 'parquet', 'excel').\n",
    "    \"\"\"\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Created output directory: {output_dir}\")\n",
    "\n",
    "    if file_format == 'csv':\n",
    "        dataframe.to_csv(output_path, index=True)\n",
    "    elif file_format == 'tsv':\n",
    "        dataframe.to_csv(output_path, sep='\\t', index=True)\n",
    "    elif file_format == 'parquet':\n",
    "        dataframe.to_parquet(output_path, index=True)\n",
    "    elif file_format == 'excel':\n",
    "        dataframe.to_excel(output_path, index=True)\n",
    "    else:\n",
    "        print(f\"Unsupported file format: {file_format}. Supported formats are 'csv', 'tsv', 'parquet', 'excel'.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Data successfully exported to: {output_path} in {file_format} format.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Main Execution Block for Data Preparation\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Data Preparation Phase...\")\n",
    "\n",
    "    # --- Configuration ---\n",
    "    # IMPORTANT: Update these paths to your actual raw data files\n",
    "    raw_expr_file = r\"C:\\Users\\shrav\\Desktop\\PYTHON\\Cancer\\Pan Cancer Analysis\\EB++AdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.xena\"\n",
    "    raw_pheno_file = r\"C:\\Users\\shrav\\Desktop\\PYTHON\\Cancer\\Pan Cancer Analysis\\TCGA_phenotype_denseDataOnlyDownload.tsv\"\n",
    "    \n",
    "    # Output directory for processed data\n",
    "    processed_data_dir = \"processed_data\"\n",
    "    os.makedirs(processed_data_dir, exist_ok=True) # Ensure output directory exists\n",
    "\n",
    "    # Define paths for processed files\n",
    "    processed_expr_file = os.path.join(processed_data_dir, \"expr_processed.tsv\")\n",
    "    processed_pheno_file = os.path.join(processed_data_dir, \"pheno_processed.tsv\")\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(raw_expr_file) and os.path.exists(raw_pheno_file):\n",
    "            expr_raw, pheno_raw = load_data(raw_expr_file, raw_pheno_file)\n",
    "            expr_processed, pheno_processed = preprocess_data(expr_raw, pheno_raw)\n",
    "            \n",
    "            export_data(expr_processed, processed_expr_file, file_format='tsv')\n",
    "            export_data(pheno_processed, processed_pheno_file, file_format='tsv')\n",
    "        else:\n",
    "            print(f\"Raw data files not found at '{raw_expr_file}' or '{raw_pheno_file}'.\")\n",
    "            print(\"Generating dummy data for demonstration purposes.\")\n",
    "            np.random.seed(42)\n",
    "            num_samples = 100\n",
    "            num_genes = 500\n",
    "            genes = [f'Gene_{i}' for i in range(num_genes)]\n",
    "            samples = [f'Sample_{i}' for i in range(num_samples)]\n",
    "            \n",
    "            expr_processed = pd.DataFrame(np.random.rand(num_samples, num_genes), index=samples, columns=genes)\n",
    "            tumor_types = ['BRCA', 'LUAD', 'COAD', 'KIRC', 'LIHC']\n",
    "            pheno_processed = pd.DataFrame({\n",
    "                '_primary_site': np.random.choice(tumor_types, num_samples),\n",
    "                'age_at_diagnosis': np.random.randint(30, 80, num_samples)\n",
    "            }, index=samples)\n",
    "            \n",
    "            export_data(expr_processed, processed_expr_file, file_format='tsv')\n",
    "            export_data(pheno_processed, processed_pheno_file, file_format='tsv')\n",
    "            print(\"Dummy processed data saved to 'processed_data' directory.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Data Preparation: {e}\")\n",
    "        print(\"Please check your file paths and data format. Exiting.\")\n",
    "\n",
    "    print(\"\\nData Preparation Phase complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f6d3a6",
   "metadata": {},
   "source": [
    "# ==============================================================================\n",
    "# Phase 1.1: EDA & Visualization (Enhanced for Publication/Sharing)\n",
    "# This script performs exploratory data analysis, dimensionality reduction\n",
    "# (PCA, UMAP), and generates various plots with improved aesthetics for\n",
    "# sharing on platforms like LinkedIn and blogs.\n",
    "#\n",
    "# Before running:\n",
    "# 1. Ensure you have pandas, numpy, matplotlib, seaborn, scikit-learn,\n",
    "#    and umap-learn installed: pip install pandas numpy matplotlib seaborn scikit-learn umap-learn\n",
    "# 2. This script assumes 'processed_data/expr_processed.tsv' and\n",
    "#    'processed_data/pheno_processed.tsv' exist from Phase 1.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edbc270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import umap.umap_ as umap\n",
    "import os\n",
    "\n",
    "# Set a consistent style for all plots for a professional look\n",
    "# 'seaborn-v0_8-darkgrid' is a good default for clean, readable plots\n",
    "# You can experiment with other styles like 'ggplot', 'seaborn-v0_8-whitegrid', etc.\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "def generate_summary_statistics(dataframe, name=\"Data\"):\n",
    "    \"\"\"\n",
    "    Generates and prints summary statistics for a given DataFrame.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The DataFrame for which to generate statistics.\n",
    "        name (str): A descriptive name for the DataFrame (e.g., \"Gene Expression\", \"Phenotype\").\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- EDA: Summary Statistics for {name} ---\")\n",
    "    print(dataframe.describe())\n",
    "    print(f\"\\nMissing values in {name}:\\n{dataframe.isnull().sum().sum()} total missing values.\")\n",
    "    if dataframe.isnull().sum().sum() > 0:\n",
    "        print(f\"Missing values per column:\\n{dataframe.isnull().sum()[dataframe.isnull().sum() > 0]}\")\n",
    "    print(f\"\\nDataFrame Info for {name}:\")\n",
    "    dataframe.info()\n",
    "    print(\"-\" * (25 + len(name)))\n",
    "\n",
    "def plot_tumor_type_distribution(phenotype_df, tumor_type_column='_primary_disease', output_path=None):\n",
    "    \"\"\"\n",
    "    Plots the distribution of tumor types from the phenotype DataFrame.\n",
    "    Enhanced for readability and aesthetics for sharing.\n",
    "\n",
    "    Args:\n",
    "        phenotype_df (pd.DataFrame): The phenotype DataFrame.\n",
    "        tumor_type_column (str): The name of the column containing tumor type information.\n",
    "        output_path (str, optional): Path to save the plot. If None, displays the plot.\n",
    "    \"\"\"\n",
    "    if tumor_type_column not in phenotype_df.columns:\n",
    "        print(f\"Error: '{tumor_type_column}' not found in phenotype DataFrame. Cannot plot tumor type distribution.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- EDA: Plotting Tumor Type Distribution ---\")\n",
    "    plt.figure(figsize=(14, 8)) # Slightly larger figure size\n",
    "    \n",
    "    # Use a vibrant and distinct palette, as requested (e.g., 'mako' or 'Spectral')\n",
    "    # Using 'mako' as per user's snippet\n",
    "    tumor_counts = phenotype_df[tumor_type_column].value_counts().sort_values(ascending=False)\n",
    "    sns.barplot(y=tumor_counts.index, x=tumor_counts.values, palette='mako')\n",
    "    \n",
    "    plt.title(f'Distribution of {tumor_type_column.replace(\"_\", \" \").title()} Across Samples', fontsize=18, weight='bold', color='darkblue') # Larger, bold title\n",
    "    plt.xlabel('Number of Samples', fontsize=14, color='dimgray') # Larger label\n",
    "    plt.ylabel(tumor_type_column.replace(\"_\", \" \").title(), fontsize=14, color='dimgray') # Larger label\n",
    "    plt.xticks(fontsize=12) # Larger tick labels\n",
    "    plt.yticks(fontsize=12) # Larger tick labels\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.6, color='lightgray') # Subtler grid\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_path:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            print(f\"Created output directory: {output_dir}\")\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight') # High DPI, ensure tight bounding box\n",
    "        print(f\"Tumor type distribution plot saved to: {output_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_expression_summary_histograms(expr_df, output_path=None):\n",
    "    \"\"\"\n",
    "    Plots histograms of mean and standard deviation of gene expression.\n",
    "    Enhanced for readability and aesthetics for sharing.\n",
    "\n",
    "    Args:\n",
    "        expr_df (pd.DataFrame): Transposed gene expression DataFrame (samples x genes).\n",
    "        output_path (str, optional): Base path to save the plots.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- EDA: Plotting Gene Expression Summary Histograms ---\")\n",
    "    summary_stats = expr_df.describe().T\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6)) # Two subplots for mean and std\n",
    "    \n",
    "    # Plot Mean Distribution\n",
    "    sns.histplot(summary_stats['mean'], bins=50, kde=True, ax=axes[0], color='skyblue', edgecolor='black')\n",
    "    axes[0].set_title('Distribution of Gene Expression Mean', fontsize=16, weight='bold', color='darkblue')\n",
    "    axes[0].set_xlabel('Mean Expression', fontsize=12, color='dimgray')\n",
    "    axes[0].set_ylabel('Number of Genes', fontsize=12, color='dimgray')\n",
    "    axes[0].tick_params(labelsize=10)\n",
    "    axes[0].grid(axis='y', linestyle='--', alpha=0.6, color='lightgray')\n",
    "\n",
    "    # Plot Standard Deviation Distribution\n",
    "    sns.histplot(summary_stats['std'], bins=50, kde=True, ax=axes[1], color='salmon', edgecolor='black')\n",
    "    axes[1].set_title('Distribution of Gene Expression Standard Deviation', fontsize=16, weight='bold', color='darkblue')\n",
    "    axes[1].set_xlabel('Standard Deviation of Expression', fontsize=12, color='dimgray')\n",
    "    axes[1].set_ylabel('Number of Genes', fontsize=12, color='dimgray')\n",
    "    axes[1].tick_params(labelsize=10)\n",
    "    axes[1].grid(axis='y', linestyle='--', alpha=0.6, color='lightgray')\n",
    "\n",
    "    plt.suptitle(\"Distribution of Gene Expression Mean and Standard Deviation Across All Genes\", fontsize=20, weight='bold', color='black', y=1.02)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.98]) # Adjust layout for suptitle\n",
    "    \n",
    "    if output_path:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Expression summary histograms saved to: {output_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def perform_pca(expr_df, n_components=2):\n",
    "    \"\"\"\n",
    "    Performs Principal Component Analysis (PCA) on the gene expression data.\n",
    "\n",
    "    Args:\n",
    "        expr_df (pd.DataFrame): Transposed gene expression DataFrame (samples x genes).\n",
    "        n_components (int): Number of principal components to compute.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - pca_result (pd.DataFrame): DataFrame with PCA components.\n",
    "            - pca_model (PCA): The fitted PCA model.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- EDA: Performing PCA with {n_components} components ---\")\n",
    "    expr_df_filled = expr_df.fillna(expr_df.mean()) # Safeguard for NaNs\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    principal_components = pca.fit_transform(expr_df_filled)\n",
    "    pca_result = pd.DataFrame(data=principal_components,\n",
    "                              columns=[f'PC{i+1}' for i in range(n_components)],\n",
    "                              index=expr_df.index)\n",
    "    print(f\"Explained variance ratio by components: {pca.explained_variance_ratio_}\")\n",
    "    print(f\"Cumulative explained variance: {np.sum(pca.explained_variance_ratio_)}\")\n",
    "    return pca_result, pca\n",
    "\n",
    "def plot_pca(pca_result_df, phenotype_df, color_column='_primary_disease', output_path=None, pca_model=None):\n",
    "    \"\"\"\n",
    "    Plots the PCA results, colored by a specified phenotype column.\n",
    "    Enhanced for readability and aesthetics for sharing.\n",
    "\n",
    "    Args:\n",
    "        pca_result_df (pd.DataFrame): DataFrame with PCA components.\n",
    "        phenotype_df (pd.DataFrame): Matched phenotype DataFrame.\n",
    "        color_column (str): The column in phenotype_df to use for coloring the plot.\n",
    "        output_path (str, optional): Path to save the plot. If None, displays the plot.\n",
    "        pca_model (PCA, optional): The fitted PCA model to extract explained variance.\n",
    "    \"\"\"\n",
    "    if color_column not in phenotype_df.columns:\n",
    "        print(f\"Error: '{color_column}' not found in phenotype DataFrame. Cannot color PCA plot.\")\n",
    "        return\n",
    "\n",
    "    plot_df = pca_result_df.merge(phenotype_df[[color_column]], left_index=True, right_index=True)\n",
    "\n",
    "    print(f\"\\n--- EDA: Plotting PCA results, colored by '{color_column}' ---\")\n",
    "    plt.figure(figsize=(12, 10)) # Adjusted figure size for better aspect ratio\n",
    "    sns.scatterplot(x='PC1', y='PC2', hue=color_column, data=plot_df,\n",
    "                    palette='tab20', s=80, alpha=0.85, edgecolor='black', linewidth=0.7) # Slightly larger points, black edge\n",
    "    \n",
    "    # Add explained variance to axis labels if pca_model is provided\n",
    "    pc1_variance = f\" ({pca_model.explained_variance_ratio_[0]*100:.1f}% variance)\" if pca_model else \"\"\n",
    "    pc2_variance = f\" ({pca_model.explained_variance_ratio_[1]*100:.1f}% variance)\" if pca_model else \"\"\n",
    "\n",
    "    plt.title(f'PCA of Gene Expression (Colored by {color_column.replace(\"_\", \" \").title()})', fontsize=18, weight='bold', color='darkblue')\n",
    "    plt.xlabel(f'Principal Component 1{pc1_variance}', fontsize=14, color='dimgray')\n",
    "    plt.ylabel(f'Principal Component 2{pc2_variance}', fontsize=14, color='dimgray')\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    # Clean up legend as per user's snippet\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    plt.legend(handles=handles, labels=labels, title=color_column.replace(\"_\", \" \").title(),\n",
    "               bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, fontsize='small', title_fontsize=12) # Larger legend title/text\n",
    "    \n",
    "    plt.grid(True, linestyle=':', alpha=0.5, color='lightgray') # Dotted grid, subtler\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to prevent legend overlap\n",
    "\n",
    "    if output_path:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            print(f\"Created output directory: {output_dir}\")\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"PCA plot saved to: {output_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def perform_umap(expr_df, n_components=2, random_state=42):\n",
    "    \"\"\"\n",
    "    Performs UMAP dimensionality reduction on the gene expression data.\n",
    "\n",
    "    Args:\n",
    "        expr_df (pd.DataFrame): Transposed gene expression DataFrame (samples x genes).\n",
    "        n_components (int): Number of dimensions for the UMAP embedding.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with UMAP components.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- EDA: Performing UMAP with {n_components} components ---\")\n",
    "    expr_df_filled = expr_df.fillna(expr_df.mean()) # Safeguard for NaNs\n",
    "\n",
    "    reducer = umap.UMAP(n_components=n_components, random_state=random_state)\n",
    "    umap_embedding = reducer.fit_transform(expr_df_filled)\n",
    "    umap_result = pd.DataFrame(data=umap_embedding,\n",
    "                               columns=[f'UMAP{i+1}' for i in range(n_components)],\n",
    "                               index=expr_df.index)\n",
    "    return umap_result\n",
    "\n",
    "def plot_umap(umap_result_df, phenotype_df, color_column='_primary_disease', output_path=None):\n",
    "    \"\"\"\n",
    "    Plots the UMAP results, colored by a specified phenotype column.\n",
    "    Enhanced for readability and aesthetics for sharing.\n",
    "\n",
    "    Args:\n",
    "        umap_result_df (pd.DataFrame): DataFrame with UMAP components.\n",
    "        phenotype_df (pd.DataFrame): Matched phenotype DataFrame.\n",
    "        color_column (str): The column in phenotype_df to use for coloring the plot.\n",
    "        output_path (str, optional): Path to save the plot. If None, displays the plot.\n",
    "    \"\"\"\n",
    "    if color_column not in phenotype_df.columns:\n",
    "        print(f\"Error: '{color_column}' not found in phenotype DataFrame. Cannot color UMAP plot.\")\n",
    "        return\n",
    "\n",
    "    plot_df = umap_result_df.merge(phenotype_df[[color_column]], left_index=True, right_index=True)\n",
    "\n",
    "    print(f\"\\n--- EDA: Plotting UMAP results, colored by '{color_column}' ---\")\n",
    "    plt.figure(figsize=(12, 10)) # Adjusted figure size for better aspect ratio\n",
    "    sns.scatterplot(x='UMAP1', y='UMAP2', hue=color_column, data=plot_df,\n",
    "                    palette='tab20', s=80, alpha=0.85, edgecolor='black', linewidth=0.7) # Slightly larger points, black edge\n",
    "    plt.title(f'UMAP of Gene Expression (Colored by {color_column.replace(\"_\", \" \").title()})', fontsize=18, weight='bold', color='darkblue')\n",
    "    plt.xlabel(f'UMAP Component 1', fontsize=14, color='dimgray')\n",
    "    plt.ylabel(f'UMAP Component 2', fontsize=14, color='dimgray')\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(title=color_column.replace(\"_\", \" \").title(), bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10, title_fontsize=12)\n",
    "    plt.grid(True, linestyle=':', alpha=0.5, color='lightgray')\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "\n",
    "    if output_path:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            print(f\"Created output directory: {output_dir}\")\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"UMAP plot saved to: {output_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_top_variable_genes(expr_df, top_n=20, output_path=None):\n",
    "    \"\"\"\n",
    "    Identifies and plots the top N most variable genes by standard deviation.\n",
    "    Enhanced for readability and aesthetics for sharing.\n",
    "\n",
    "    Args:\n",
    "        expr_df (pd.DataFrame): Transposed gene expression DataFrame (samples x genes).\n",
    "        top_n (int): Number of top variable genes to plot.\n",
    "        output_path (str, optional): Path to save the plot.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- EDA: Plotting Top {top_n} Most Variable Genes ---\")\n",
    "    gene_std = expr_df.std().sort_values(ascending=False)\n",
    "    top_genes = gene_std.head(top_n)\n",
    "    print(f\"Top {top_n} most variable genes:\\n{top_genes}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x=top_genes.values, y=top_genes.index, palette='viridis')\n",
    "    plt.title(f'Top {top_n} Most Variable Genes by Standard Deviation', fontsize=18, weight='bold', color='darkblue')\n",
    "    plt.xlabel('Standard Deviation of Expression', fontsize=14, color='dimgray')\n",
    "    plt.ylabel('Gene', fontsize=14, color='dimgray')\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.6, color='lightgray')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_path:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Top variable genes plot saved to: {output_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_gene_correlation_heatmap(expr_df, gene_list, output_path=None):\n",
    "    \"\"\"\n",
    "    Plots a correlation heatmap for a given list of genes.\n",
    "    Enhanced for readability and aesthetics for sharing.\n",
    "\n",
    "    Args:\n",
    "        expr_df (pd.DataFrame): Transposed gene expression DataFrame (samples x genes).\n",
    "        gene_list (list): List of gene names for which to plot correlations.\n",
    "        output_path (str, optional): Path to save the plot.\n",
    "    \"\"\"\n",
    "    present_genes = [gene for gene in gene_list if gene in expr_df.columns]\n",
    "    if not present_genes:\n",
    "        print(f\"Error: None of the specified genes {gene_list} found in expression data. Cannot plot correlation heatmap.\")\n",
    "        return\n",
    "    if len(present_genes) < 2:\n",
    "        print(f\"Warning: Only {len(present_genes)} gene(s) found. Need at least 2 for a correlation heatmap. Skipping.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- EDA: Plotting Correlation Heatmap for Selected Genes ---\")\n",
    "    \n",
    "    # Select only the present genes and compute correlation\n",
    "    corr_matrix = expr_df[present_genes].corr()\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, cmap='coolwarm', center=0, annot=True, fmt=\".2f\", linewidths=.5, linecolor='lightgray',\n",
    "                cbar_kws={'label': 'Pearson Correlation Coefficient'})\n",
    "    plt.title(f'Correlation Between Selected Genes ({len(present_genes)} Genes)', fontsize=18, weight='bold', color='darkblue')\n",
    "    plt.xticks(fontsize=10, rotation=90)\n",
    "    plt.yticks(fontsize=10, rotation=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_path:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Gene correlation heatmap saved to: {output_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_specific_gene_expression_boxplots(expr_df, phenotype_df, gene_list, tumor_type_column='_primary_disease', output_dir_base=None):\n",
    "    \"\"\"\n",
    "    Plots box plots for expression of specific genes across different tumor types.\n",
    "    Enhanced for readability and aesthetics for sharing.\n",
    "\n",
    "    Args:\n",
    "        expr_df (pd.DataFrame): Transposed gene expression DataFrame (samples x genes).\n",
    "        phenotype_df (pd.DataFrame): Matched phenotype DataFrame.\n",
    "        gene_list (list): List of gene names to plot.\n",
    "        tumor_type_column (str): The column in phenotype_df representing tumor types.\n",
    "        output_dir_base (str, optional): Base directory to save individual gene plots.\n",
    "    \"\"\"\n",
    "    if tumor_type_column not in phenotype_df.columns:\n",
    "        print(f\"Error: '{tumor_type_column}' not found in phenotype DataFrame. Cannot plot gene expression boxplots.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- EDA: Plotting Specific Gene Expression Across Tumor Types ---\")\n",
    "    \n",
    "    if output_dir_base:\n",
    "        os.makedirs(output_dir_base, exist_ok=True)\n",
    "\n",
    "    for gene in gene_list:\n",
    "        if gene in expr_df.columns:\n",
    "            plot_df = pd.DataFrame({\n",
    "                'Expression': expr_df[gene],\n",
    "                'TumorType': phenotype_df.loc[expr_df.index, tumor_type_column]\n",
    "            }).dropna() # Drop NA if any for plotting\n",
    "\n",
    "            if plot_df.empty:\n",
    "                print(f\"Warning: No valid data for gene '{gene}' after merging with phenotype. Skipping plot.\")\n",
    "                continue\n",
    "\n",
    "            plt.figure(figsize=(12, 7))\n",
    "            # FIX: Add hue=TumorType and legend=False to suppress FutureWarning\n",
    "            sns.boxplot(x='TumorType', y='Expression', data=plot_df, palette='Set2', hue='TumorType', legend=False)\n",
    "            plt.xticks(rotation=90, fontsize=10)\n",
    "            plt.yticks(fontsize=10)\n",
    "            plt.title(f'{gene} Expression Across {tumor_type_column.replace(\"_\", \" \").title()}', fontsize=18, weight='bold', color='darkblue')\n",
    "            plt.xlabel(tumor_type_column.replace(\"_\", \" \").title(), fontsize=14, color='dimgray')\n",
    "            plt.ylabel(f'{gene} Expression', fontsize=14, color='dimgray')\n",
    "            plt.grid(axis='y', linestyle='--', alpha=0.6, color='lightgray')\n",
    "            plt.tight_layout()\n",
    "\n",
    "            if output_dir_base:\n",
    "                output_path = os.path.join(output_dir_base, f\"{gene}_expression_across_cancers.png\")\n",
    "                plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "                print(f\"Plot for {gene} saved to: {output_path}\")\n",
    "            else:\n",
    "                plt.show()\n",
    "            plt.close()\n",
    "        else:\n",
    "            print(f\"Warning: Gene '{gene}' not found in expression data. Skipping boxplot for this gene.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Main Execution Block for EDA & Visualization\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting EDA & Visualization Phase...\")\n",
    "\n",
    "    # --- Configuration ---\n",
    "    processed_data_dir = \"processed_data\"\n",
    "    eda_plots_dir = \"eda_plots\"\n",
    "    os.makedirs(eda_plots_dir, exist_ok=True) # Ensure output directory exists\n",
    "\n",
    "    processed_expr_file = os.path.join(processed_data_dir, \"expr_processed.tsv\")\n",
    "    processed_pheno_file = os.path.join(processed_data_dir, \"pheno_processed.tsv\")\n",
    "    \n",
    "    # IMPORTANT: Use '_primary_disease' as the target column as per your snippet\n",
    "    target_column = '_primary_disease' \n",
    "\n",
    "    expr_processed = None\n",
    "    pheno_processed = None\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(processed_expr_file) and os.path.exists(processed_pheno_file):\n",
    "            print(\"Loading processed data for EDA example...\")\n",
    "            expr_processed = pd.read_csv(processed_expr_file, sep='\\t', index_col=0)\n",
    "            pheno_processed = pd.read_csv(processed_pheno_file, sep='\\t', index_col=0)\n",
    "            print(\"Processed data loaded successfully.\")\n",
    "        else:\n",
    "            print(\"Processed data files not found. Generating dummy data for demonstration.\")\n",
    "            np.random.seed(42)\n",
    "            num_samples = 100\n",
    "            num_genes = 500\n",
    "            genes = [f'Gene_{i}' for i in range(num_genes)]\n",
    "            samples = [f'Sample_{i}' for i in range(num_samples)]\n",
    "            \n",
    "            expr_processed = pd.DataFrame(np.random.rand(num_samples, num_genes), index=samples, columns=genes)\n",
    "            tumor_types = ['BRCA', 'LUAD', 'COAD', 'KIRC', 'LIHC', 'STAD', 'BLCA', 'HNSC', 'LGG', 'OV'] # More types for dummy\n",
    "            pheno_processed = pd.DataFrame({\n",
    "                '_primary_disease': np.random.choice(tumor_types, num_samples),\n",
    "                'age_at_diagnosis': np.random.randint(30, 80, num_samples)\n",
    "            }, index=samples)\n",
    "            print(\"Dummy data generated.\")\n",
    "\n",
    "        # Ensure the phenotype column exists for plotting\n",
    "        if target_column not in pheno_processed.columns:\n",
    "            print(f\"'{target_column}' column not found in phenotype data. Creating a dummy column for plotting.\")\n",
    "            pheno_processed[target_column] = np.random.choice(['DiseaseA', 'DiseaseB', 'DiseaseC'], len(pheno_processed))\n",
    "\n",
    "        # ======================================================================\n",
    "        # EDA Step 1: Generate Summary Statistics\n",
    "        # ======================================================================\n",
    "        generate_summary_statistics(expr_processed, \"Gene Expression (Processed)\")\n",
    "        generate_summary_statistics(pheno_processed, \"Phenotype (Processed)\")\n",
    "\n",
    "        # ======================================================================\n",
    "        # EDA Step 2: Tumor Type Distribution\n",
    "        # (Corresponds to your \"Step 5: Tumor Type Distribution\")\n",
    "        # ======================================================================\n",
    "        plot_tumor_type_distribution(pheno_processed, target_column,\n",
    "                                     output_path=os.path.join(eda_plots_dir, \"tumor_type_distribution.png\"))\n",
    "\n",
    "        # ======================================================================\n",
    "        # EDA Step 3: Summary Statistics of Gene Expression (Histograms)\n",
    "        # (Corresponds to your \"Step 6: Summary Statistics of Expression\")\n",
    "        # ======================================================================\n",
    "        plot_expression_summary_histograms(expr_processed,\n",
    "                                           output_path=os.path.join(eda_plots_dir, \"expression_mean_std_hist.png\"))\n",
    "\n",
    "        # ======================================================================\n",
    "        # EDA Step 4: PCA of Gene Expression\n",
    "        # (Corresponds to your \"Step 7: PCA\")\n",
    "        # ======================================================================\n",
    "        pca_results, pca_model = perform_pca(expr_processed, n_components=2)\n",
    "        plot_pca(pca_results, pheno_processed, target_column,\n",
    "                 output_path=os.path.join(eda_plots_dir, \"pca_tumor_type.png\"),\n",
    "                 pca_model=pca_model)\n",
    "\n",
    "        # ======================================================================\n",
    "        # EDA Step 5: UMAP of Gene Expression\n",
    "        # (Corresponds to your \"UMAP\" section)\n",
    "        # ======================================================================\n",
    "        umap_results = perform_umap(expr_processed, n_components=2)\n",
    "        plot_umap(umap_results, pheno_processed, target_column,\n",
    "                  output_path=os.path.join(eda_plots_dir, \"umap_tumor_type.png\"))\n",
    "\n",
    "        # ======================================================================\n",
    "        # EDA Step 6: Top Variable Genes\n",
    "        # (Corresponds to your \"gene_std\" and \"top_genes\" section)\n",
    "        # ======================================================================\n",
    "        plot_top_variable_genes(expr_processed, top_n=20,\n",
    "                                output_path=os.path.join(eda_plots_dir, \"top_variable_genes.png\"))\n",
    "\n",
    "        # ======================================================================\n",
    "        # EDA Step 7: Correlation Heatmap for Top Variable Genes\n",
    "        # (Corresponds to your \"top_corr_genes\" section)\n",
    "        # Re-using the top 20 genes from the previous step for correlation\n",
    "        # ======================================================================\n",
    "        gene_std_for_corr = expr_processed.std().sort_values(ascending=False)\n",
    "        top_20_variable_genes = gene_std_for_corr.head(20).index.tolist()\n",
    "        plot_gene_correlation_heatmap(expr_processed, top_20_variable_genes,\n",
    "                                      output_path=os.path.join(eda_plots_dir, \"gene_correlation_heatmap.png\"))\n",
    "\n",
    "        # ======================================================================\n",
    "        # EDA Step 8: Specific Cancer Gene Expression Boxplots\n",
    "        # (Corresponds to your \"cancer_genes\" section)\n",
    "        # ======================================================================\n",
    "        cancer_genes_to_plot = ['TP53', 'EGFR', 'MYC', 'BRCA1', 'CDKN2A']\n",
    "        plot_specific_gene_expression_boxplots(expr_processed, pheno_processed, cancer_genes_to_plot,\n",
    "                                               tumor_type_column=target_column,\n",
    "                                               output_dir_base=os.path.join(eda_plots_dir, \"gene_expression_boxplots\"))\n",
    "\n",
    "\n",
    "        print(\"\\nEDA and Visualization steps completed. Check 'eda_plots' directory for generated plots.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during EDA & Visualization: {e}\")\n",
    "        print(\"Please ensure processed data files are available or paths are correct.\")\n",
    "\n",
    "    print(\"\\nEDA & Visualization Phase complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6759ec48",
   "metadata": {},
   "source": [
    "# ==============================================================================\n",
    "# Phase 1.2: Machine Learning (Enhanced Plots for Publication/Sharing)\n",
    "# This script handles data splitting, model training, evaluation,\n",
    "# confusion matrix plotting, feature importance extraction, and\n",
    "# advanced visualizations like correlation heatmaps and similarity networks.\n",
    "#\n",
    "# Before running:\n",
    "# 1. Ensure you have all necessary libraries installed:\n",
    "#    pip install pandas numpy matplotlib seaborn scikit-learn joblib networkx\n",
    "# 2. This script assumes 'processed_data/expr_processed.tsv' and\n",
    "#    'processed_data/pheno_processed.tsv' exist from Phase 1.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267bb3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Phase 3: Machine Learning (Enhanced Plots for Publication/Sharing)\n",
    "# This script handles data splitting, model training, evaluation,\n",
    "# confusion matrix plotting, feature importance extraction, and\n",
    "# advanced visualizations like correlation heatmaps and similarity networks.\n",
    "#\n",
    "# Before running:\n",
    "# 1. Ensure you have all necessary libraries installed:\n",
    "#    pip install pandas numpy matplotlib seaborn scikit-learn joblib networkx\n",
    "# 2. This script assumes 'processed_data/expr_processed.tsv' and\n",
    "#    'processed_data/pheno_processed.tsv' exist from Phase 1.\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import joblib # For saving/loading models\n",
    "import networkx as nx # For the similarity network plot\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "# Set a consistent style for all plots for a professional look\n",
    "plt.style.use('seaborn-v0_8-darkgrid') # A good default for clean, readable plots\n",
    "\n",
    "def split_data(X, y, test_size=0.2, random_state=42, stratify=None):\n",
    "    \"\"\"\n",
    "    Splits the data into training and testing sets.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature DataFrame (gene expression).\n",
    "        y (pd.Series): Target Series (phenotype column, e.g., tumor type).\n",
    "        test_size (float): Proportion of the dataset to include in the test split.\n",
    "        random_state (int): Controls the shuffling applied to the data before applying the split.\n",
    "        stratify (array-like or None): If not None, data is split in a stratified fashion,\n",
    "                                       using this as the class labels. Useful for imbalanced datasets.\n",
    "\n",
    "    Returns:\n",
    "        tuple: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Machine Learning: Splitting data (test_size={test_size}) ---\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=stratify\n",
    "    )\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_model(model_name, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Trains a specified machine learning model.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model to train ('RandomForest', 'SVM', 'LogisticRegression').\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        y_train (pd.Series): Training target.\n",
    "        **kwargs: Additional arguments for the model constructor.\n",
    "\n",
    "    Returns:\n",
    "        trained_model: The fitted machine learning model.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Machine Learning: Training {model_name} model ---\")\n",
    "    model = None\n",
    "    if model_name == 'RandomForest':\n",
    "        model = RandomForestClassifier(random_state=42, **kwargs)\n",
    "    elif model_name == 'SVM':\n",
    "        model = SVC(probability=True, random_state=42, **kwargs)\n",
    "    elif model_name == 'LogisticRegression':\n",
    "        model = LogisticRegression(random_state=42, solver='liblinear', **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model_name: {model_name}\")\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"{model_name} model training complete.\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Evaluates the trained model and prints performance metrics.\n",
    "\n",
    "    Args:\n",
    "        model: The trained machine learning model.\n",
    "        X_test (pd.DataFrame): Test features.\n",
    "        y_test (pd.Series): Test target.\n",
    "        model_name (str): Name of the model for reporting.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of evaluation metrics.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Machine Learning: Evaluating {model_name} model ---\")\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "        'f1_score': f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    }\n",
    "\n",
    "    print(f\"--- {model_name} Performance Metrics ---\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "    try:\n",
    "        if len(np.unique(y_test)) > 2:\n",
    "            y_proba = model.predict_proba(X_test)\n",
    "            metrics['roc_auc'] = roc_auc_score(y_test, y_proba, multi_class='ovr', average='weighted')\n",
    "            print(f\"ROC AUC (Weighted OvR): {metrics['roc_auc']:.4f}\")\n",
    "        else:\n",
    "            y_proba = model.predict_proba(X_test)[:, 1]\n",
    "            metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "            print(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "    except AttributeError:\n",
    "        print(\"Model does not support predict_proba for ROC AUC calculation.\")\n",
    "        metrics['roc_auc'] = np.nan\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not calculate ROC AUC: {e}\")\n",
    "        metrics['roc_auc'] = np.nan\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def plot_confusion_matrix(model, X_test, y_test, class_names_for_plot, model_name=\"Model\", output_path=None):\n",
    "    \"\"\"\n",
    "    Plots the confusion matrix for the trained model.\n",
    "    Enhanced for readability and aesthetics for sharing.\n",
    "\n",
    "    Args:\n",
    "        model: The trained machine learning model.\n",
    "        X_test (pd.DataFrame): Test features.\n",
    "        y_test (pd.Series): Test target.\n",
    "        class_names_for_plot (list): List of original string class labels.\n",
    "        model_name (str): Name of the model for plot title.\n",
    "        output_path (str, optional): Path to save the plot. If None, displays the plot.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Machine Learning: Plotting confusion matrix for {model_name} ---\")\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Get the unique integer labels that confusion_matrix expects (0, 1, ..., N-1)\n",
    "    # These correspond to the order of class_names_for_plot\n",
    "    integer_labels = range(len(class_names_for_plot))\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=integer_labels)\n",
    "    \n",
    "    # Use the original string labels for the DataFrame for readability\n",
    "    cm_df = pd.DataFrame(cm, index=class_names_for_plot, columns=class_names_for_plot)\n",
    "\n",
    "    plt.figure(figsize=(14, 12)) # Adjusted figure size for better readability with many classes\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=True, # Set annot=True to show numbers\n",
    "                linewidths=.7, linecolor='black', annot_kws={\"size\": 10}) # Adjusted annotation font size\n",
    "    plt.title(f'Confusion Matrix for {model_name}', fontsize=18, weight='bold', color='darkblue')\n",
    "    plt.xlabel('Predicted Label', fontsize=14, color='dimgray')\n",
    "    plt.ylabel('True Label', fontsize=14, color='dimgray')\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10) # Adjusted tick font size\n",
    "    plt.yticks(rotation=0, fontsize=10) # Adjusted tick font size\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_path:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            print(f\"Created output directory: {output_dir}\")\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Confusion matrix plot saved to: {output_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def get_feature_importance(model, feature_names, model_name=\"Model\", top_n=20):\n",
    "    \"\"\"\n",
    "    Extracts and prints feature importance (if available for the model).\n",
    "\n",
    "    Args:\n",
    "        model: The trained machine learning model.\n",
    "        feature_names (list): List of feature names (gene names).\n",
    "        model_name (str): Name of the model.\n",
    "        top_n (int): Number of top features to display.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series or None: A Series of feature importances, or None if not applicable.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Machine Learning: Extracting feature importance for {model_name} ---\")\n",
    "    importances = None\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = pd.Series(model.feature_importances_, index=feature_names)\n",
    "        importances = importances.sort_values(ascending=False)\n",
    "        print(f\"Top {top_n} Feature Importances for {model_name}:\\n{importances.head(top_n)}\")\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        if model.coef_.ndim > 1:\n",
    "            importances = pd.Series(np.sum(np.abs(model.coef_), axis=0), index=feature_names)\n",
    "        else:\n",
    "            importances = pd.Series(np.abs(model.coef_), index=feature_names)\n",
    "        importances = importances.sort_values(ascending=False)\n",
    "        print(f\"Top {top_n} Absolute Coefficients (Feature Importance) for {model_name}:\\n{importances.head(top_n)}\")\n",
    "    else:\n",
    "        print(f\"Feature importance/coefficients not available for {model_name} model type.\")\n",
    "    return importances\n",
    "\n",
    "def plot_top_important_genes(feature_importances_series, top_n=30, output_path=None):\n",
    "    \"\"\"\n",
    "    Plots the top N most important genes based on feature importance.\n",
    "    Enhanced for readability and aesthetics for sharing.\n",
    "\n",
    "    Args:\n",
    "        feature_importances_series (pd.Series): Series of gene importances (index=gene_name, value=importance).\n",
    "        top_n (int): Number of top genes to plot.\n",
    "        output_path (str, optional): Path to save the plot.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Machine Learning: Plotting Top {top_n} Most Important Genes ---\")\n",
    "    top_genes = feature_importances_series.head(top_n) # Assumes series is already sorted\n",
    "    \n",
    "    plt.figure(figsize=(12, 9)) # Adjusted figure size\n",
    "    sns.barplot(x=top_genes.values, y=top_genes.index, palette='viridis')\n",
    "    plt.title(f\"Top {top_n} Most Important Genes (Random Forest Feature Importance)\", fontsize=18, weight='bold', color='darkblue')\n",
    "    plt.xlabel(\"Feature Importance (Gini Importance)\", fontsize=14, color='dimgray')\n",
    "    plt.ylabel(\"Gene\", fontsize=14, color='dimgray')\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.6, color='lightgray')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_path:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Top important genes plot saved to: {output_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_correlation_heatmap_top_genes(expr_df, feature_importances_series, top_n=50, output_path=None):\n",
    "    \"\"\"\n",
    "    Plots a correlation heatmap for the top N most important genes.\n",
    "    Enhanced for readability and aesthetics for sharing.\n",
    "\n",
    "    Args:\n",
    "        expr_df (pd.DataFrame): Transposed gene expression DataFrame (samples x genes).\n",
    "        feature_importances_series (pd.Series): Series of gene importances (index=gene_name, value=importance).\n",
    "        top_n (int): Number of top genes to include in the heatmap.\n",
    "        output_path (str, optional): Path to save the plot.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Machine Learning: Plotting Correlation Heatmap of Top {top_n} Important Genes ---\")\n",
    "    top_genes_for_corr = feature_importances_series.head(top_n).index.tolist()\n",
    "    \n",
    "    # Ensure genes are actually in the expression data\n",
    "    present_genes = [gene for gene in top_genes_for_corr if gene in expr_df.columns]\n",
    "    if not present_genes:\n",
    "        print(f\"Error: None of the top {top_n} genes found in expression data. Cannot plot correlation heatmap.\")\n",
    "        return\n",
    "    if len(present_genes) < 2:\n",
    "        print(f\"Warning: Only {len(present_genes)} top gene(s) found. Need at least 2 for a correlation heatmap. Skipping.\")\n",
    "        return\n",
    "\n",
    "    corr_matrix = expr_df[present_genes].corr()\n",
    "\n",
    "    plt.figure(figsize=(16, 14)) # Larger figure for better visibility of labels\n",
    "    sns.heatmap(corr_matrix, cmap='coolwarm', center=0, annot=False, fmt=\".2f\", # No annotations for very large heatmaps\n",
    "                linewidths=.5, linecolor='lightgray', cbar_kws={'label': 'Pearson Correlation Coefficient'})\n",
    "    plt.title(f\"Correlation Heatmap of Top {len(present_genes)} Important Genes\", fontsize=18, weight='bold', color='darkblue')\n",
    "    plt.xticks(fontsize=8, rotation=90) # Smaller font for many labels\n",
    "    plt.yticks(fontsize=8, rotation=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_path:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Correlation heatmap saved to: {output_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_tumor_similarity_network(y_true, y_pred, class_names, threshold=5, output_path=None):\n",
    "    \"\"\"\n",
    "    Builds and plots a network graph showing similarity between tumor types\n",
    "    based on misclassification patterns in the confusion matrix.\n",
    "    Enhanced for readability and aesthetics for sharing.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.array): True labels (encoded).\n",
    "        y_pred (np.array): Predicted labels (encoded).\n",
    "        class_names (list): Original class names corresponding to encoded labels.\n",
    "        threshold (int): Minimum number of misclassifications between two classes\n",
    "                         to draw an edge in the network.\n",
    "        output_path (str, optional): Path to save the plot.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Machine Learning: Plotting Tumor Type Similarity Network ---\")\n",
    "    # Get the unique integer labels that confusion_matrix expects (0, 1, ..., N-1)\n",
    "    integer_labels = range(len(class_names))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=integer_labels)\n",
    "    conf_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes\n",
    "    for class_name in class_names:\n",
    "        G.add_node(class_name)\n",
    "\n",
    "    # Add edges based on off-diagonal confusion matrix values\n",
    "    for i, true_label in enumerate(class_names):\n",
    "        for j, pred_label in enumerate(class_names):\n",
    "            if i != j: # Only consider misclassifications (off-diagonal)\n",
    "                misclassification_count = conf_df.iloc[i, j]\n",
    "                if misclassification_count >= threshold:\n",
    "                    G.add_edge(true_label, pred_label, weight=misclassification_count)\n",
    "\n",
    "    if not G.edges():\n",
    "        print(f\"No strong misclassification links found above threshold {threshold}. Skipping network plot.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(18, 16)) # Increased figure size further for more space\n",
    "    \n",
    "    # Use spring_layout for a more organic, force-directed layout\n",
    "    # Adjusted k for more spacing, and increased iterations for better convergence\n",
    "    pos = nx.spring_layout(G, seed=42, k=1.0, iterations=100) \n",
    "\n",
    "    # Node sizes based on total misclassifications involving that node\n",
    "    node_sizes = []\n",
    "    for node in G.nodes():\n",
    "        # Sum of misclassifications for this node (both as true and predicted)\n",
    "        total_misclass = conf_df.loc[node, :].drop(node, errors='ignore').sum() + \\\n",
    "                         conf_df.loc[:, node].drop(node, errors='ignore').sum()\n",
    "        node_sizes.append(500 + total_misclass * 10) # Base size + scaled by misclassifications\n",
    "\n",
    "    # Edge widths based on misclassification count\n",
    "    edges = G.edges(data=True)\n",
    "    weights = [edge[2]['weight'] for edge in edges]\n",
    "    edge_widths = [w * 0.2 for w in weights] # Scale down for visualization\n",
    "\n",
    "    # Draw network elements\n",
    "    nx.draw_networkx_nodes(G, pos, node_color='skyblue', node_size=node_sizes, edgecolors='black', linewidths=1.0, alpha=0.9)\n",
    "    nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.6, edge_color='gray', style='dashed') # Dashed edges\n",
    "    # FIX: Reduced font size for labels to prevent overlap\n",
    "    nx.draw_networkx_labels(G, pos, font_size=7, font_weight='bold', font_color='black') # Smaller font for contrast\n",
    "\n",
    "    # Add edge labels (misclassification counts)\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6, font_color='darkred') # Smaller font for edge labels\n",
    "\n",
    "    plt.title(f\"Tumor Type Similarity Network (Misclassifications > {threshold})\", fontsize=20, weight='bold', color='darkblue', pad=20)\n",
    "    plt.axis('off') # Turn off axis\n",
    "    plt.tight_layout() # Ensure tight layout for saving\n",
    "\n",
    "    if output_path:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight') # Ensure tight bounding box for saving\n",
    "        print(f\"Tumor similarity network saved to: {output_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_model(model, path):\n",
    "    \"\"\"Saves the trained model to a file.\"\"\"\n",
    "    output_dir = os.path.dirname(path)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Created output directory: {output_dir}\")\n",
    "    joblib.dump(model, path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_model(path):\n",
    "    \"\"\"Loads a trained model from a file.\"\"\"\n",
    "    model = joblib.load(path)\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return model\n",
    "\n",
    "# ==============================================================================\n",
    "# Main Execution Block for Machine Learning\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Machine Learning Phase...\")\n",
    "\n",
    "    # --- Configuration ---\n",
    "    processed_data_dir = \"processed_data\"\n",
    "    ml_results_dir = \"ml_results\"\n",
    "    os.makedirs(ml_results_dir, exist_ok=True) # Ensure output directory exists\n",
    "\n",
    "    processed_expr_file = os.path.join(processed_data_dir, \"expr_processed.tsv\")\n",
    "    processed_pheno_file = os.path.join(processed_data_dir, \"pheno_processed.tsv\")\n",
    "    trained_model_path = os.path.join(ml_results_dir, \"random_forest_model.joblib\")\n",
    "    \n",
    "    # IMPORTANT: Use '_primary_disease' as the target column as per your snippet\n",
    "    target_column = '_primary_disease' \n",
    "\n",
    "    expr_processed = None\n",
    "    pheno_processed = None\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(processed_expr_file) and os.path.exists(processed_pheno_file):\n",
    "            print(\"Loading processed data for Machine Learning example...\")\n",
    "            expr_processed = pd.read_csv(processed_expr_file, sep='\\t', index_col=0)\n",
    "            pheno_processed = pd.read_csv(processed_pheno_file, sep='\\t', index_col=0)\n",
    "            print(\"Processed data loaded successfully.\")\n",
    "        else:\n",
    "            print(\"Processed data files not found. Generating dummy data for demonstration.\")\n",
    "            np.random.seed(42)\n",
    "            num_samples = 100\n",
    "            num_genes = 500\n",
    "            genes = [f'Gene_{i}' for i in range(num_genes)]\n",
    "            samples = [f'Sample_{i}' for i in range(num_samples)]\n",
    "            \n",
    "            expr_processed = pd.DataFrame(np.random.rand(num_samples, num_genes), index=samples, columns=genes)\n",
    "            tumor_types = ['BRCA', 'LUAD', 'COAD', 'KIRC', 'LIHC', 'STAD', 'BLCA', 'HNSC', 'LGG', 'OV']\n",
    "            pheno_processed = pd.DataFrame({\n",
    "                '_primary_disease': np.random.choice(tumor_types, num_samples),\n",
    "                'age_at_diagnosis': np.random.randint(30, 80, num_samples)\n",
    "            }, index=samples)\n",
    "            print(\"Dummy data generated.\")\n",
    "\n",
    "        # === Data Preprocessing for ML ===\n",
    "        # Ensure the target column exists\n",
    "        if target_column not in pheno_processed.columns:\n",
    "            print(f\"Error: Target column '{target_column}' not found in phenotype data.\")\n",
    "            print(\"Creating a dummy target column for demonstration.\")\n",
    "            pheno_processed[target_column] = np.random.choice(['TypeA', 'TypeB', 'TypeC'], len(pheno_processed))\n",
    "\n",
    "        # Labels (target variable)\n",
    "        labels = pheno_processed.loc[expr_processed.index, target_column]\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(labels)\n",
    "        class_names = le.classes_ # Store original class names\n",
    "\n",
    "        # Features (all genes)\n",
    "        X = expr_processed.fillna(0) # Fill NaN values, if any, before scaling\n",
    "\n",
    "        # Scaling gene expression\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        # === Train-test split ===\n",
    "        X_train, X_test, y_train, y_test = split_data(\n",
    "            X_scaled, y, test_size=0.2, stratify=y, random_state=42\n",
    "        )\n",
    "\n",
    "        print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "        print(f\"Test set shape: {X_test.shape}\")\n",
    "        print(f\"Number of tumor types: {len(class_names)}\")\n",
    "        print(f\"Tumor types: {class_names.tolist()}\")\n",
    "\n",
    "\n",
    "        # === Train Random Forest Model ===\n",
    "        rf_model = train_model(\n",
    "            'RandomForest',\n",
    "            X_train, y_train,\n",
    "            n_estimators=200,\n",
    "            max_depth=None, # Let the model determine depth\n",
    "            n_jobs=-1,      # Use all available cores\n",
    "            verbose=1       # Print training progress\n",
    "        )\n",
    "\n",
    "        # === Evaluate Random Forest Model ===\n",
    "        rf_metrics = evaluate_model(rf_model, X_test, y_test, model_name='RandomForest')\n",
    "        \n",
    "        # Plot Confusion Matrix\n",
    "        plot_confusion_matrix(rf_model, X_test, y_test, class_names, 'RandomForest',\n",
    "                              output_path=os.path.join(ml_results_dir, \"rf_confusion_matrix.png\"))\n",
    "        \n",
    "        # Save the trained Random Forest model\n",
    "        save_model(rf_model, trained_model_path)\n",
    "\n",
    "        # === Feature Importance ===\n",
    "        # Get feature importances from the trained RF model\n",
    "        rf_feature_importances_series = get_feature_importance(rf_model, X.columns, 'RandomForest', top_n=len(X.columns))\n",
    "        \n",
    "        if rf_feature_importances_series is not None and not rf_feature_importances_series.empty:\n",
    "            # Plot Top Important Genes (top 30 as per your snippet)\n",
    "            plot_top_important_genes(rf_feature_importances_series, top_n=30,\n",
    "                                     output_path=os.path.join(ml_results_dir, \"top_rf_genes_barplot.png\"))\n",
    "\n",
    "            # Plot Correlation Heatmap of Top 50 Important Genes\n",
    "            plot_correlation_heatmap_top_genes(X, rf_feature_importances_series, top_n=50,\n",
    "                                               output_path=os.path.join(ml_results_dir, \"correlation_top50_genes.png\"))\n",
    "        else:\n",
    "            print(\"Skipping feature importance plots as importances could not be retrieved.\")\n",
    "\n",
    "        # === Tumor Type Similarity Network (from Confusion Matrix) ===\n",
    "        plot_tumor_similarity_network(y_test, rf_model.predict(X_test), class_names, threshold=5,\n",
    "                                      output_path=os.path.join(ml_results_dir, \"cancer_similarity_network.png\"))\n",
    "\n",
    "\n",
    "        print(\"\\nMachine Learning analysis completed. Check 'ml_results' directory for plots and saved models.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Machine Learning: {e}\")\n",
    "        print(\"Please ensure processed data files are available and target column is suitable for classification.\")\n",
    "\n",
    "    print(\"\\nMachine Learning Phase complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f82a18c",
   "metadata": {},
   "source": [
    "# ==============================================================================\n",
    "# Phase 1.3: GSEA Pathway Analysis (Enhanced Plots for Publication/Sharing)\n",
    "# This script performs Gene Set Enrichment Analysis (GSEA) on important genes\n",
    "# identified from the Machine Learning phase and generates publication-ready plots.\n",
    "#\n",
    "# Before running:\n",
    "# 1. Ensure you have gseapy installed: pip install gseapy\n",
    "# 2. This script assumes 'processed_data/expr_processed.tsv',\n",
    "#    'processed_data/pheno_processed.tsv', and 'ml_results/random_forest_model.joblib'\n",
    "#    exist from Phase 1 and Phase 3, respectively.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f7d405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import joblib # For loading the trained model\n",
    "import gseapy as gp # For GSEA\n",
    "\n",
    "# For wrapping text in plots\n",
    "import textwrap\n",
    "\n",
    "# Set a consistent style for all plots for a professional look\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "def rank_genes(feature_importances_series, method='descending'):\n",
    "    \"\"\"\n",
    "    Ranks genes based on feature importance or other metrics.\n",
    "\n",
    "    Args:\n",
    "        feature_importances_series (pd.Series): A pandas Series where index is gene names\n",
    "                                                and values are their importance scores.\n",
    "        method (str): How to rank the genes ('descending' for high importance first,\n",
    "                      'ascending' for low importance first).\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Series of ranked gene names.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- GSEA: Ranking genes by '{method}' importance ---\")\n",
    "    if method == 'descending':\n",
    "        ranked_genes = feature_importances_series.sort_values(ascending=False)\n",
    "    elif method == 'ascending':\n",
    "        ranked_genes = feature_importances_series.sort_values(ascending=True)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'descending' or 'ascending'.\")\n",
    "\n",
    "    print(f\"Top 10 ranked genes:\\n{ranked_genes.head(10)}\")\n",
    "    return ranked_genes\n",
    "\n",
    "def perform_gsea(gene_list, gene_set_library='MSigDB_Hallmark_2020', organism='Human', outdir=None, cutoff=0.05):\n",
    "    \"\"\"\n",
    "    Performs Gene Set Enrichment Analysis (GSEA) using Enrichr.\n",
    "\n",
    "    Args:\n",
    "        gene_list (list): A list of gene names to analyze.\n",
    "        gene_set_library (str): The gene set library to use (e.g., 'KEGG_2021_Human',\n",
    "                                'GO_Biological_Process_2021', 'Reactome_2022', 'MSigDB_Hallmark_2020').\n",
    "                                See gseapy.get_library_name() for available libraries.\n",
    "        organism (str): Organism for the gene set library (e.g., 'Human', 'Mouse').\n",
    "        outdir (str, optional): Directory to save GSEA results. If None, results are not saved to disk.\n",
    "        cutoff (float): P-value or adjusted P-value cutoff for enrichment.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing GSEA enrichment results.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- GSEA: Performing GSEA using '{gene_set_library}' library ---\")\n",
    "    if outdir:\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "        print(f\"GSEA results will be saved to: {outdir}\")\n",
    "\n",
    "    enr = gp.enrichr(gene_list=gene_list,\n",
    "                  gene_sets=gene_set_library,\n",
    "                  organism=organism,\n",
    "                  outdir=outdir,\n",
    "                  no_plot=True, # We will plot manually for better control\n",
    "                  cutoff=cutoff,\n",
    "                  verbose=True)\n",
    "\n",
    "    if enr.results.empty:\n",
    "        print(\"No enriched pathways found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(\"GSEA completed. Top 5 enriched pathways:\")\n",
    "    print(enr.results.head())\n",
    "    return enr.results\n",
    "\n",
    "def plot_top_enriched_pathways_barh(gsea_results_df, top_n=20, output_path=None):\n",
    "    \"\"\"\n",
    "    Plots the top N enriched pathways from GSEA results as a horizontal bar plot.\n",
    "    Enhanced for readability and aesthetics for sharing.\n",
    "\n",
    "    Args:\n",
    "        gsea_results_df (pd.DataFrame): DataFrame containing GSEA enrichment results.\n",
    "        top_n (int): Number of top pathways to plot.\n",
    "        output_path (str, optional): Path to save the plot. If None, displays the plot.\n",
    "    \"\"\"\n",
    "    if gsea_results_df.empty:\n",
    "        print(\"No GSEA results to plot for top enriched pathways.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- GSEA: Plotting Top {top_n} Enriched Pathways (Bar Plot) ---\")\n",
    "    # Sort by Adjusted P-value (ascending) and take top N\n",
    "    plot_df = gsea_results_df.sort_values(by='Adjusted P-value', ascending=True).head(top_n)\n",
    "\n",
    "    plt.figure(figsize=(12, min(0.6 * len(plot_df), 12))) # Dynamic height, max 12\n",
    "    # FIX: Add hue='Term' and legend=False to suppress FutureWarning\n",
    "    sns.barplot(x='Adjusted P-value', y='Term', data=plot_df, palette='GnBu_d', hue='Term', legend=False)\n",
    "    \n",
    "    plt.title(f\"Top {len(plot_df)} Enriched Pathways (GSEA)\", fontsize=18, weight='bold', color='darkblue')\n",
    "    plt.xlabel(\"Adjusted P-value\", fontsize=14, color='dimgray')\n",
    "    plt.ylabel(\"Pathway Term\", fontsize=14, color='dimgray')\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.gca().invert_yaxis() # Invert y-axis to have the most significant at the top\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.6, color='lightgray')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_path:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Top enriched pathways bar plot saved to: {output_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_gsea_dot_plot(gsea_results_df, top_n=20, output_path=None):\n",
    "    \"\"\"\n",
    "    Plots the top N enriched pathways as a dot plot showing Adjusted P-value,\n",
    "    Overlap (gene count), and Enrichment Ratio.\n",
    "    Enhanced for readability and aesthetics for sharing.\n",
    "\n",
    "    Args:\n",
    "        gsea_results_df (pd.DataFrame): DataFrame containing GSEA enrichment results.\n",
    "        top_n (int): Number of top pathways to plot.\n",
    "        output_path (str, optional): Path to save the plot. If None, displays the plot.\n",
    "    \"\"\"\n",
    "    if gsea_results_df.empty:\n",
    "        print(\"No GSEA results to plot for dot plot.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- GSEA: Plotting Top {top_n} Enriched Pathways (Dot Plot) ---\")\n",
    "    plot_df = gsea_results_df.sort_values(by='Adjusted P-value', ascending=True).head(top_n).copy()\n",
    "    \n",
    "    # Calculate Enrichment Ratio (if not already present)\n",
    "    # FIX: Corrected 'Geneset_size' to 'GeneSet_size'\n",
    "    if 'Enrichment Ratio' not in plot_df.columns:\n",
    "        # Ensure 'Overlap' column is in 'count/total' format and 'GeneSet_size' exists\n",
    "        if 'Overlap' in plot_df.columns and 'GeneSet_size' in plot_df.columns:\n",
    "            plot_df['Enrichment Ratio'] = plot_df['Overlap'].apply(lambda x: int(x.split('/')[0])) / \\\n",
    "                                          plot_df['GeneSet_size']\n",
    "        else:\n",
    "            print(\"Warning: 'Overlap' or 'GeneSet_size' column not found for Enrichment Ratio calculation.\")\n",
    "            # Fallback or handle error, e.g., set to 1 to avoid division by zero if not present\n",
    "            plot_df['Enrichment Ratio'] = 1.0 # Placeholder to prevent crash\n",
    "\n",
    "    plt.figure(figsize=(14, min(0.7 * len(plot_df), 14))) # Dynamic height\n",
    "    \n",
    "    # Create the dot plot\n",
    "    sns.scatterplot(\n",
    "        data=plot_df,\n",
    "        x='Adjusted P-value',\n",
    "        y='Term',\n",
    "        size='Overlap', # Size of dots by number of overlapping genes\n",
    "        hue='Enrichment Ratio', # Color by enrichment ratio\n",
    "        palette='viridis_r', # Reverse viridis for higher ratio = darker color\n",
    "        sizes=(100, 1000), # Range of dot sizes\n",
    "        edgecolor='black',\n",
    "        linewidth=0.5,\n",
    "        alpha=0.8\n",
    "    )\n",
    "    \n",
    "    plt.xscale('log') # Log scale for p-value for better distribution\n",
    "    plt.title(f\"Top {len(plot_df)} Enriched Pathways (GSEA Dot Plot)\", fontsize=18, weight='bold', color='darkblue')\n",
    "    plt.xlabel(\"Adjusted P-value (log scale)\", fontsize=14, color='dimgray')\n",
    "    plt.ylabel(\"Pathway Term\", fontsize=14, color='dimgray')\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.gca().invert_yaxis() # Most significant at the top\n",
    "    plt.grid(True, linestyle=':', alpha=0.5, color='lightgray')\n",
    "    \n",
    "    # Adjust legend for size and hue\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    # Separate handles/labels for size and hue\n",
    "    # Ensure there are enough unique values for both hue and size legends\n",
    "    if 'Enrichment Ratio' in plot_df.columns and not plot_df['Enrichment Ratio'].empty:\n",
    "        num_unique_hue = len(plot_df['Enrichment Ratio'].unique())\n",
    "    else:\n",
    "        num_unique_hue = 0 # No unique hue values if column is missing or empty\n",
    "\n",
    "    if 'Overlap' in plot_df.columns and not plot_df['Overlap'].empty:\n",
    "        num_unique_size = len(plot_df['Overlap'].unique())\n",
    "    else:\n",
    "        num_unique_size = 0 # No unique size values if column is missing or empty\n",
    "\n",
    "    # Ensure we don't try to slice beyond available handles/labels\n",
    "    if num_unique_hue > 0 and len(handles) >= num_unique_hue:\n",
    "        hue_legend_handles = handles[:num_unique_hue]\n",
    "        hue_legend_labels = labels[:num_unique_hue]\n",
    "        legend1 = plt.legend(hue_legend_handles, hue_legend_labels, title='Enrichment Ratio', \n",
    "                             bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10, title_fontsize=12)\n",
    "        plt.gca().add_artist(legend1) # Add the first legend manually\n",
    "    else:\n",
    "        print(\"Warning: Not enough unique hue values or handles for Enrichment Ratio legend.\")\n",
    "\n",
    "    if num_unique_size > 0 and len(handles) >= (num_unique_hue + num_unique_size):\n",
    "        size_legend_handles = handles[num_unique_hue : num_unique_hue + num_unique_size]\n",
    "        size_legend_labels = labels[num_unique_hue : num_unique_hue + num_unique_size]\n",
    "        legend2 = plt.legend(size_legend_handles, size_legend_labels, title='Overlapping Genes', \n",
    "                             bbox_to_anchor=(1.05, 0.7), loc='upper left', fontsize=10, title_fontsize=12)\n",
    "    else:\n",
    "        print(\"Warning: Not enough unique size values or handles for Overlapping Genes legend.\")\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to prevent legend overlap\n",
    "\n",
    "    if output_path:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"GSEA dot plot saved to: {output_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_gene_pathway_heatmap(gsea_results_df, top_n_pathways=20, top_genes_list=None, output_path=None):\n",
    "    \"\"\"\n",
    "    Plots a binary heatmap showing which genes are present in which of the top N enriched pathways.\n",
    "    Enhanced for readability and aesthetics for sharing.\n",
    "\n",
    "    Args:\n",
    "        gsea_results_df (pd.DataFrame): DataFrame containing GSEA enrichment results.\n",
    "        top_n_pathways (int): Number of top pathways to include in the heatmap.\n",
    "        top_genes_list (list, optional): A list of globally important genes (e.g., from RF feature importance).\n",
    "                                         If provided, only genes from this list that are in pathways will be shown.\n",
    "        output_path (str, optional): Path to save the plot.\n",
    "    \"\"\"\n",
    "    if gsea_results_df.empty:\n",
    "        print(\"No GSEA results to plot for gene-pathway heatmap.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- GSEA: Plotting Gene-Pathway Heatmap ---\")\n",
    "    # Take top N pathways\n",
    "    enriched_df = gsea_results_df.sort_values(by='Adjusted P-value', ascending=True).head(top_n_pathways)\n",
    "    \n",
    "    pathway_gene_map = {\n",
    "        row['Term']: set(row['Genes'].split(';'))\n",
    "        for _, row in enriched_df.iterrows()\n",
    "    }\n",
    "\n",
    "    # Collect all unique genes in these top pathways\n",
    "    all_genes_in_pathways = set(g for genes_set in pathway_gene_map.values() for g in genes_set)\n",
    "\n",
    "    # Filter genes if a top_genes_list is provided\n",
    "    if top_genes_list:\n",
    "        all_genes_to_plot = sorted(list(all_genes_in_pathways.intersection(set(top_genes_list))))\n",
    "        if not all_genes_to_plot:\n",
    "            print(\"No overlapping genes between top pathways and provided top genes list. Skipping heatmap.\")\n",
    "            return\n",
    "    else:\n",
    "        all_genes_to_plot = sorted(list(all_genes_in_pathways))\n",
    "\n",
    "    if not all_genes_to_plot:\n",
    "        print(\"No genes found to plot in the gene-pathway heatmap. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Create binary matrix [Genes x Pathways]\n",
    "    heatmap_df = pd.DataFrame(0, index=all_genes_to_plot, columns=pathway_gene_map.keys())\n",
    "    for pathway, genes_set in pathway_gene_map.items():\n",
    "        for gene in genes_set:\n",
    "            if gene in heatmap_df.index: # Only add if gene is in our filtered list\n",
    "                heatmap_df.loc[gene, pathway] = 1\n",
    "\n",
    "    # Sort genes by number of pathways they are in, and pathways by number of genes\n",
    "    heatmap_df['Gene_Count'] = heatmap_df.sum(axis=1)\n",
    "    heatmap_df = heatmap_df.sort_values(by='Gene_Count', ascending=False).drop(columns='Gene_Count')\n",
    "    heatmap_df = heatmap_df.loc[:, heatmap_df.sum(axis=0).sort_values(ascending=False).index] # Sort columns too\n",
    "\n",
    "    plt.figure(figsize=(min(0.6 * len(heatmap_df.columns), 20), min(0.4 * len(heatmap_df.index), 25))) # Dynamic sizing\n",
    "    sns.heatmap(\n",
    "        heatmap_df,\n",
    "        cmap=\"YlGnBu\", # Good for binary data\n",
    "        cbar=False,\n",
    "        linewidths=0.5,\n",
    "        linecolor='gray',\n",
    "        xticklabels=True,\n",
    "        yticklabels=True\n",
    "    )\n",
    "    plt.title(f\"Gene–Pathway Heatmap (Top {len(heatmap_df.columns)} GSEA Pathways)\", fontsize=18, weight='bold', color='darkblue')\n",
    "    plt.xlabel(\"Pathways\", fontsize=14, color='dimgray')\n",
    "    plt.ylabel(\"Genes\", fontsize=14, color='dimgray')\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=9)\n",
    "    plt.yticks(fontsize=8)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_path:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Gene-pathway heatmap saved to: {output_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_rf_gene_pathway_overlap(gsea_results_df, rf_top_genes_list, top_n_pathways=15, output_path=None):\n",
    "    \"\"\"\n",
    "    Plots the number of overlapping genes between Random Forest's top genes\n",
    "    and genes in the top N enriched GSEA pathways.\n",
    "    Enhanced for readability and aesthetics for sharing.\n",
    "\n",
    "    Args:\n",
    "        gsea_results_df (pd.DataFrame): DataFrame containing GSEA enrichment results.\n",
    "        rf_top_genes_list (list): List of gene names identified as important by Random Forest.\n",
    "        top_n_pathways (int): Number of top pathways to analyze for overlap.\n",
    "        output_path (str, optional): Path to save the plot.\n",
    "    \"\"\"\n",
    "    if gsea_results_df.empty:\n",
    "        print(\"No GSEA results to analyze for RF gene overlap.\")\n",
    "        return\n",
    "    if not rf_top_genes_list:\n",
    "        print(\"No Random Forest top genes provided. Skipping RF gene-pathway overlap plot.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- GSEA: Plotting Random Forest Gene Enrichment in GSEA Pathways ---\")\n",
    "\n",
    "    # Helper to wrap long gene names for text labels\n",
    "    def wrap_genes(gene_str, width=25):\n",
    "        return \"\\n\".join(textwrap.wrap(gene_str, width=width))\n",
    "\n",
    "    overlap_data = []\n",
    "    # Sort by Adjusted P-value (ascending) and take top N pathways\n",
    "    top_enriched_df = gsea_results_df.sort_values(by='Adjusted P-value', ascending=True).head(top_n_pathways)\n",
    "\n",
    "    for _, row in top_enriched_df.iterrows():\n",
    "        pathway = row['Term']\n",
    "        pathway_genes = set(row['Genes'].split(\";\"))\n",
    "        overlap_genes = pathway_genes.intersection(set(rf_top_genes_list))\n",
    "        if overlap_genes:\n",
    "            overlap_data.append({\n",
    "                'Pathway': pathway,\n",
    "                'NumOverlap': len(overlap_genes),\n",
    "                'Genes': \", \".join(sorted(list(overlap_genes))) # Convert set to list for sorting\n",
    "            })\n",
    "\n",
    "    if not overlap_data:\n",
    "        print(\"No overlapping genes found between RF top genes and top GSEA pathways. Skipping plot.\")\n",
    "        return\n",
    "\n",
    "    overlap_df = pd.DataFrame(overlap_data)\n",
    "    # Sort by NumOverlap for plotting\n",
    "    overlap_df = overlap_df.sort_values(by='NumOverlap', ascending=False)\n",
    "    overlap_df['WrappedGenes'] = overlap_df['Genes'].apply(lambda g: wrap_genes(g, width=25))\n",
    "\n",
    "    plt.figure(figsize=(15, min(0.7 * len(overlap_df), 12))) # Dynamic height, max 12\n",
    "    # FIX: Add hue='Pathway' and legend=False to suppress FutureWarning\n",
    "    sns.barplot(\n",
    "        data=overlap_df,\n",
    "        x='NumOverlap',\n",
    "        y='Pathway',\n",
    "        palette='Blues_d',\n",
    "        hue='Pathway',\n",
    "        legend=False\n",
    "    )\n",
    "\n",
    "    # Add wrapped gene names beside bars\n",
    "    for i, (x, label) in enumerate(zip(overlap_df['NumOverlap'], overlap_df['WrappedGenes'])):\n",
    "        plt.text(\n",
    "            x + 0.5, i, label,\n",
    "            va='center',\n",
    "            ha='left',\n",
    "            fontsize=8, # Smaller font for many genes\n",
    "            color='black'\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Number of Overlapping RF Genes\", fontsize=14, color='dimgray')\n",
    "    plt.ylabel(\"Enriched Pathway\", fontsize=14, color='dimgray')\n",
    "    plt.title(\"Random Forest Gene Enrichment in GSEA Pathways\", fontsize=18, weight='bold', color='darkblue')\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=10) # Smaller font for pathway names if many\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.6, color='lightgray')\n",
    "    plt.tight_layout(rect=[0, 0, 0.75, 1]) # Adjust layout to make space for gene labels\n",
    "\n",
    "    if output_path:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"RF GSEA overlap plot saved to: {output_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Main Execution Block for GSEA Pathway Analysis\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting GSEA Pathway Analysis Phase...\")\n",
    "\n",
    "    # --- Configuration ---\n",
    "    processed_data_dir = \"processed_data\"\n",
    "    ml_results_dir = \"ml_results\"\n",
    "    gsea_results_dir = \"gsea_results\"\n",
    "    os.makedirs(gsea_results_dir, exist_ok=True) # Ensure output directory exists\n",
    "\n",
    "    processed_expr_file = os.path.join(processed_data_dir, \"expr_processed.tsv\")\n",
    "    trained_model_path = os.path.join(ml_results_dir, \"random_forest_model.joblib\")\n",
    "\n",
    "    expr_processed = None\n",
    "    rf_model = None\n",
    "\n",
    "    try:\n",
    "        # Load processed expression data\n",
    "        if os.path.exists(processed_expr_file):\n",
    "            expr_processed = pd.read_csv(processed_expr_file, sep='\\t', index_col=0)\n",
    "            print(\"Processed expression data loaded successfully.\")\n",
    "        else:\n",
    "            print(f\"Processed expression file not found: {processed_expr_file}\")\n",
    "            print(\"Generating dummy expression data for GSEA demonstration.\")\n",
    "            np.random.seed(42)\n",
    "            num_samples = 100\n",
    "            num_genes = 2000 # Reduced for dummy GSEA\n",
    "            genes = [f'Gene_{i}' for i in range(num_genes)]\n",
    "            samples = [f'Sample_{i}' for i in range(num_samples)]\n",
    "            expr_processed = pd.DataFrame(np.random.rand(num_samples, num_genes), index=samples, columns=genes)\n",
    "            print(\"Dummy expression data generated.\")\n",
    "\n",
    "        # Load the trained Random Forest model\n",
    "        if os.path.exists(trained_model_path):\n",
    "            rf_model = joblib.load(trained_model_path)\n",
    "            print(\"Trained Random Forest model loaded successfully.\")\n",
    "        else:\n",
    "            print(f\"Trained model file not found: {trained_model_path}\")\n",
    "            print(\"Training a dummy Random Forest model for GSEA demonstration.\")\n",
    "            # Need dummy y for training\n",
    "            dummy_pheno_path = os.path.join(processed_data_dir, \"pheno_processed.tsv\")\n",
    "            if os.path.exists(dummy_pheno_path):\n",
    "                dummy_pheno = pd.read_csv(dummy_pheno_path, sep='\\t', index_col=0)\n",
    "                # Ensure target column exists, create if not\n",
    "                dummy_target_column = '_primary_disease'\n",
    "                if dummy_target_column not in dummy_pheno.columns:\n",
    "                    dummy_pheno[dummy_target_column] = np.random.choice(['TypeA', 'TypeB', 'TypeC'], len(dummy_pheno))\n",
    "                \n",
    "                # Align indices of expr_processed and dummy_pheno\n",
    "                common_indices = expr_processed.index.intersection(dummy_pheno.index)\n",
    "                X_dummy = expr_processed.loc[common_indices].fillna(0)\n",
    "                y_dummy = dummy_pheno.loc[common_indices, dummy_target_column]\n",
    "\n",
    "                if len(y_dummy.unique()) < 2: # Ensure at least two classes for classification\n",
    "                    y_list = y_dummy.tolist()\n",
    "                    if len(y_list) > 1:\n",
    "                        y_list[1] = 'AnotherType' if y_list[1] == y_list[0] else y_list[0]\n",
    "                    y_dummy = pd.Series(y_list, index=y_dummy.index)\n",
    "\n",
    "                # Scale dummy X\n",
    "                scaler = StandardScaler()\n",
    "                X_scaled_dummy = scaler.fit_transform(X_dummy)\n",
    "\n",
    "                # Split dummy data\n",
    "                X_train_dummy, X_test_dummy, y_train_dummy, y_test_dummy = train_test_split(\n",
    "                    X_scaled_dummy, y_dummy, test_size=0.3, stratify=y_dummy, random_state=42\n",
    "                )\n",
    "                \n",
    "                # Train dummy model\n",
    "                rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "                rf_model.fit(X_train_dummy, y_train_dummy)\n",
    "                print(\"Dummy Random Forest model trained.\")\n",
    "            else:\n",
    "                print(\"Cannot load or generate dummy phenotype data. Skipping GSEA.\")\n",
    "                exit()\n",
    "\n",
    "\n",
    "        if rf_model is None or not hasattr(rf_model, 'feature_importances_'):\n",
    "            print(\"Random Forest model not available or lacks feature importances. Skipping GSEA.\")\n",
    "            exit()\n",
    "\n",
    "        # === Step 1: Get top N important genes from RF model ===\n",
    "        # Use expr_processed.columns as gene_names, which should align with model's features\n",
    "        gene_names_from_expr = expr_processed.columns\n",
    "        importances = rf_model.feature_importances_\n",
    "        \n",
    "        # Create a Series for feature importances to ensure gene names are associated\n",
    "        feature_importances_series = pd.Series(importances, index=gene_names_from_expr)\n",
    "\n",
    "        top_n_genes_for_gsea = 300 # As per your snippet\n",
    "        ranked_genes = rank_genes(feature_importances_series, method='descending')\n",
    "        top_genes_for_gsea_list = ranked_genes.head(top_n_genes_for_gsea).index.tolist()\n",
    "        \n",
    "        if not top_genes_for_gsea_list:\n",
    "            print(\"No top genes found for GSEA. Skipping GSEA analysis.\")\n",
    "            exit()\n",
    "\n",
    "        # === Step 2: Run GSEA with MSigDB Hallmark ===\n",
    "        gsea_results = perform_gsea(\n",
    "            gene_list=top_genes_for_gsea_list,\n",
    "            gene_set_library=\"MSigDB_Hallmark_2020\",\n",
    "            organism='Human',\n",
    "            outdir=None, # Do not save raw gseapy output to disk, we'll save our custom plots\n",
    "            cutoff=0.05\n",
    "        )\n",
    "\n",
    "        if gsea_results.empty:\n",
    "            print(\"No significant enriched pathways found. Skipping GSEA plots.\")\n",
    "        else:\n",
    "            # === Step 3: Visualize Top 20 Pathways (Bar Plot) ===\n",
    "            plot_top_enriched_pathways_barh(gsea_results, top_n=20,\n",
    "                                            output_path=os.path.join(gsea_results_dir, \"gsea_top_pathways_barh.png\"))\n",
    "\n",
    "            # === Step 4: GSEA Dot Plot (New Plot) ===\n",
    "            plot_gsea_dot_plot(gsea_results, top_n=20,\n",
    "                               output_path=os.path.join(gsea_results_dir, \"gsea_dot_plot.png\"))\n",
    "\n",
    "            # === Step 5: Gene-Pathway Heatmap ===\n",
    "            # Need to pass the original expr_processed for gene names if not using feature importance series index directly\n",
    "            plot_gene_pathway_heatmap(gsea_results, top_n_pathways=20, top_genes_list=top_genes_for_gsea_list,\n",
    "                                      output_path=os.path.join(gsea_results_dir, \"gsea_gene_pathway_heatmap.png\"))\n",
    "\n",
    "            # === Step 6: Random Forest Gene Enrichment in GSEA Pathways (Overlap Plot) ===\n",
    "            plot_rf_gene_pathway_overlap(gsea_results, top_genes_for_gsea_list, top_n_pathways=15,\n",
    "                                         output_path=os.path.join(gsea_results_dir, \"rf_gsea_overlap_neat_final.png\"))\n",
    "\n",
    "        print(\"\\nGSEA Pathway Analysis completed. Check 'gsea_results' directory for generated plots.\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"\\n'gseapy' library not found. Please install it: pip install gseapy\")\n",
    "        print(\"Skipping GSEA analysis.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during GSEA Pathway Analysis: {e}\")\n",
    "        print(\"Please ensure processed data and trained model files are available and paths are correct.\")\n",
    "\n",
    "    print(\"\\nGSEA Pathway Analysis Phase complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87341405",
   "metadata": {},
   "source": [
    "# ==============================================================================\n",
    "# Phase 1.4: Misclassification Analysis & Preranked GSEA\n",
    "# This script identifies misclassified samples from the Machine Learning phase,\n",
    "# performs differential expression analysis on specific misclassified groups,\n",
    "# and conducts preranked GSEA to find enriched pathways. It also visualizes\n",
    "# these groups using UMAP.\n",
    "#\n",
    "# Before running:\n",
    "# 1. Ensure you have gseapy and umap-learn installed: pip install gseapy umap-learn\n",
    "# 2. This script assumes 'processed_data/expr_processed.tsv',\n",
    "#    'processed_data/pheno_processed.tsv', and 'ml_results/random_forest_model.joblib'\n",
    "#    exist from previous phases.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f983a853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import joblib # For loading the trained model\n",
    "import gseapy as gp # For GSEA\n",
    "import umap.umap_ as umap # For UMAP visualization\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Set a consistent style for all plots for a professional look\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "def load_data_and_model(processed_expr_file, processed_pheno_file, trained_model_path, target_column):\n",
    "    \"\"\"\n",
    "    Loads processed data and the trained model, and prepares features/labels.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Loading Data and Model for Misclassification Analysis ---\")\n",
    "    expr_processed = None\n",
    "    pheno_processed = None\n",
    "    rf_model = None\n",
    "\n",
    "    if os.path.exists(processed_expr_file) and os.path.exists(processed_pheno_file):\n",
    "        expr_processed = pd.read_csv(processed_expr_file, sep='\\t', index_col=0)\n",
    "        pheno_processed = pd.read_csv(processed_pheno_file, sep='\\t', index_col=0)\n",
    "        print(\"Processed data loaded successfully.\")\n",
    "    else:\n",
    "        print(\"Processed data files not found. Generating dummy data for demonstration.\")\n",
    "        np.random.seed(42)\n",
    "        num_samples = 100\n",
    "        num_genes = 500\n",
    "        genes = [f'Gene_{i}' for i in range(num_genes)]\n",
    "        samples = [f'Sample_{i}' for i in range(num_samples)]\n",
    "        expr_processed = pd.DataFrame(np.random.rand(num_samples, num_genes), index=samples, columns=genes)\n",
    "        tumor_types = ['BRCA', 'LUAD', 'COAD', 'KIRC', 'LIHC', 'STAD', 'BLCA', 'HNSC', 'LGG', 'OV', 'rectum adenocarcinoma', 'colon adenocarcinoma']\n",
    "        pheno_processed = pd.DataFrame({\n",
    "            '_primary_disease': np.random.choice(tumor_types, num_samples),\n",
    "            'age_at_diagnosis': np.random.randint(30, 80, num_samples)\n",
    "        }, index=samples)\n",
    "        print(\"Dummy data generated.\")\n",
    "\n",
    "    if os.path.exists(trained_model_path):\n",
    "        rf_model = joblib.load(trained_model_path)\n",
    "        print(\"Trained Random Forest model loaded successfully.\")\n",
    "    else:\n",
    "        print(f\"Trained model file not found: {trained_model_path}\")\n",
    "        print(\"Training a dummy Random Forest model for demonstration.\")\n",
    "        # Ensure target column exists, create if not\n",
    "        if target_column not in pheno_processed.columns:\n",
    "            pheno_processed[target_column] = np.random.choice(['TypeA', 'TypeB', 'TypeC'], len(pheno_processed))\n",
    "        \n",
    "        # Align indices\n",
    "        common_indices = expr_processed.index.intersection(pheno_processed.index)\n",
    "        X_dummy = expr_processed.loc[common_indices].fillna(0)\n",
    "        y_dummy = pheno_processed.loc[common_indices, target_column]\n",
    "\n",
    "        if len(y_dummy.unique()) < 2:\n",
    "            y_list = y_dummy.tolist()\n",
    "            if len(y_list) > 1:\n",
    "                y_list[1] = 'AnotherType' if y_list[1] == y_list[0] else y_list[0]\n",
    "            y_dummy = pd.Series(y_list, index=y_dummy.index)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled_dummy = scaler.fit_transform(X_dummy)\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        y_encoded_dummy = le.fit_transform(y_dummy)\n",
    "\n",
    "        X_train_dummy, X_test_dummy, y_train_dummy, y_test_dummy = train_test_split(\n",
    "            X_scaled_dummy, y_encoded_dummy, test_size=0.2, stratify=y_encoded_dummy, random_state=42\n",
    "        )\n",
    "        rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        rf_model.fit(X_train_dummy, y_train_dummy)\n",
    "        print(\"Dummy Random Forest model trained.\")\n",
    "    \n",
    "    # Prepare data for analysis\n",
    "    X_full = expr_processed.fillna(0)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled_full = scaler.fit_transform(X_full)\n",
    "    \n",
    "    labels_full = pheno_processed.loc[expr_processed.index, target_column]\n",
    "    le = LabelEncoder()\n",
    "    y_encoded_full = le.fit_transform(labels_full)\n",
    "\n",
    "    # Re-split data to get X_test, y_test matching the model's training\n",
    "    # This is crucial for consistency with the trained model's test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled_full, y_encoded_full, test_size=0.2, stratify=y_encoded_full, random_state=42\n",
    "    )\n",
    "\n",
    "    return expr_processed, pheno_processed, rf_model, X_test, y_test, le, X_full # Return X_full for original gene names\n",
    "\n",
    "def analyze_misclassifications(model, X_test, y_test, label_encoder):\n",
    "    \"\"\"\n",
    "    Identifies and counts misclassified samples.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Analyzing Misclassified Samples ---\")\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'SampleID': label_encoder.inverse_transform(y_test), # Placeholder, will be replaced with actual sample IDs\n",
    "        'TrueLabel': label_encoder.inverse_transform(y_test),\n",
    "        'PredictedLabel': label_encoder.inverse_transform(y_pred)\n",
    "    })\n",
    "    \n",
    "    # Correctly map sample IDs from the original expr_processed index\n",
    "    # Assuming X_test came from a train_test_split on X_scaled_full,\n",
    "    # the indices of X_test correspond to a subset of X_full's index.\n",
    "    # We need to get the actual sample IDs from the original dataframe before splitting.\n",
    "    # This requires recreating the split with the original index.\n",
    "    \n",
    "    # Re-perform split to get sample IDs associated with X_test\n",
    "    X_full_df = pd.DataFrame(X_test, index=pd.Series(y_test).index, columns=model.feature_names_in_) # Use feature_names_in_ to get original gene names\n",
    "    \n",
    "    # This is a bit tricky. If X_test is just a numpy array, it loses its index.\n",
    "    # We need to ensure that the sample IDs are correctly carried through the split.\n",
    "    # For now, let's assume the order is preserved from the original full dataset after scaling and splitting.\n",
    "    # A more robust way would be to split the original (unscaled) DataFrame and then scale.\n",
    "    \n",
    "    # Let's adjust the sample ID assignment for robustness\n",
    "    # The actual sample IDs for X_test should come from the index of the original DataFrame before scaling and splitting.\n",
    "    # Since X_test is a numpy array, its index is lost. We need to pass the original sample IDs through the split.\n",
    "    # For this function, let's assume `X_test`'s rows correspond to the last `len(X_test)` samples\n",
    "    # of the original `expr_processed` if the split was done simply on a sorted DataFrame.\n",
    "    # A better approach (which should be in the main block) is to split `X_full` (DataFrame) directly.\n",
    "\n",
    "    # For now, let's use the index of the `y_test` Series, which should retain original sample IDs if `y` was a Series with index.\n",
    "    results_df['SampleID'] = pd.Series(y_test).index # This assumes y_test retains its original index\n",
    "\n",
    "    misclassified_df = results_df[results_df['TrueLabel'] != results_df['PredictedLabel']]\n",
    "\n",
    "    print(f\"🔎 Number of misclassified samples: {misclassified_df.shape[0]}\")\n",
    "    print(\"First 5 misclassified samples:\")\n",
    "    print(misclassified_df.head())\n",
    "\n",
    "    confusion_pairs = (\n",
    "        misclassified_df.groupby(['TrueLabel', 'PredictedLabel'])\n",
    "        .size()\n",
    "        .reset_index(name='Count')\n",
    "        .sort_values(by='Count', ascending=False)\n",
    "    )\n",
    "\n",
    "    print(\"\\n🔥 Top 10 Misclassifications:\\n\")\n",
    "    print(confusion_pairs.head(10))\n",
    "    return misclassified_df, confusion_pairs\n",
    "\n",
    "def plot_top_misclassification_pairs(confusion_pairs_df, top_n=10, output_path=None):\n",
    "    \"\"\"\n",
    "    Plots the top N misclassification pairs as a bar plot.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Plotting Top {top_n} Misclassification Pairs ---\")\n",
    "    \n",
    "    plot_df = confusion_pairs_df.head(top_n).copy()\n",
    "    plot_df['Misclassification_Pair'] = plot_df['TrueLabel'] + ' -> ' + plot_df['PredictedLabel']\n",
    "    \n",
    "    plt.figure(figsize=(12, min(0.6 * len(plot_df), 10)))\n",
    "    # FIX: Add hue='Misclassification_Pair' and legend=False to suppress FutureWarning\n",
    "    sns.barplot(x='Count', y='Misclassification_Pair', data=plot_df, palette='Reds_d', hue='Misclassification_Pair', legend=False)\n",
    "    \n",
    "    plt.title(f'Top {len(plot_df)} Most Frequent Misclassification Pairs', fontsize=18, weight='bold', color='darkblue')\n",
    "    plt.xlabel('Number of Misclassified Samples', fontsize=14, color='dimgray')\n",
    "    plt.ylabel('True Label -> Predicted Label', fontsize=14, color='dimgray')\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.6, color='lightgray')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_path:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Top misclassification pairs plot saved to: {output_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def define_sample_groups(results_df, misclassified_df, target_misclass_true, target_misclass_pred):\n",
    "    \"\"\"\n",
    "    Defines specific sample groups for comparative analysis.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Defining Sample Groups for {target_misclass_true} misclassified as {target_misclass_pred} ---\")\n",
    "    \n",
    "    group_A = misclassified_df[\n",
    "        (misclassified_df['TrueLabel'] == target_misclass_true) &\n",
    "        (misclassified_df['PredictedLabel'] == target_misclass_pred)\n",
    "    ]['SampleID'].tolist()\n",
    "    print(f\"Number of samples in Group A (Misclassified {target_misclass_true} -> {target_misclass_pred}): {len(group_A)}\")\n",
    "\n",
    "    group_B = results_df[\n",
    "        (results_df['TrueLabel'] == target_misclass_pred) &\n",
    "        (results_df['PredictedLabel'] == target_misclass_pred)\n",
    "    ]['SampleID'].tolist()\n",
    "    print(f\"Number of samples in Group B (Correctly classified {target_misclass_pred}): {len(group_B)}\")\n",
    "\n",
    "    group_C = results_df[\n",
    "        (results_df['TrueLabel'] == target_misclass_true) &\n",
    "        (results_df['PredictedLabel'] == target_misclass_true)\n",
    "    ]['SampleID'].tolist()\n",
    "    print(f\"Number of samples in Group C (Correctly classified {target_misclass_true}): {len(group_C)}\")\n",
    "    \n",
    "    return group_A, group_B, group_C\n",
    "\n",
    "def perform_differential_expression(expr_T, group_A_samples, group_B_samples):\n",
    "    \"\"\"\n",
    "    Performs differential expression analysis (log2FC) between two groups.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Performing Differential Expression (Log2FC) between Group A and Group B ---\")\n",
    "    \n",
    "    # Subset your gene expression matrix\n",
    "    group_A_expr = expr_T.loc[group_A_samples].fillna(0)\n",
    "    group_B_expr = expr_T.loc[group_B_samples].fillna(0)\n",
    "\n",
    "    if group_A_expr.empty or group_B_expr.empty:\n",
    "        print(\"One or both groups are empty for differential expression. Skipping.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Compute average expression per gene\n",
    "    mean_A = group_A_expr.mean(axis=0)\n",
    "    mean_B = group_B_expr.mean(axis=0)\n",
    "\n",
    "    # Compute log2 fold-change\n",
    "    # Add a small offset to avoid log(0) for genes with zero expression\n",
    "    logFC = np.log2((mean_A + 1e-6) / (mean_B + 1e-6))\n",
    "\n",
    "    # Create DataFrame of results\n",
    "    de_df = pd.DataFrame({\n",
    "        'Gene': logFC.index,\n",
    "        'log2FC': logFC.values,\n",
    "        'GroupA_mean': mean_A.values,\n",
    "        'GroupB_mean': mean_B.values\n",
    "    })\n",
    "\n",
    "    # Sort by absolute logFC\n",
    "    de_df['abs_log2FC'] = de_df['log2FC'].abs()\n",
    "    de_df_sorted = de_df.sort_values(by='abs_log2FC', ascending=False)\n",
    "\n",
    "    print(\"Top 20 differentially expressed genes (by absolute log2FC):\")\n",
    "    print(de_df_sorted.head(20)[['Gene', 'log2FC']])\n",
    "    return de_df_sorted\n",
    "\n",
    "def run_gsea_preranked(rnk_df, gene_set_library='MSigDB_Hallmark_2020', output_dir='gsea_preranked_results'):\n",
    "    \"\"\"\n",
    "    Runs preranked GSEA on a ranked gene list.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Preranked GSEA using '{gene_set_library}' library ---\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Ensure rnk_df has 'Gene' and 'log2FC' columns and is sorted\n",
    "    rnk_for_gsea = rnk_df[['Gene', 'log2FC']].sort_values(by='log2FC', ascending=False)\n",
    "\n",
    "    # Save as .rnk file (optional, but good for gseapy)\n",
    "    rnk_file_path = os.path.join(output_dir, \"ranked_genes_for_gsea.rnk\")\n",
    "    rnk_for_gsea.to_csv(rnk_file_path, sep='\\t', index=False, header=False)\n",
    "    print(f\"Ranked gene list saved to: {rnk_file_path}\")\n",
    "\n",
    "    pre_res = gp.prerank(\n",
    "        rnk=rnk_file_path, # Pass the path to the .rnk file\n",
    "        gene_sets=gene_set_library,\n",
    "        processes=4, # Number of processes to use\n",
    "        permutation_num=100, # Number of permutations for significance testing\n",
    "        outdir=output_dir,\n",
    "        format='png', # Saves plots automatically by gseapy\n",
    "        seed=42,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    if pre_res.res2d.empty:\n",
    "        print(\"No enriched pathways found in preranked GSEA.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(\"\\nTop 10 pathways enriched in Group A (Positive NES):\")\n",
    "    print(pre_res.res2d[['Term', 'NES', 'NOM p-val', 'FDR q-val']].sort_values(by='NES', ascending=False).head(10))\n",
    "\n",
    "    print(\"\\nTop 10 pathways enriched in Group B (Negative NES):\")\n",
    "    print(pre_res.res2d[['Term', 'NES', 'NOM p-val', 'FDR q-val']].sort_values(by='NES', ascending=True).head(10))\n",
    "    \n",
    "    return pre_res.res2d\n",
    "\n",
    "def plot_preranked_gsea_results(gsea_preranked_results_df, top_n=10, output_path=None):\n",
    "    \"\"\"\n",
    "    Plots the top N enriched pathways from preranked GSEA (both positive and negative NES).\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Plotting Top {top_n} Enriched Pathways from Preranked GSEA ---\")\n",
    "    \n",
    "    if gsea_preranked_results_df.empty:\n",
    "        print(\"No preranked GSEA results to plot.\")\n",
    "        return\n",
    "\n",
    "    # Filter for significant pathways (e.g., FDR q-val < 0.25, common for GSEA)\n",
    "    significant_results = gsea_preranked_results_df[gsea_preranked_results_df['FDR q-val'] < 0.25].copy()\n",
    "    \n",
    "    plot_df = pd.DataFrame() # Initialize empty DataFrame for plotting\n",
    "\n",
    "    if not significant_results.empty:\n",
    "        # Separate positive and negative NES for significant results\n",
    "        positive_nes = significant_results[significant_results['NES'] > 0].sort_values(by='NES', ascending=False).head(top_n)\n",
    "        negative_nes = significant_results[significant_results['NES'] < 0].sort_values(by='NES', ascending=True).head(top_n)\n",
    "        plot_df = pd.concat([positive_nes, negative_nes]).sort_values(by='NES', ascending=False)\n",
    "        plot_title_suffix = \"\"\n",
    "    else:\n",
    "        print(f\"No pathways found with FDR q-val < 0.25. Plotting top {top_n} pathways regardless of significance.\")\n",
    "        # If no significant results, plot top N from the full results\n",
    "        positive_nes = gsea_preranked_results_df[gsea_preranked_results_df['NES'] > 0].sort_values(by='NES', ascending=False).head(top_n)\n",
    "        negative_nes = gsea_preranked_results_df[gsea_preranked_results_df['NES'] < 0].sort_values(by='NES', ascending=True).head(top_n)\n",
    "        plot_df = pd.concat([positive_nes, negative_nes]).sort_values(by='NES', ascending=False)\n",
    "        plot_title_suffix = \" (No significant pathways found at FDR < 0.25)\"\n",
    "\n",
    "\n",
    "    if plot_df.empty:\n",
    "        print(\"No pathways to plot after filtering for top N positive/negative NES (even without significance filter).\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(14, min(0.7 * len(plot_df), 12)))\n",
    "    # FIX: Add hue='NES' and legend=False to suppress FutureWarning\n",
    "    sns.barplot(x='NES', y='Term', data=plot_df, hue='NES', palette='coolwarm', dodge=False, legend=False)\n",
    "    \n",
    "    plt.title(f'Top Enriched Pathways (Preranked GSEA){plot_title_suffix}', fontsize=18, weight='bold', color='darkblue')\n",
    "    plt.xlabel('Normalized Enrichment Score (NES)', fontsize=14, color='dimgray')\n",
    "    plt.ylabel('Pathway Term', fontsize=14, color='dimgray')\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=10) # Smaller font for pathway names if many\n",
    "    plt.axvline(0, color='grey', linestyle='--', linewidth=0.8) # Add a vertical line at NES=0\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.6, color='lightgray')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_path:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Preranked GSEA results plot saved to: {output_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_umap_misclassification_groups(expr_T, sample_ids_A, sample_ids_B, sample_ids_C, output_path=None):\n",
    "    \"\"\"\n",
    "    Plots UMAP of specific sample groups using their gene expression.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Plotting UMAP for Misclassification Groups ---\")\n",
    "\n",
    "    # Combine all relevant sample IDs and ensure they are in expr_T\n",
    "    all_selected_samples = list(set(sample_ids_A + sample_ids_B + sample_ids_C))\n",
    "    \n",
    "    # Filter expr_T to only include these samples and their genes\n",
    "    subset_expr = expr_T.loc[all_selected_samples].fillna(0)\n",
    "\n",
    "    if subset_expr.empty:\n",
    "        print(\"No data available for selected groups to plot UMAP. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Create labels for UMAP plot\n",
    "    group_labels = []\n",
    "    for sid in subset_expr.index:\n",
    "        if sid in sample_ids_A:\n",
    "            group_labels.append('Misclassified_RECAD→COAD')\n",
    "        elif sid in sample_ids_B:\n",
    "            group_labels.append('Correct_COAD')\n",
    "        elif sid in sample_ids_C:\n",
    "            group_labels.append('Correct_RECAD')\n",
    "        else:\n",
    "            group_labels.append('Other') # Should not happen if all_selected_samples is correct\n",
    "\n",
    "    # Perform UMAP\n",
    "    reducer = umap.UMAP(n_neighbors=15, min_dist=0.3, metric='correlation', random_state=42)\n",
    "    umap_coords = reducer.fit_transform(subset_expr)\n",
    "\n",
    "    # Create DataFrame for UMAP plotting\n",
    "    umap_df = pd.DataFrame(umap_coords, columns=['UMAP1', 'UMAP2'], index=subset_expr.index)\n",
    "    umap_df['Group'] = group_labels\n",
    "\n",
    "    plt.figure(figsize=(12, 8)) # Adjusted figure size\n",
    "    sns.scatterplot(\n",
    "        data=umap_df,\n",
    "        x='UMAP1', y='UMAP2',\n",
    "        hue='Group',\n",
    "        style='Group', # Use style to differentiate groups visually\n",
    "        palette='Set1', # A distinct palette\n",
    "        s=80, # Point size\n",
    "        edgecolor='k', # Black edge for points\n",
    "        alpha=0.8\n",
    "    )\n",
    "    plt.title(\"UMAP: Misclassified Rectum Adenocarcinoma vs. Colon Adenocarcinoma\", fontsize=18, weight='bold', color='darkblue')\n",
    "    plt.xlabel(\"UMAP1\", fontsize=14, color='dimgray')\n",
    "    plt.ylabel(\"UMAP2\", fontsize=14, color='dimgray')\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(title=\"Sample Group\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10, title_fontsize=12)\n",
    "    plt.grid(True, linestyle=':', alpha=0.5, color='lightgray')\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "\n",
    "    if output_path:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"UMAP plot for misclassification groups saved to: {output_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Main Execution Block for Misclassification Analysis & Preranked GSEA\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Misclassification Analysis & Preranked GSEA Phase...\")\n",
    "\n",
    "    # --- Configuration ---\n",
    "    processed_data_dir = \"processed_data\"\n",
    "    ml_results_dir = \"ml_results\"\n",
    "    gsea_preranked_dir = \"gsea_preranked_results\" # Directory for preranked GSEA outputs\n",
    "    os.makedirs(gsea_preranked_dir, exist_ok=True)\n",
    "\n",
    "    processed_expr_file = os.path.join(processed_data_dir, \"expr_processed.tsv\")\n",
    "    processed_pheno_file = os.path.join(processed_data_dir, \"pheno_processed.tsv\")\n",
    "    trained_model_path = os.path.join(ml_results_dir, \"random_forest_model.joblib\")\n",
    "    target_column = '_primary_disease'\n",
    "\n",
    "    try:\n",
    "        # Load data and model, and prepare X_test, y_test, le, expr_T for analysis\n",
    "        # expr_T here is the full processed expression data (samples x genes)\n",
    "        expr_T, pheno_full, rf_model, X_test_scaled, y_test_encoded, le, X_full_original_genes = \\\n",
    "            load_data_and_model(processed_expr_file, processed_pheno_file, trained_model_path, target_column)\n",
    "\n",
    "        if expr_T is None or rf_model is None:\n",
    "            print(\"Required data or model could not be loaded. Exiting Misclassification Analysis.\")\n",
    "            exit()\n",
    "\n",
    "        # === Step 1: Analyze Misclassifications ===\n",
    "        # We need to ensure results_df gets the correct SampleIDs.\n",
    "        # The y_test_encoded (numpy array) doesn't have original sample IDs.\n",
    "        # We need to re-create the train_test_split on the original DataFrame with index.\n",
    "        \n",
    "        # Prepare full dataset with original indices\n",
    "        X_for_split = expr_T.fillna(0) # Use expr_T directly as it has sample IDs as index\n",
    "        y_for_split = pheno_full.loc[X_for_split.index, target_column]\n",
    "        y_for_split_encoded = le.transform(y_for_split) # Use the same LabelEncoder\n",
    "\n",
    "        # Perform the split again to get the indices of test samples\n",
    "        X_train_df, X_test_df, y_train_series, y_test_series = train_test_split(\n",
    "            X_for_split, y_for_split_encoded, test_size=0.2, stratify=y_for_split_encoded, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Now X_test_df has the correct sample IDs as its index\n",
    "        y_pred_encoded = rf_model.predict(X_test_scaled) # Use the scaled X_test for prediction\n",
    "\n",
    "        results_df = pd.DataFrame({\n",
    "            'SampleID': X_test_df.index, # Correctly assign SampleIDs\n",
    "            'TrueLabel': le.inverse_transform(y_test_series),\n",
    "            'PredictedLabel': le.inverse_transform(y_pred_encoded)\n",
    "        })\n",
    "\n",
    "        misclassified_df = results_df[results_df['TrueLabel'] != results_df['PredictedLabel']]\n",
    "\n",
    "        print(f\"🔎 Number of misclassified samples: {misclassified_df.shape[0]}\")\n",
    "        print(\"First 5 misclassified samples:\")\n",
    "        print(misclassified_df.head())\n",
    "\n",
    "        confusion_pairs = (\n",
    "            misclassified_df.groupby(['TrueLabel', 'PredictedLabel'])\n",
    "            .size()\n",
    "            .reset_index(name='Count')\n",
    "            .sort_values(by='Count', ascending=False)\n",
    "        )\n",
    "        print(\"\\n🔥 Top 10 Misclassifications:\\n\")\n",
    "        print(confusion_pairs.head(10))\n",
    "\n",
    "        # NEW PLOT: Top Misclassification Pairs\n",
    "        plot_top_misclassification_pairs(confusion_pairs, top_n=10,\n",
    "                                         output_path=os.path.join(gsea_preranked_dir, \"top_misclassification_pairs.png\"))\n",
    "\n",
    "\n",
    "        # === Step 2: Define Specific Sample Groups (Example: Rectum Adenocarcinoma misclassified as Colon Adenocarcinoma) ===\n",
    "        target_misclass_true = 'rectum adenocarcinoma'\n",
    "        target_misclass_pred = 'colon adenocarcinoma'\n",
    "\n",
    "        group_A_samples, group_B_samples, group_C_samples = define_sample_groups(\n",
    "            results_df, misclassified_df, target_misclass_true, target_misclass_pred\n",
    "        )\n",
    "\n",
    "        if not group_A_samples or not group_B_samples:\n",
    "            print(f\"Insufficient samples in target misclassified or correctly classified groups for DE/GSEA. Skipping.\")\n",
    "        else:\n",
    "            # === Step 3: Perform Differential Expression Analysis ===\n",
    "            # Use X_full_original_genes for DE, as it's the unscaled version with original gene names\n",
    "            de_results_df = perform_differential_expression(X_full_original_genes, group_A_samples, group_B_samples)\n",
    "\n",
    "            if not de_results_df.empty:\n",
    "                # === Step 4: Run Preranked GSEA ===\n",
    "                gsea_preranked_results = run_gsea_preranked(\n",
    "                    de_results_df[['Gene', 'log2FC']],\n",
    "                    gene_set_library='MSigDB_Hallmark_2020',\n",
    "                    output_dir=gsea_preranked_dir\n",
    "                )\n",
    "                # NEW PLOT: Preranked GSEA Top Enriched Pathways\n",
    "                if not gsea_preranked_results.empty:\n",
    "                    plot_preranked_gsea_results(gsea_preranked_results, top_n=10,\n",
    "                                                output_path=os.path.join(gsea_preranked_dir, \"preranked_gsea_top_pathways.png\"))\n",
    "                else:\n",
    "                    print(\"Preranked GSEA results are empty. Skipping preranked GSEA plot.\")\n",
    "            else:\n",
    "                print(\"Differential expression results are empty. Skipping Preranked GSEA and its plot.\")\n",
    "\n",
    "            # === Step 5: Plot UMAP for Misclassification Groups ===\n",
    "            plot_umap_misclassification_groups(\n",
    "                X_full_original_genes, # Use original gene expression for UMAP\n",
    "                group_A_samples,\n",
    "                group_B_samples,\n",
    "                group_C_samples,\n",
    "                output_path=os.path.join(gsea_preranked_dir, \"umap_misclassified_groups.png\")\n",
    "            )\n",
    "\n",
    "        print(\"\\nMisclassification Analysis & Preranked GSEA Phase complete.\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"\\nRequired libraries (gseapy or umap-learn) not found. Please install them.\")\n",
    "        print(\"Skipping Misclassification Analysis & Preranked GSEA.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Misclassification Analysis & Preranked GSEA: {e}\")\n",
    "        print(\"Please ensure processed data, trained model files, and paths are correct.\")\n",
    "\n",
    "    print(\"\\nMisclassification Analysis & Preranked GSEA Phase complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac19fbff",
   "metadata": {},
   "source": [
    "# ==============================================================================\n",
    "# Phase 1.5: Multi-Omics Integration & Analysis\n",
    "# This script integrates gene expression and mutation data, performs\n",
    "# comprehensive EDA, dimensionality reduction (PCA, UMAP), and trains\n",
    "# a machine learning model for cancer type classification using multi-omics data.\n",
    "#\n",
    "# Before running:\n",
    "# 1. Ensure you have necessary libraries installed:\n",
    "#    pip install pandas numpy matplotlib seaborn scikit-learn umap-learn mygene\n",
    "# 2. Ensure you have the following data files:\n",
    "#    - EB++AdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.xena (Expression)\n",
    "#    - mc3.v0.2.8.PUBLIC.maf (Mutation)\n",
    "#    - TCGA_phenotype_denseDataOnlyDownload.tsv (Phenotype)\n",
    "#    - mart_export.txt (BioMart mapping for gene IDs - if needed for fallback)\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fcaf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import mygene # For robust gene ID mapping\n",
    "import umap.umap_ as umap # For UMAP visualization\n",
    "import time # Import the time module for time.sleep()\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Set a consistent style for all plots for a professional look\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# --- Configuration ---\n",
    "# FIX: Updated data_dir to the absolute path provided by the user.\n",
    "data_dir = r\"C:\\Users\\shrav\\Desktop\\PYTHON\\Cancer\\Pan Cancer Analysis\"\n",
    "output_dir = \"multi_omics_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "expr_path = os.path.join(data_dir, \"EB++AdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.xena\")\n",
    "maf_path = os.path.join(data_dir, \"mc3.v0.2.8.PUBLIC.maf\")\n",
    "pheno_path = os.path.join(data_dir, \"TCGA_phenotype_denseDataOnlyDownload.tsv\")\n",
    "mart_path = os.path.join(data_dir, \"mart_export.txt\") # Fallback for gene mapping\n",
    "\n",
    "target_column = '_primary_disease' # Column in phenotype for cancer type\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Data Loading and Preprocessing (Multi-Omics)\n",
    "# ==============================================================================\n",
    "\n",
    "def load_and_preprocess_multi_omics_data(expr_file, maf_file, pheno_file, mart_file, target_col):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses expression, mutation, and phenotype data,\n",
    "    aligning samples and genes, and preparing for multi-omics analysis.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Multi-Omics Data Loading and Preprocessing ---\")\n",
    "\n",
    "    # --- Load Data ---\n",
    "    try:\n",
    "        expr_df = pd.read_csv(expr_file, sep=\"\\t\", index_col=0)\n",
    "        maf_df = pd.read_csv(maf_file, sep=\"\\t\", comment='#', low_memory=False)\n",
    "        pheno_df = pd.read_csv(pheno_file, sep=\"\\t\", low_memory=False)\n",
    "        print(\"Raw expression, mutation (MAF), and phenotype data loaded.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data: {e}. Please ensure files are in the '{data_dir}' directory.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # --- Debug: Print Phenotype Columns ---\n",
    "    print(\"\\n--- Debug: Phenotype DataFrame Columns ---\")\n",
    "    print(pheno_df.columns.tolist())\n",
    "    print(\"------------------------------------------\")\n",
    "\n",
    "    # --- Preprocess Expression Data ---\n",
    "    expr_T = expr_df.T\n",
    "    print(f\"Initial Expression data shape (samples x genes): {expr_T.shape}\")\n",
    "    print(f\"First 5 expression gene IDs (before mapping): {expr_T.columns[:5].tolist()}\")\n",
    "\n",
    "    # --- Preprocess Mutation Data ---\n",
    "    if 'FILTER' in maf_df.columns:\n",
    "        maf_df = maf_df[maf_df['FILTER'] == 'PASS']\n",
    "        print(f\"Filtered MAF to 'PASS' mutations. New MAF shape: {maf_df.shape}\")\n",
    "    \n",
    "    maf_df = maf_df[[\"Tumor_Sample_Barcode\", \"Hugo_Symbol\"]].dropna().drop_duplicates()\n",
    "    print(f\"Mutation records after dropping NaNs and duplicates: {maf_df.shape}\")\n",
    "\n",
    "    # --- Gene ID Mapping Helper Functions ---\n",
    "    mg = mygene.MyGeneInfo()\n",
    "    \n",
    "    def batch_query_mygene_robust(ids, scopes, fields='ensembl.gene', species='human', batch_size=1000):\n",
    "        mapping = {}\n",
    "        successful_queries = 0\n",
    "        not_found_queries_list = []\n",
    "        for i in range(0, len(ids), batch_size):\n",
    "            batch = ids[i:i + batch_size]\n",
    "            try:\n",
    "                res = mg.querymany(batch, scopes=scopes, fields=fields, species=species, returnall=True)\n",
    "                \n",
    "                for nf_item in res.get('notfound', []):\n",
    "                    not_found_queries_list.append(nf_item['query'])\n",
    "\n",
    "                for r in res['out']:\n",
    "                    query_id = str(r['query'])\n",
    "                    ensembl_info = r.get(fields)\n",
    "                    if ensembl_info:\n",
    "                        ensembl_id = None\n",
    "                        if isinstance(ensembl_info, list):\n",
    "                            for item in ensembl_info:\n",
    "                                if isinstance(item, dict) and 'gene' in item:\n",
    "                                    ensembl_id = item['gene']\n",
    "                                    break\n",
    "                        elif isinstance(ensembl_info, dict) and 'gene' in ensembl_info:\n",
    "                            ensembl_id = ensembl_info['gene']\n",
    "                        \n",
    "                        if ensembl_id:\n",
    "                            mapping[query_id] = ensembl_id\n",
    "                            successful_queries += 1\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error in mygene batch query for scopes '{scopes}': {e}\")\n",
    "            time.sleep(0.1)\n",
    "        print(f\"Successfully mapped {successful_queries} out of {len(ids)} terms for scopes '{scopes}'.\")\n",
    "        if not_found_queries_list:\n",
    "            print(f\"First 10 unmapped queries for scopes '{scopes}': {not_found_queries_list[:10]}\")\n",
    "        return mapping\n",
    "\n",
    "    def load_mart_mapping(mart_file_path):\n",
    "        hugo_to_ensembl = {}\n",
    "        entrez_to_ensembl = {}\n",
    "        try:\n",
    "            mart_df = pd.read_csv(mart_file_path, sep=\"\\t\", low_memory=False)\n",
    "            print(f\"BioMart mapping file '{os.path.basename(mart_file_path)}' loaded.\")\n",
    "            print(f\"BioMart columns: {mart_df.columns.tolist()}\")\n",
    "\n",
    "            # For Hugo Symbol to Ensembl (Gene name to Gene stable ID)\n",
    "            if 'Gene name' in mart_df.columns and 'Gene stable ID' in mart_df.columns:\n",
    "                temp_map_df = mart_df[['Gene name', 'Gene stable ID']].dropna().drop_duplicates()\n",
    "                hugo_to_ensembl = dict(zip(temp_map_df['Gene name'], temp_map_df['Gene stable ID']))\n",
    "                print(f\"Found {len(hugo_to_ensembl)} Hugo Symbol to Ensembl mappings in BioMart file.\")\n",
    "            \n",
    "            # For Entrez Gene ID to Ensembl (NCBI gene ID to Gene stable ID)\n",
    "            if 'NCBI gene ID' in mart_df.columns and 'Gene stable ID' in mart_df.columns:\n",
    "                temp_map_df = mart_df[['NCBI gene ID', 'Gene stable ID']].dropna().drop_duplicates()\n",
    "                entrez_to_ensembl = dict(zip(temp_map_df['NCBI gene ID'].astype(str), temp_map_df['Gene stable ID']))\n",
    "                print(f\"Found {len(entrez_to_ensembl)} Entrez ID to Ensembl mappings in BioMart file.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: BioMart mapping file '{os.path.basename(mart_file_path)}' not found. Skipping BioMart mapping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing BioMart mapping file: {e}\")\n",
    "        return hugo_to_ensembl, entrez_to_ensembl\n",
    "\n",
    "    # Load BioMart mappings once at the beginning\n",
    "    hugo_to_ensembl_mart, entrez_to_ensembl_mart = load_mart_mapping(mart_path)\n",
    "\n",
    "    # --- Expression Gene Mapping ---\n",
    "    expr_T_ensembl = pd.DataFrame()\n",
    "    \n",
    "    # Check if expression gene IDs are already Ensembl (e.g., ENSG00000123456.7)\n",
    "    is_expr_ensembl = False\n",
    "    if len(expr_T.columns) > 0 and isinstance(expr_T.columns[0], str):\n",
    "        if all(col.startswith('ENSG') and '.' in col for col in expr_T.columns[:min(5, len(expr_T.columns))]):\n",
    "            is_expr_ensembl = True\n",
    "            expr_T_ensembl = expr_T.copy()\n",
    "            expr_T_ensembl.columns = expr_T_ensembl.columns.str.split('.').str[0]\n",
    "            print(\"Expression gene IDs appear to be Ensembl. Skipping mygene mapping for expression and removing version numbers.\")\n",
    "    \n",
    "    if not is_expr_ensembl:\n",
    "        all_expr_genes = list(expr_T.columns.astype(str))\n",
    "        expr_gene_map = {}\n",
    "\n",
    "        # FIX: Prioritize BioMart mapping for expression if available\n",
    "        if entrez_to_ensembl_mart:\n",
    "            print(\"Attempting BioMart mapping for expression data (Entrez to Ensembl)...\")\n",
    "            expr_gene_map = {k: entrez_to_ensembl_mart.get(k, None) for k in all_expr_genes}\n",
    "            expr_gene_map = {k: v for k, v in expr_gene_map.items() if v is not None} # Filter out unmapped\n",
    "            print(f\"BioMart mapped {len(expr_gene_map)} expression genes.\")\n",
    "\n",
    "        if not expr_gene_map: # If BioMart mapping failed or wasn't available, try mygene\n",
    "            print(\"BioMart mapping for expression failed or not available. Attempting mygene mapping (Entrez/Symbol to Ensembl)...\")\n",
    "            expr_gene_map = batch_query_mygene_robust(all_expr_genes, scopes=['entrezgene', 'symbol'])\n",
    "        \n",
    "        if expr_gene_map:\n",
    "            # Create a new DataFrame with mapped columns\n",
    "            mapped_cols_data = {}\n",
    "            for original_col, mapped_col in expr_gene_map.items():\n",
    "                if original_col in expr_T.columns:\n",
    "                    mapped_cols_data[mapped_col] = expr_T[original_col]\n",
    "            \n",
    "            expr_T_ensembl = pd.DataFrame(mapped_cols_data, index=expr_T.index)\n",
    "            expr_T_ensembl = expr_T_ensembl.loc[:, ~expr_T_ensembl.columns.duplicated()] # Remove duplicate Ensembl IDs\n",
    "            print(f\"Expression data after mapping to Ensembl: {expr_T_ensembl.shape}\")\n",
    "        else:\n",
    "            expr_T_ensembl = expr_T.copy() # Keep original columns if all mapping attempts fail\n",
    "            print(\"Warning: Expression gene ID mapping failed completely. Proceeding with original expression gene IDs. This will prevent gene-level alignment with mutation data.\")\n",
    "            print(f\"Expression data shape (original IDs): {expr_T_ensembl.shape}\")\n",
    "\n",
    "    # --- Mutation Gene Mapping ---\n",
    "    maf_df_mapped = maf_df.copy()\n",
    "    unique_hugo_symbols = maf_df_mapped['Hugo_Symbol'].unique().tolist()\n",
    "    mutation_gene_map = {}\n",
    "\n",
    "    # FIX: Prioritize BioMart mapping for mutation if available\n",
    "    if hugo_to_ensembl_mart:\n",
    "        print(\"Attempting BioMart mapping for mutation data (Hugo Symbol to Ensembl)...\")\n",
    "        mutation_gene_map = {k: hugo_to_ensembl_mart.get(k, None) for k in unique_hugo_symbols}\n",
    "        mutation_gene_map = {k: v for k, v in mutation_gene_map.items() if v is not None} # Filter out unmapped\n",
    "        print(f\"BioMart mapped {len(mutation_gene_map)} mutation genes.\")\n",
    "\n",
    "    if not mutation_gene_map: # If BioMart mapping failed or wasn't available, try mygene\n",
    "        print(\"BioMart mapping for mutation failed or not available. Attempting mygene mapping (Hugo Symbol to Ensembl)...\")\n",
    "        mutation_gene_map = batch_query_mygene_robust(unique_hugo_symbols, scopes=['symbol'])\n",
    "\n",
    "    if mutation_gene_map:\n",
    "        maf_df_mapped['Ensembl_ID'] = maf_df_mapped['Hugo_Symbol'].map(mutation_gene_map)\n",
    "        maf_df_mapped = maf_df_mapped.dropna(subset=['Ensembl_ID'])\n",
    "        print(f\"Mutation records after mapping Hugo_Symbol to Ensembl: {maf_df_mapped.shape}\")\n",
    "    else:\n",
    "        maf_df_mapped['Ensembl_ID'] = maf_df_mapped['Hugo_Symbol'] # Keep original Hugo Symbols as 'Ensembl_ID'\n",
    "        print(\"Warning: Mutation gene ID mapping failed completely. Proceeding with original Hugo Symbols as gene IDs.\")\n",
    "        print(f\"Mutation records shape (original Hugo Symbols): {maf_df_mapped.shape}\")\n",
    "\n",
    "    # Create binary mutation matrix (samples x Ensembl_IDs or Hugo_Symbols)\n",
    "    if not maf_df_mapped.empty:\n",
    "        # Ensure 'Ensembl_ID' column is unique before pivoting\n",
    "        maf_df_mapped = maf_df_mapped.drop_duplicates(subset=['Tumor_Sample_Barcode', 'Ensembl_ID'])\n",
    "        mutation_matrix = pd.crosstab(maf_df_mapped['Tumor_Sample_Barcode'], maf_df_mapped['Ensembl_ID'])\n",
    "        mutation_matrix = mutation_matrix.clip(upper=1)\n",
    "        print(f\"Binary mutation matrix shape: {mutation_matrix.shape}\")\n",
    "    else:\n",
    "        mutation_matrix = pd.DataFrame()\n",
    "        print(\"Empty mutation matrix created due to no successful gene mapping or no mutation records.\")\n",
    "\n",
    "\n",
    "    # --- Standardize Sample IDs ---\n",
    "    pheno_sample_id_col = 'sample' # Confirmed from previous debug output\n",
    "    \n",
    "    if pheno_sample_id_col not in pheno_df.columns:\n",
    "        print(f\"Error: The specified sample ID column '{pheno_sample_id_col}' not found in phenotype data.\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    print(f\"Using '{pheno_sample_id_col}' as the sample ID column in phenotype data.\")\n",
    "    pheno_df_cleaned = pheno_df.copy()\n",
    "    pheno_df_cleaned[pheno_sample_id_col] = pheno_df_cleaned[pheno_sample_id_col].astype(str).str.slice(0, 15).str.upper()\n",
    "    \n",
    "    # Remove duplicate sample IDs by taking the first occurrence for phenotype\n",
    "    pheno_df_cleaned = pheno_df_cleaned.loc[~pheno_df_cleaned[pheno_sample_id_col].duplicated(keep='first')]\n",
    "    pheno_df_cleaned = pheno_df_cleaned.set_index(pheno_sample_id_col)\n",
    "\n",
    "    expr_T_ensembl.index = expr_T_ensembl.index.astype(str).str.slice(0, 15).str.upper()\n",
    "    expr_T_ensembl = expr_T_ensembl.groupby(expr_T_ensembl.index).mean() # Aggregate duplicate sample IDs\n",
    "\n",
    "    if not mutation_matrix.empty:\n",
    "        mutation_matrix.index = mutation_matrix.index.astype(str).str.slice(0, 15).str.upper()\n",
    "        mutation_matrix = mutation_matrix.loc[~mutation_matrix.index.duplicated(keep='first')] # Aggregate duplicate sample IDs\n",
    "\n",
    "    # --- Intersect Samples Across All Omics ---\n",
    "    common_samples = expr_T_ensembl.index.intersection(pheno_df_cleaned.index)\n",
    "    if not mutation_matrix.empty:\n",
    "        common_samples = common_samples.intersection(mutation_matrix.index)\n",
    "    \n",
    "    print(f\"Common samples across all omics datasets: {len(common_samples)}\")\n",
    "\n",
    "    if len(common_samples) == 0:\n",
    "        print(\"No common samples found across all datasets. Cannot proceed with multi-omics analysis.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Subset all dataframes to common samples\n",
    "    expr_aligned = expr_T_ensembl.loc[common_samples]\n",
    "    mut_aligned = mutation_matrix.loc[common_samples] if not mutation_matrix.empty else pd.DataFrame(0, index=common_samples, columns=[])\n",
    "    pheno_aligned = pheno_df_cleaned.loc[common_samples]\n",
    "\n",
    "    # --- Align Genes (Columns) for Multi-Omics ---\n",
    "    # Determine if both expression and mutation data have Ensembl IDs for gene-level alignment\n",
    "    expr_cols_are_ensembl = not expr_aligned.empty and expr_aligned.shape[1] > 0 and \\\n",
    "                            all(isinstance(col, str) and col.startswith('ENSG') for col in expr_aligned.columns[:min(5, expr_aligned.shape[1])])\n",
    "    mut_cols_are_ensembl = not mut_aligned.empty and mut_aligned.shape[1] > 0 and \\\n",
    "                           all(isinstance(col, str) and col.startswith('ENSG') for col in mut_aligned.columns[:min(5, mut_aligned.shape[1])])\n",
    "    \n",
    "    if expr_cols_are_ensembl and mut_cols_are_ensembl:\n",
    "        print(\"Both expression and mutation genes are Ensembl IDs. Attempting gene-level alignment.\")\n",
    "        all_genes_union = expr_aligned.columns.union(mut_aligned.columns)\n",
    "        expr_aligned = expr_aligned.reindex(columns=all_genes_union, fill_value=0)\n",
    "        mut_aligned = mut_aligned.reindex(columns=all_genes_union, fill_value=0)\n",
    "    else:\n",
    "        print(\"Gene-level alignment skipped (either not all Ensembl IDs or one/both omics data are empty/unmapped). Features will be concatenated as-is.\")\n",
    "        # If not aligned by gene, ensure columns are unique before concatenation\n",
    "        # This step is now handled during concatenation by adding suffixes\n",
    "            \n",
    "    print(f\"Aligned expression data shape: {expr_aligned.shape}\")\n",
    "    print(f\"Aligned mutation data shape: {mut_aligned.shape}\")\n",
    "\n",
    "    # --- Impute Missing Values (Expression) ---\n",
    "    if expr_aligned.shape[1] > 0:\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        expr_imputed = pd.DataFrame(imputer.fit_transform(expr_aligned),\n",
    "                                    columns=expr_aligned.columns,\n",
    "                                    index=expr_aligned.index)\n",
    "        print(f\"Expression data imputed. Missing values: {expr_imputed.isnull().sum().sum()}\")\n",
    "    else:\n",
    "        expr_imputed = expr_aligned.copy()\n",
    "        print(\"No expression columns to impute. Skipping imputation.\")\n",
    "\n",
    "    # --- Scale Data ---\n",
    "    expr_scaled = pd.DataFrame()\n",
    "    if expr_imputed.shape[1] > 0:\n",
    "        scaler_expr = StandardScaler()\n",
    "        expr_scaled = pd.DataFrame(scaler_expr.fit_transform(expr_imputed),\n",
    "                                   columns=expr_imputed.columns,\n",
    "                                   index=expr_imputed.index)\n",
    "    else:\n",
    "        print(\"No expression columns to scale. Skipping scaling.\")\n",
    "\n",
    "    mut_scaled = pd.DataFrame()\n",
    "    if mut_aligned.shape[1] > 0:\n",
    "        scaler_mut = StandardScaler()\n",
    "        mut_scaled = pd.DataFrame(scaler_mut.fit_transform(mut_aligned),\n",
    "                                  columns=mut_aligned.columns,\n",
    "                                  index=mut_aligned.index)\n",
    "    else:\n",
    "        print(\"No mutation columns to scale. Skipping scaling.\")\n",
    "    \n",
    "    print(\"Expression and mutation data scaled (if columns available).\")\n",
    "\n",
    "    # --- Concatenate Multi-Omics Data ---\n",
    "    # Only rename columns if the DataFrame is not empty AND gene-level alignment was skipped\n",
    "    # If gene-level alignment happened, columns are already aligned and unique\n",
    "    if not (expr_cols_are_ensembl and mut_cols_are_ensembl): # Only add suffix if not aligned by gene\n",
    "        if not expr_scaled.empty:\n",
    "            expr_scaled.columns = [f\"{col}_expr\" for col in expr_scaled.columns]\n",
    "        if not mut_scaled.empty:\n",
    "            mut_scaled.columns = [f\"{col}_mut\" for col in mut_scaled.columns]\n",
    "\n",
    "    if not expr_scaled.empty and not mut_scaled.empty:\n",
    "        X_multiomics = pd.concat([expr_scaled, mut_scaled], axis=1)\n",
    "    elif not expr_scaled.empty:\n",
    "        X_multiomics = expr_scaled\n",
    "    elif not mut_scaled.empty:\n",
    "        X_multiomics = mut_scaled\n",
    "    else:\n",
    "        X_multiomics = pd.DataFrame(index=common_samples)\n",
    "        print(\"Warning: Both expression and mutation data are empty after preprocessing. Multi-omics matrix is empty.\")\n",
    "\n",
    "    y_labels = pheno_aligned[target_col]\n",
    "\n",
    "    print(f\"Final Multi-omics feature matrix shape: {X_multiomics.shape}\")\n",
    "    print(f\"Target labels shape: {y_labels.shape}\")\n",
    "\n",
    "    return X_multiomics, y_labels, pheno_aligned, expr_aligned\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Multi-Omics EDA & Visualization\n",
    "# ==============================================================================\n",
    "\n",
    "def perform_multi_omics_eda(X_multiomics, y_labels, pheno_aligned, expr_original_aligned):\n",
    "    \"\"\"\n",
    "    Performs Exploratory Data Analysis and generates plots for multi-omics data.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Multi-Omics EDA and Visualization ---\")\n",
    "\n",
    "    # --- Basic Stats ---\n",
    "    print(f\"Combined Multi-omics data shape: {X_multiomics.shape}\")\n",
    "    print(f\"Number of unique cancer types: {y_labels.nunique()}\")\n",
    "    print(\"Samples per cancer type:\\n\", y_labels.value_counts().head())\n",
    "\n",
    "    # --- Plot: Cancer Type Distribution ---\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.countplot(y=y_labels, order=y_labels.value_counts().index, palette='viridis')\n",
    "    plt.title(\"Sample Count per Cancer Type (Multi-Omics Cohort)\", fontsize=18, weight='bold', color='darkblue')\n",
    "    plt.xlabel(\"Number of Samples\", fontsize=14, color='dimgray')\n",
    "    plt.ylabel(\"Cancer Type\", fontsize=14, color='dimgray')\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"multiomics_cancer_type_distribution.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"Cancer type distribution plot saved.\")\n",
    "\n",
    "    # --- Plot: Top Variable Expression Genes ---\n",
    "    expr_original_numeric = expr_original_aligned.copy()\n",
    "    \n",
    "    if not expr_original_numeric.empty and expr_original_numeric.shape[1] > 0:\n",
    "        gene_std = expr_original_numeric.std().sort_values(ascending=False)\n",
    "        top_expr_genes = gene_std.head(50).index.tolist()\n",
    "\n",
    "        if top_expr_genes:\n",
    "            plt.figure(figsize=(15, 8))\n",
    "            sns.boxplot(data=expr_original_numeric[top_expr_genes], palette='Set3')\n",
    "            plt.xticks(rotation=90, fontsize=10)\n",
    "            plt.yticks(fontsize=10)\n",
    "            plt.title(\"Top 50 Most Variable Expression Genes\", fontsize=18, weight='bold', color='darkblue')\n",
    "            plt.xlabel(\"Gene\", fontsize=14, color='dimgray')\n",
    "            plt.ylabel(\"Expression Level\", fontsize=14, color='dimgray')\n",
    "            plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, \"multiomics_top_variable_expr_genes.png\"), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(\"Top variable expression genes plot saved.\")\n",
    "        else:\n",
    "            print(\"No top variable expression genes found for plotting.\")\n",
    "    else:\n",
    "        print(\"Original expression data is empty or has no columns. Skipping top variable expression genes plot.\")\n",
    "\n",
    "    # --- Plot: Top Mutated Genes ---\n",
    "    mut_data_cols = [col for col in X_multiomics.columns if col.endswith('_mut')]\n",
    "    if mut_data_cols and len(mut_data_cols) > 0:\n",
    "        mut_numeric = X_multiomics[mut_data_cols].copy()\n",
    "        # Remove '_mut' suffix to get original gene names\n",
    "        mut_numeric.columns = [col.replace('_mut', '') for col in mut_numeric.columns]\n",
    "\n",
    "        top_mutated_genes_counts = mut_numeric.sum().sort_values(ascending=False).head(50)\n",
    "\n",
    "        if not top_mutated_genes_counts.empty:\n",
    "            # Only attempt mapping for plotting if genes are not already Ensembl\n",
    "            # This logic needs to be careful if the columns are original Hugo Symbols\n",
    "            # Let's assume for plotting we want Hugo Symbols if not mapped to Ensembl\n",
    "            \n",
    "            # Check if the columns are Ensembl IDs (from successful mapping)\n",
    "            if all(isinstance(col, str) and col.startswith('ENSG') for col in top_mutated_genes_counts.index[:min(5, len(top_mutated_genes_counts))]):\n",
    "                # If they are Ensembl, try to map them to Hugo Symbols for readability in plot\n",
    "                mg = mygene.MyGeneInfo()\n",
    "                ensembl_ids_to_map = top_mutated_genes_counts.index.tolist()\n",
    "                gene_info = mg.querymany(ensembl_ids_to_map, scopes='ensembl.gene', fields='symbol', species='human', returnall=True)\n",
    "\n",
    "                ensg_to_symbol = {}\n",
    "                for entry in gene_info['out']:\n",
    "                    if 'notfound' not in entry and 'symbol' in entry:\n",
    "                        ensg_to_symbol[entry['query']] = entry['symbol']\n",
    "                \n",
    "                mapped_gene_names = [ensg_to_symbol.get(gene_id, gene_id) for gene_id in top_mutated_genes_counts.index]\n",
    "                top_mutated_genes_counts.index = mapped_gene_names\n",
    "                print(\"Mapped top mutated Ensembl IDs to Hugo Symbols for plotting.\")\n",
    "            else:\n",
    "                print(\"Top mutated genes are not Ensembl IDs. Plotting with their current IDs.\")\n",
    "            \n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.barplot(x=top_mutated_genes_counts.values, y=top_mutated_genes_counts.index, palette='Reds_d')\n",
    "            plt.title(\"Top 50 Most Frequently Mutated Genes (by Sample Count)\", fontsize=18, weight='bold', color='darkblue')\n",
    "            plt.xlabel(\"Number of Mutated Samples\", fontsize=14, color='dimgray')\n",
    "            plt.ylabel(\"Gene\", fontsize=14, color='dimgray')\n",
    "            plt.xticks(fontsize=12)\n",
    "            plt.yticks(fontsize=12)\n",
    "            plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, \"multiomics_top_mutated_genes_barplot.png\"), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(\"Top mutated genes bar plot saved.\")\n",
    "        else:\n",
    "            print(\"No top mutated genes found for plotting.\")\n",
    "    else:\n",
    "        print(\"No mutation data columns found. Skipping top mutated genes plot.\")\n",
    "\n",
    "    # --- NEW PLOT: Top Mutation Frequencies Across Cancer Types Heatmap ---\n",
    "    # This plot requires 'mut_numeric' and 'y_labels' to be available and not empty.\n",
    "    # It also requires a list of top mutated gene IDs (either Ensembl or Hugo for plotting).\n",
    "    \n",
    "    # Re-evaluate conditions for heatmap plotting\n",
    "    if mut_data_cols and len(mut_data_cols) > 0 and not y_labels.empty:\n",
    "        mut_numeric = X_multiomics[[col for col in X_multiomics.columns if col.endswith('_mut')]].copy()\n",
    "        mut_numeric.columns = [col.replace('_mut', '') for col in mut_numeric.columns] # Remove suffix for gene names\n",
    "\n",
    "        top_20_mutated_genes_ensembl_or_original = mut_numeric.sum().sort_values(ascending=False).head(20).index.tolist()\n",
    "\n",
    "        if top_20_mutated_genes_ensembl_or_original:\n",
    "            mut_df_for_heatmap = mut_numeric[top_20_mutated_genes_ensembl_or_original].copy()\n",
    "            mut_df_for_heatmap['CancerType'] = y_labels\n",
    "\n",
    "            mutation_frequency_per_cancer = mut_df_for_heatmap.groupby('CancerType')[top_20_mutated_genes_ensembl_or_original].sum()\n",
    "            \n",
    "            sample_counts = y_labels.value_counts()\n",
    "            mutation_frequency_per_cancer = mutation_frequency_per_cancer.div(sample_counts, axis=0) * 100\n",
    "\n",
    "            # Map gene names for heatmap labels if they are Ensembl IDs\n",
    "            heatmap_gene_names = top_20_mutated_genes_ensembl_or_original\n",
    "            if all(isinstance(col, str) and col.startswith('ENSG') for col in top_20_mutated_genes_ensembl_or_original[:min(5, len(top_20_mutated_genes_ensembl_or_original))]):\n",
    "                mg = mygene.MyGeneInfo()\n",
    "                gene_info_heatmap = mg.querymany(top_20_mutated_genes_ensembl_or_original, scopes='ensembl.gene', fields='symbol', species='human', returnall=True)\n",
    "                ensg_to_symbol_heatmap = {}\n",
    "                for entry in gene_info_heatmap['out']:\n",
    "                    if 'notfound' not in entry and 'symbol' in entry:\n",
    "                        ensg_to_symbol_heatmap[entry['query']] = entry['symbol']\n",
    "                heatmap_gene_names = [ensg_to_symbol_heatmap.get(gene_id, gene_id) for gene_id in top_20_mutated_genes_ensembl_or_original]\n",
    "                print(\"Mapped heatmap gene IDs to Hugo Symbols for readability.\")\n",
    "            else:\n",
    "                print(\"Heatmap gene IDs are not Ensembl. Plotting with their current IDs.\")\n",
    "\n",
    "            mutation_frequency_per_cancer.columns = heatmap_gene_names\n",
    "\n",
    "            plt.figure(figsize=(16, 12))\n",
    "            sns.heatmap(mutation_frequency_per_cancer.T, cmap='YlGnBu', annot=True, fmt=\".1f\", linewidths=.5, linecolor='gray',\n",
    "                        cbar_kws={'label': 'Mutation Frequency (%)'})\n",
    "            plt.title(\"Top 20 Mutation Frequencies Across Cancer Types\", fontsize=18, weight='bold', color='darkblue')\n",
    "            plt.xlabel(\"Cancer Type\", fontsize=14, color='dimgray')\n",
    "            plt.ylabel(\"Gene\", fontsize=14, color='dimgray')\n",
    "            plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "            plt.yticks(fontsize=10)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, \"multiomics_mutation_frequency_heatmap.png\"), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(\"Mutation frequency heatmap saved.\")\n",
    "        else:\n",
    "            print(\"No top 20 mutated genes to plot in heatmap.\")\n",
    "    else:\n",
    "        print(\"Insufficient mutation data or labels for mutation frequency heatmap.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Multi-Omics Machine Learning & Dimensionality Reduction\n",
    "# ==============================================================================\n",
    "\n",
    "def perform_multi_omics_ml(X_multiomics, y_labels):\n",
    "    \"\"\"\n",
    "    Performs dimensionality reduction and trains a Random Forest classifier\n",
    "    on multi-omics data.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Multi-Omics Machine Learning & Dimensionality Reduction ---\")\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y_labels)\n",
    "    class_names = le.classes_\n",
    "    print(f\"Encoded {len(class_names)} cancer types.\")\n",
    "\n",
    "    n_components_pca = min(50, X_multiomics.shape[1])\n",
    "    if X_multiomics.shape[1] > 1 and n_components_pca > 1:\n",
    "        pca = PCA(n_components=n_components_pca, random_state=42)\n",
    "        X_pca = pca.fit_transform(X_multiomics)\n",
    "        print(f\"PCA reduced data shape: {X_pca.shape}\")\n",
    "        print(f\"PCA explained variance ratio (first {n_components_pca} components): {np.sum(pca.explained_variance_ratio_):.2f}\")\n",
    "    else:\n",
    "        print(\"Not enough features for PCA. Skipping PCA.\")\n",
    "        X_pca = X_multiomics.values\n",
    "\n",
    "    if X_pca.shape[1] >= 2:\n",
    "        reducer = umap.UMAP(n_components=2, random_state=42, metric='euclidean')\n",
    "        X_umap = reducer.fit_transform(X_pca)\n",
    "        print(f\"UMAP reduced data shape: {X_umap.shape}\")\n",
    "\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.scatterplot(x=X_umap[:, 0], y=X_umap[:, 1], hue=y_labels, palette='tab20',\n",
    "                        s=80, alpha=0.85, edgecolor='black', linewidth=0.7)\n",
    "        plt.title(\"UMAP Projection of Multi-Omics Data (Expression + Mutation)\", fontsize=18, weight='bold', color='darkblue')\n",
    "        plt.xlabel(\"UMAP Component 1\", fontsize=14, color='dimgray')\n",
    "        plt.ylabel(\"UMAP Component 2\", fontsize=14, color='dimgray')\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.legend(title=\"Cancer Type\", bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, fontsize='small', title_fontsize=12)\n",
    "        plt.grid(True, linestyle=':', alpha=0.5)\n",
    "        plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "        plt.savefig(os.path.join(output_dir, \"multiomics_umap_projection.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"UMAP projection plot saved.\")\n",
    "    else:\n",
    "        print(\"Not enough dimensions after PCA for UMAP. Skipping UMAP plot.\")\n",
    "\n",
    "    if X_multiomics.shape[0] > 1 and X_multiomics.shape[1] > 0 and len(np.unique(y_encoded)) > 1:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_multiomics, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
    "        )\n",
    "        print(f\"Training data shape: {X_train.shape}, Test data shape: {X_test.shape}\")\n",
    "\n",
    "        clf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1, verbose=1)\n",
    "        print(\"Training Random Forest Classifier...\")\n",
    "        clf.fit(X_train, y_train)\n",
    "        print(\"Random Forest Classifier training complete.\")\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        print(\"\\n📊 Classification Report (Multi-Omics Random Forest):\")\n",
    "        print(classification_report(y_test, y_pred, target_names=class_names, zero_division=0))\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred, normalize='true', labels=range(len(class_names)))\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(16, 16))\n",
    "        disp.plot(cmap='Blues', ax=ax, colorbar=False, xticks_rotation='vertical')\n",
    "        plt.title(\"Normalized Confusion Matrix (Multi-Omics Random Forest)\", fontsize=18, weight='bold', color='darkblue')\n",
    "        plt.xlabel(\"Predicted Label\", fontsize=14, color='dimgray')\n",
    "        plt.ylabel(\"True Label\", fontsize=14, color='dimgray')\n",
    "        plt.xticks(fontsize=10)\n",
    "        plt.yticks(fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, \"multiomics_confusion_matrix.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"Normalized Confusion Matrix plot saved.\")\n",
    "    else:\n",
    "        print(\"Insufficient data or classes for Machine Learning. Skipping model training and evaluation.\")\n",
    "\n",
    "    print(\"\\nMulti-Omics Machine Learning & Dimensionality Reduction complete.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Main Execution Block for Multi-Omics Analysis\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Multi-Omics Integration & Analysis Phase...\")\n",
    "\n",
    "    try:\n",
    "        X_multiomics_features, y_multiomics_labels, pheno_aligned_df, expr_original_aligned_df = \\\n",
    "            load_and_preprocess_multi_omics_data(expr_path, maf_path, pheno_path, mart_path, target_column)\n",
    "\n",
    "        if X_multiomics_features is None or X_multiomics_features.empty or y_multiomics_labels.empty:\n",
    "            print(\"Multi-omics data preparation failed or resulted in empty data. Exiting.\")\n",
    "        else:\n",
    "            perform_multi_omics_eda(X_multiomics_features, y_multiomics_labels, pheno_aligned_df, expr_original_aligned_df)\n",
    "            perform_multi_omics_ml(X_multiomics_features, y_multiomics_labels)\n",
    "\n",
    "        print(\"\\nMulti-Omics Integration & Analysis Phase complete.\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"\\nOne or more required libraries (mygene, umap-learn) not found. Please install them.\")\n",
    "        print(\"Skipping Multi-Omics Integration & Analysis.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during Multi-Omics Analysis: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b75fea",
   "metadata": {},
   "source": [
    "# ==============================================================================\n",
    "# Phase 1.6: Advanced Multi-Omics Integration & Machine Learning\n",
    "# This script integrates gene expression, mutation, copy number variation (CNV),\n",
    "# and miRNA expression data. It performs comprehensive EDA, dimensionality\n",
    "# reduction (PCA, UMAP), and trains multiple machine learning models for\n",
    "# cancer type classification using the integrated multi-omics data.\n",
    "#\n",
    "# Before running:\n",
    "# 1. Ensure you have necessary libraries installed:\n",
    "#    pip install pandas numpy matplotlib seaborn scikit-learn umap-learn mygene lightgbm\n",
    "# 2. Ensure you have the following data files in the specified 'data_dir':\n",
    "#    - EB++AdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.xena (Gene Expression)\n",
    "#    - mc3.v0.2.8.PUBLIC.maf (Somatic Mutation)\n",
    "#    - TCGA_phenotype_denseDataOnlyDownload.tsv (Phenotype)\n",
    "#    - mart_export.txt (BioMart gene mapping - if available)\n",
    "#    - pancanMiRs_EBadjOnProtocolPlatformWithoutRepsWithUnCorrectMiRs_08_04_16.xena (miRNA Expression)\n",
    "#    - TCGA.PANCAN.sampleMap_Gistic2_CopyNumber_Gistic2_all_data_by_genes (Copy Number Variation)\n",
    "#\n",
    "# NOTE: Methylation data (e.g., beta values) is NOT included as only a probe map\n",
    "#       was provided. If you have the actual methylation data, please specify\n",
    "#       its filename for inclusion in a future update.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07b079b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Multi-Omics Workflow (EDA Focus)...\n",
      "\n",
      "==================================================\n",
      "EXECUTING PHASE 1: DATA PREPARATION (Advanced Multi-Omics)\n",
      "==================================================\n",
      "\n",
      "--- Multi-Omics Data Loading and Preprocessing (Advanced) ---\n",
      "Attempting to load methylation data in chunks from: C:\\Users\\shrav\\Desktop\\PYTHON\\Cancer\\Pan Cancer Analysis\\jhu-usc.edu_PANCAN_HumanMethylation450.betaValue_whitelisted.tsv.synapse_download_5096262.xena\n",
      "  Processed chunk 1, current chunk shape: (50000, 9664)\n",
      "  Processed chunk 2, current chunk shape: (50000, 9664)\n",
      "  Processed chunk 3, current chunk shape: (50000, 9664)\n",
      "  Processed chunk 4, current chunk shape: (50000, 9664)\n",
      "  Processed chunk 5, current chunk shape: (50000, 9664)\n",
      "  Processed chunk 6, current chunk shape: (50000, 9664)\n",
      "  Processed chunk 7, current chunk shape: (50000, 9664)\n",
      "  Processed chunk 8, current chunk shape: (46065, 9664)\n",
      "Identified top 1000 most variable methylation probes from 9664 total probes.\n",
      "Methylation data re-loaded with only top 1000 variable probes. Final shape: (396065, 1000)\n",
      "Raw expression, mutation (MAF), phenotype, miRNA, CNV, RPPA, and Methylation data loaded.\n",
      "\n",
      "--- Debug: Phenotype DataFrame Columns ---\n",
      "['sample', 'sample_type_id', 'sample_type', '_primary_disease']\n",
      "------------------------------------------\n",
      "BioMart mapping file 'mart_export.txt' loaded.\n",
      "BioMart columns: ['Gene stable ID', 'Gene stable ID version', 'Transcript stable ID', 'Transcript stable ID version', 'Gene name', 'NCBI gene (formerly Entrezgene) ID', 'EntrezGene transcript name ID']\n",
      "Found 41164 Hugo Symbol to Ensembl mappings in BioMart file.\n",
      "Initial Gene Expression data shape (samples x genes): (11069, 20531)\n",
      "First 5 gene expression IDs (before mapping): ['100130426', '100133144', '100134869', '10357', '10431']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input sequence provided is already in string format. No operation performed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioMart mapping for Gene Expression failed or not available. Attempting mygene mapping (Entrez/Symbol to Ensembl)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19 input query terms found dup hits:\t[('ABCA11P', 2), ('ABCA17P', 2), ('ABCC13', 2), ('ABCC6P1', 2), ('ABCC6P2', 3), ('ADAM21P1', 2), ('A\n",
      "82 input query terms found no hit:\t['100130426', '100133144', '10431', '136542', '317712', '391343', '553137', '57714', '645851', '6529\n",
      "Input sequence provided is already in string format. No operation performed\n",
      "6 input query terms found dup hits:\t[('ATP6AP1L', 2), ('ATP8B5P', 2), ('BAGE2', 2), ('BIRC8', 2), ('BMS1P4', 2), ('BRD7P3', 2)]\n",
      "338 input query terms found no hit:\t['ARMC4', 'ARNTL2', 'ARNTL', 'ARPM1', 'ARSE', 'ASAM', 'ASAP1IT1', 'ASFMR1', 'ASNA1', 'ATHL1', 'ATP5A\n",
      "Input sequence provided is already in string format. No operation performed\n",
      "1 input query terms found dup hits:\t[('C3P1', 2)]\n",
      "668 input query terms found no hit:\t['C17orf103', 'C17orf104', 'C17orf105', 'C17orf106', 'C17orf108', 'C17orf28', 'C17orf37', 'C17orf39'\n",
      "Input sequence provided is already in string format. No operation performed\n",
      "10 input query terms found dup hits:\t[('CAST', 2), ('CATSPER2P1', 2), ('CCDC144NL', 2), ('CCT6P1', 2), ('CDR1', 2), ('CEACAM22P', 2), ('C\n",
      "97 input query terms found no hit:\t['CBARA1', 'CBWD1', 'CBWD2', 'CBWD3', 'CBWD5', 'CBWD6', 'CCBL1', 'CCBL2', 'CCBP2', 'CCDC101', 'CCDC1\n",
      "Input sequence provided is already in string format. No operation performed\n",
      "10 input query terms found dup hits:\t[('COL6A4P2', 2), ('CXADRP2', 3), ('CXADRP3', 2), ('CXCR2P1', 2), ('CYP4Z2P', 2), ('DDX11L2', 2), ('\n",
      "95 input query terms found no hit:\t['CNTD2', 'COBRA1', 'COL29A1', 'COL4A3BP', 'COPG', 'COX4NB', 'CP110', 'CPSF3L', 'CRAMP1L', 'CRIPAK',\n",
      "Input sequence provided is already in string format. No operation performed\n",
      "11 input query terms found dup hits:\t[('DNAJB3', 2), ('DNM1P35', 2), ('DPRXP4', 2), ('DPY19L2P1', 2), ('DPY19L2P2', 3), ('DPY19L2P4', 2),\n",
      "125 input query terms found no hit:\t['DLX6AS', 'DOM3Z', 'DOPEY1', 'DOPEY2', 'DPCR1', 'DPH3B', 'DSCR3', 'DSCR6', 'DULLARD', 'DUPD1', 'DUS\n",
      "Input sequence provided is already in string format. No operation performed\n",
      "19 input query terms found dup hits:\t[('FAM182B', 2), ('FAM197Y2', 3), ('FAM27B', 2), ('FAM41AY1', 2), ('FAM74A1', 2), ('FAM74A3', 2), ('\n",
      "214 input query terms found no hit:\t['FAM183A', 'FAM183B', 'FAM188A', 'FAM188B', 'FAM189A1', 'FAM189A2', 'FAM189B', 'FAM18A', 'FAM18B2',\n",
      "Input sequence provided is already in string format. No operation performed\n",
      "19 input query terms found dup hits:\t[('GLRA4', 2), ('GLYCAM1', 2), ('GNRHR2', 2), ('GSTM2P1', 2), ('GTF2H2B', 2), ('GTF2IP1', 2), ('GUCY\n",
      "179 input query terms found no hit:\t['GIF', 'GIYD2', 'GK3P', 'GLT25D1', 'GLT25D2', 'GLTPD1', 'GLTSCR1', 'GLTSCR2', 'GMCL1L', 'GNASAS', '\n",
      "Input sequence provided is already in string format. No operation performed\n",
      "7 input query terms found dup hits:\t[('HSD17B7P2', 2), ('HSP90AB4P', 2), ('HTR7P1', 2), ('ID2B', 2), ('IFITM4P', 7), ('INGX', 2), ('ISCA\n",
      "137 input query terms found no hit:\t['HOXA11AS', 'HPVC1', 'HRASLS2', 'HRASLS5', 'HRASLS', 'HRNBP3', 'HRSP12', 'HSFYL1', 'HSN2', 'HSPB11'\n",
      "Input sequence provided is already in string format. No operation performed\n",
      "2 input query terms found dup hits:\t[('KIR3DX1', 5), ('KLKP1', 2)]\n",
      "358 input query terms found no hit:\t['KIAA1468', 'KIAA1486', 'KIAA1522', 'KIAA1524', 'KIAA1529', 'KIAA1530', 'KIAA1539', 'KIAA1543', 'KI\n",
      "Input sequence provided is already in string format. No operation performed\n",
      "8 input query terms found dup hits:\t[('LPAL2', 2), ('LRP5L', 2), ('LY6G6E', 8), ('LYPLA2P1', 6), ('MALAT1', 2), ('MBL1P', 2), ('MEG8', 2\n",
      "267 input query terms found no hit:\t['LOC399815', 'LOC399959', 'LOC400027', 'LOC400043', 'LOC400657', 'LOC400696', 'LOC400752', 'LOC4007\n",
      "Input sequence provided is already in string format. No operation performed\n",
      "17 input query terms found dup hits:\t[('MMD2', 2), ('MMP23A', 2), ('MORF4', 2), ('MRS2P2', 2), ('MST1P2', 2), ('MSTO2P', 2), ('MSX2P1', 2\n",
      "117 input query terms found no hit:\t['MOBKL1A', 'MOBKL1B', 'MOBKL2A', 'MOBKL2B', 'MOBKL2C', 'MOBKL3', 'MOSC1', 'MOSC2', 'MPP5', 'MPP6', \n",
      "Input sequence provided is already in string format. No operation performed\n",
      "19 input query terms found dup hits:\t[('NPY6R', 2), ('NRADDP', 2), ('NSUN5P2', 2), ('NUDT9P1', 2), ('NXF4', 2), ('NXF5', 2), ('OR10J3', 3\n",
      "50 input query terms found no hit:\t['NOTCH2NL', 'NOV', 'NPIP', 'NPIPL3', 'NRD1', 'NT5C3', 'NT5C3L', 'NUDT16P1', 'NUPL1', 'NUPL2', 'OBFC\n",
      "Input sequence provided is already in string format. No operation performed\n",
      "22 input query terms found dup hits:\t[('PCDHB19P', 3), ('PCDHGB8P', 2), ('PCNAP1', 2), ('PDZK1P1', 2), ('PGM5P2', 2), ('PI4KAP1', 2), ('P\n",
      "64 input query terms found no hit:\t['PCDHB17', 'PCDHB18', 'PCDP1', 'PCNX', 'PCNXL2', 'PCNXL3', 'PDDC1', 'PDIA3P', 'PDXDC2', 'PDZD3', 'P\n",
      "Input sequence provided is already in string format. No operation performed\n",
      "11 input query terms found dup hits:\t[('PRKY', 2), ('PRSS30P', 3), ('PSORS1C3', 8), ('PTENP1', 2), ('PTPRVP', 2), ('PYY2', 2), ('RAET1K',\n",
      "60 input query terms found no hit:\t['PRDXDD1P', 'PRHOXNB', 'PRIC285', 'PRKCDBP', 'PRKRIR', 'PRMT10', 'PRO0611', 'PRO0628', 'PRO1768', '\n",
      "Input sequence provided is already in string format. No operation performed\n",
      "34 input query terms found dup hits:\t[('RNF126P1', 2), ('RNF138P1', 2), ('RNF5P1', 2), ('RP9P', 2), ('RPL13AP17', 2), ('RPL13AP3', 2), ('\n",
      "93 input query terms found no hit:\t['RLTPR', 'RNASEN', 'RNF160', 'RNF165', 'RNF216L', 'RNF219', 'RNMTL1', 'RNU5E', 'ROBLD3', 'ROD1', 'R\n",
      "Input sequence provided is already in string format. No operation performed\n",
      "29 input query terms found dup hits:\t[('SIGLEC5', 2), ('SLC6A10P', 2), ('SLC7A5P2', 2), ('SNHG6', 2), ('SNHG8', 2), ('SNHG9', 3), ('SNORA\n",
      "36 input query terms found no hit:\t['SIGLECP3', 'SILV', 'SIP1', 'SKINTL', 'SKIV2L2', 'SKIV2L', 'SLC22A18', 'SLC22A18AS', 'SLC22A20', 'S\n",
      "Input sequence provided is already in string format. No operation performed\n",
      "16 input query terms found dup hits:\t[('SPDYE7P', 2), ('SPRR2C', 2), ('STAG3L1', 2), ('STAG3L2', 2), ('STAG3L3', 2), ('STAG3L4', 2), ('SU\n",
      "58 input query terms found no hit:\t['SOLH', 'SOX2OT', 'SPANXB2', 'SPANXE', 'SPATA5', 'SPATA5L1', 'SPDYE8P', 'SPERT', 'SPG20', 'SPHAR', \n",
      "Input sequence provided is already in string format. No operation performed\n",
      "17 input query terms found dup hits:\t[('TMED10P1', 2), ('TMEM191A', 2), ('TOP1P1', 2), ('TPI1P2', 2), ('TPI1P3', 2), ('TPRXL', 2), ('TPTE\n",
      "79 input query terms found no hit:\t['TIAF1', 'TIMM16', 'TM7SF4', 'TMCO7', 'TMEM110', 'TMEM111', 'TMEM133', 'TMEM136', 'TMEM146', 'TMEM1\n",
      "Input sequence provided is already in string format. No operation performed\n",
      "12 input query terms found dup hits:\t[('UBE2MP1', 2), ('UBE2Q2P1', 3), ('UOX', 2), ('VENTXP1', 2), ('VENTXP7', 2), ('WASH2P', 2), ('WASH3\n",
      "75 input query terms found no hit:\t['TXNRD3IT1', 'UBE2CBP', 'UCKL1AS', 'UFD1L', 'UHRF1BP1', 'UHRF1BP1L', 'UPF0639', 'UPK3BL', 'UQCC', '\n",
      "Input sequence provided is already in string format. No operation performed\n",
      "6 input query terms found dup hits:\t[('ZNF204P', 2), ('ZNF286B', 2), ('ZNF702P', 2), ('ZNF781', 2), ('ZNF876P', 2), ('ZSCAN12P1', 2)]\n",
      "31 input query terms found no hit:\t['ZNF192', 'ZNF193', 'ZNF238', 'ZNF252', 'ZNF259', 'ZNF271', 'ZNF295', 'ZNF321', 'ZNF322A', 'ZNF322B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully mapped 0 out of 20531 terms for scopes '['entrezgene', 'symbol']'.\n",
      "Warning: Gene Expression ID mapping failed completely. Proceeding with original gene IDs. This may affect gene-level alignment.\n",
      "Gene Expression data shape (original IDs): (11069, 20531)\n",
      "Mutation records after dropping NaNs and duplicates: (2378187, 2)\n",
      "Attempting BioMart mapping for Mutation data (Hugo Symbol to Ensembl)...\n",
      "BioMart mapped 18862 mutation genes.\n",
      "Mutation records after mapping Hugo_Symbol to Ensembl: (2228644, 3)\n",
      "Binary mutation matrix shape: (9104, 18862)\n",
      "Initial miRNA Expression data shape (samples x miRNAs): (10824, 743)\n",
      "First 5 miRNA IDs: ['hsa-let-7a-2-3p', 'hsa-let-7a-3p', 'hsa-let-7a-5p', 'hsa-let-7b-3p', 'hsa-let-7b-5p']\n",
      "Initial CNV data shape (samples x genes): (10845, 24776)\n",
      "First 5 CNV gene IDs: ['ACAP3', 'ACTRT2', 'AGRN', 'ANKRD65', 'ATAD3A']\n",
      "Attempting BioMart mapping for CNV data (Hugo Symbol to Ensembl)...\n",
      "BioMart mapped 21521 CNV genes.\n",
      "CNV data after mapping to Ensembl: (10845, 21521)\n",
      "Initial RPPA data shape (samples x proteins): (7754, 258)\n",
      "First 5 RPPA protein IDs: ['X1433EPSILON', 'X4EBP1', 'X4EBP1_pS65', 'X4EBP1_pT37T46', 'X53BP1']\n",
      "Initial Methylation data shape (samples x probes): (1000, 396065)\n",
      "First 5 Methylation probe IDs: ['cg00000029', 'cg00000165', 'cg00000236', 'cg00000289', 'cg00000292']\n",
      "Common samples across all omics datasets: 568\n",
      "Gene-level alignment skipped (either not all Ensembl IDs or one/more gene-level omics data are empty/unmapped). Features will be concatenated as-is.\n",
      "Aligned gene expression data shape: (568, 20531)\n",
      "Aligned mutation data shape: (568, 18862)\n",
      "Aligned miRNA expression data shape: (568, 743)\n",
      "Aligned CNV data shape: (568, 21521)\n",
      "Aligned RPPA data shape: (568, 258)\n",
      "Aligned Methylation data shape: (568, 396065)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\blocks.py:393: RuntimeWarning: divide by zero encountered in log2\n",
      "  result = func(self.values, **kwargs)\n",
      "C:\\Users\\shrav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\blocks.py:393: RuntimeWarning: invalid value encountered in log2\n",
      "  result = func(self.values, **kwargs)\n",
      "C:\\Users\\shrav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene Expression data log2 transformed.\n",
      "Removed 302 zero-variance features from Gene Expression data.\n",
      "Gene Expression data imputed. Missing values: 0\n",
      "Removed 6025 zero-variance features from Mutation data.\n",
      "Mutation data imputed. Missing values: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\blocks.py:393: RuntimeWarning: invalid value encountered in log2\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miRNA Expression data log2 transformed.\n",
      "Removed 1 zero-variance features from miRNA Expression data.\n",
      "miRNA Expression data imputed. Missing values: 0\n",
      "CNV data imputed. Missing values: 0\n",
      "RPPA data imputed. Missing values: 0\n",
      "Methylation data imputed. Missing values: 0\n",
      "All omics data imputed and scaled (if columns available).\n",
      "Final Multi-omics feature matrix shape: (568, 451633)\n",
      "Target labels shape: (568,)\n",
      "X_multiomics_features successfully exported to: multi_omics_results_advanced\\X_multiomics_features.pkl\n",
      "y_multiomics_labels successfully exported to: multi_omics_results_advanced\\y_multiomics_labels.pkl\n",
      "pheno_aligned_df successfully exported to: multi_omics_results_advanced\\pheno_aligned_df.pkl\n",
      "\n",
      "Processed multi-omics data saved to .pkl files for subsequent phases.\n",
      "\n",
      "==================================================\n",
      "EXECUTING PHASE 2: EDA & VISUALIZATION\n",
      "==================================================\n",
      "\n",
      "--- Detailed EDA of Loaded Multi-Omics Data ---\n",
      "\n",
      "Original Multi-omics Feature Matrix (X_multiomics_features):\n",
      "Shape: (568, 451633)\n",
      "First 5 rows:\n",
      "                 100130426_expr  100133144_expr  100134869_expr  10357_expr  \\\n",
      "TCGA-2A-A8VV-01       -0.215095        1.084036        0.805305   -0.139193   \n",
      "TCGA-2Z-A9J3-01       -0.215095        0.224176       -0.364058   -0.818858   \n",
      "TCGA-2Z-A9J6-01       -0.215095        0.615141        1.012674   -0.832402   \n",
      "TCGA-2Z-A9J7-01       -0.215095        0.553621        0.264220   -1.375718   \n",
      "TCGA-2Z-A9JE-01       -0.215095        0.318354        0.256579   -1.149168   \n",
      "\n",
      "                 10431_expr  136542_expr  155060_expr  26823_expr  \\\n",
      "TCGA-2A-A8VV-01    0.304891    -0.041996     0.089764   -1.276634   \n",
      "TCGA-2Z-A9J3-01    0.224305    -0.041996     1.274322   -0.244713   \n",
      "TCGA-2Z-A9J6-01    0.505063    -0.041996     0.345051    0.648401   \n",
      "TCGA-2Z-A9J7-01    1.838354    -0.041996     1.629512    0.045982   \n",
      "TCGA-2Z-A9JE-01    2.173173    -0.041996     0.089764   -1.276634   \n",
      "\n",
      "                 280660_expr  340602_expr  ...  rs7746156_meth  rs798149_meth  \\\n",
      "TCGA-2A-A8VV-01    -0.375007     0.605429  ...       -0.199857       1.181680   \n",
      "TCGA-2Z-A9J3-01    -0.375007    -1.305524  ...       -1.418252      -1.050050   \n",
      "TCGA-2Z-A9J6-01    -0.375007    -1.305524  ...       -0.108891       1.150687   \n",
      "TCGA-2Z-A9J7-01    -0.375007    -0.405900  ...        1.215360       1.155248   \n",
      "TCGA-2Z-A9JE-01    -0.375007    -1.305524  ...        1.227256       1.179041   \n",
      "\n",
      "                 rs845016_meth  rs877309_meth  rs9292570_meth  rs9363764_meth  \\\n",
      "TCGA-2A-A8VV-01      -1.277169      -1.210063       -1.640642        1.096298   \n",
      "TCGA-2Z-A9J3-01      -0.004293       1.376907        0.327942        0.958259   \n",
      "TCGA-2Z-A9J6-01      -1.190556       0.190632       -0.226713        1.028429   \n",
      "TCGA-2Z-A9J7-01      -0.047719       0.137184       -0.303416       -1.552307   \n",
      "TCGA-2Z-A9JE-01       1.411558       0.175603       -0.214041        1.070237   \n",
      "\n",
      "                 rs939290_meth  rs951295_meth  rs966367_meth  rs9839873_meth  \n",
      "TCGA-2A-A8VV-01       1.177536       0.168477       1.428796        0.628206  \n",
      "TCGA-2Z-A9J3-01      -0.106571       0.223820      -0.085130        0.414076  \n",
      "TCGA-2Z-A9J6-01       0.260937       0.169126      -0.099591       -1.269578  \n",
      "TCGA-2Z-A9J7-01       1.173022      -1.353195       1.341533       -0.664372  \n",
      "TCGA-2Z-A9JE-01      -1.513267       0.175400       1.362315        0.581429  \n",
      "\n",
      "[5 rows x 451633 columns]\n",
      "\n",
      "Descriptive statistics for X_multiomics_features (first 50 columns):\n",
      "       100130426_expr  100133144_expr  100134869_expr    10357_expr  \\\n",
      "count    5.680000e+02    5.680000e+02    5.680000e+02  5.680000e+02   \n",
      "mean     1.563694e-17    1.501147e-16   -5.879491e-16 -7.505733e-17   \n",
      "std      1.000881e+00    1.000881e+00    1.000881e+00  1.000881e+00   \n",
      "min     -1.468675e+00   -2.782754e+00   -4.616723e+00 -5.944519e+00   \n",
      "25%     -2.150947e-01   -3.566681e-01   -5.174082e-01 -7.111810e-01   \n",
      "50%     -2.150947e-01    2.421349e-01    1.556132e-01  1.040127e-01   \n",
      "75%     -2.150947e-01    6.725658e-01    6.730173e-01  7.418856e-01   \n",
      "max      1.024474e+01    1.454948e+00    1.805484e+00  2.237909e+00   \n",
      "\n",
      "         10431_expr   136542_expr   155060_expr    26823_expr   280660_expr  \\\n",
      "count  5.680000e+02  5.680000e+02  5.680000e+02  5.680000e+02  5.680000e+02   \n",
      "mean   8.800472e-15  6.254778e-18  4.378344e-16 -7.505733e-17 -3.752867e-17   \n",
      "std    1.000881e+00  1.000881e+00  1.000881e+00  1.000881e+00  1.000881e+00   \n",
      "min   -3.604847e+00 -4.199605e-02 -3.901723e+00 -1.276634e+00 -3.750073e-01   \n",
      "25%   -6.193294e-01 -4.199605e-02 -7.023562e-01 -1.276634e+00 -3.750073e-01   \n",
      "50%   -3.967385e-02 -4.199605e-02  8.976433e-02 -6.255925e-03 -3.750073e-01   \n",
      "75%    6.242907e-01 -4.199605e-02  6.567203e-01  8.265494e-01 -3.750073e-01   \n",
      "max    4.648484e+00  2.381176e+01  3.142499e+00  2.538502e+00  5.262384e+00   \n",
      "\n",
      "        340602_expr  ...  AADACL3_expr  AADACL4_expr    AADAT_expr  \\\n",
      "count  5.680000e+02  ...  5.680000e+02  5.680000e+02  5.680000e+02   \n",
      "mean   1.876433e-16  ... -1.094586e-17  3.127389e-17 -2.251720e-15   \n",
      "std    1.000881e+00  ...  1.000881e+00  1.000881e+00  1.000881e+00   \n",
      "min   -1.463035e+00  ... -2.298520e+00 -4.101693e-01 -6.049401e+00   \n",
      "25%   -1.305524e+00  ... -2.318274e-01 -4.101693e-01 -5.021947e-01   \n",
      "50%    1.139298e-01  ... -2.318274e-01 -4.101693e-01  7.817489e-02   \n",
      "75%    8.378220e-01  ... -2.318274e-01 -4.101693e-01  7.437884e-01   \n",
      "max    2.650746e+00  ...  9.437835e+00  7.344411e+00  1.853875e+00   \n",
      "\n",
      "         AAGAB_expr     AAK1_expr     AAMP_expr    AANAT_expr     AARS_expr  \\\n",
      "count  5.680000e+02  5.680000e+02  5.680000e+02  5.680000e+02  5.680000e+02   \n",
      "mean   1.737264e-15  3.415109e-15  1.169643e-14  1.735701e-16 -3.089860e-15   \n",
      "std    1.000881e+00  1.000881e+00  1.000881e+00  1.000881e+00  1.000881e+00   \n",
      "min   -3.400579e+00 -4.410495e+00 -5.924493e+00 -3.342208e+00 -3.241946e+00   \n",
      "25%   -6.713333e-01 -5.185755e-01 -5.217858e-01 -6.541448e-01 -7.461774e-01   \n",
      "50%    1.657426e-01 -2.050103e-02  1.598921e-01  9.655943e-02 -1.165190e-01   \n",
      "75%    6.923667e-01  5.498597e-01  6.493011e-01  7.536110e-01  8.301214e-01   \n",
      "max    2.760437e+00  2.941518e+00  3.530907e+00  2.548505e+00  2.493308e+00   \n",
      "\n",
      "         AARS2_expr   AARSD1_expr  \n",
      "count  5.680000e+02  5.680000e+02  \n",
      "mean   5.304051e-15  3.777886e-15  \n",
      "std    1.000881e+00  1.000881e+00  \n",
      "min   -2.309778e+00 -3.127451e+00  \n",
      "25%   -6.634286e-01 -4.576230e-01  \n",
      "50%   -8.934524e-02 -5.350972e-02  \n",
      "75%    5.080690e-01  5.312444e-01  \n",
      "max    6.209412e+00  3.057904e+00  \n",
      "\n",
      "[8 rows x 50 columns]\n",
      "\n",
      "Missing values in X_multiomics_features:\n",
      "0\n",
      "\n",
      "Original Target Labels (y_multiomics_labels):\n",
      "Shape: (568,)\n",
      "Value counts:\n",
      "_primary_disease\n",
      "thyroid carcinoma                        172\n",
      "brain lower grade glioma                 119\n",
      "kidney papillary cell carcinoma          105\n",
      "thymoma                                   35\n",
      "kidney chromophobe                        32\n",
      "sarcoma                                   24\n",
      "adrenocortical cancer                     14\n",
      "prostate adenocarcinoma                   11\n",
      "uterine corpus endometrioid carcinoma      9\n",
      "uveal melanoma                             9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Aligned Phenotype Data (pheno_aligned_df):\n",
      "Shape: (568, 3)\n",
      "First 5 rows:\n",
      "                 sample_type_id    sample_type  \\\n",
      "TCGA-2A-A8VV-01             1.0  Primary Tumor   \n",
      "TCGA-2Z-A9J3-01             1.0  Primary Tumor   \n",
      "TCGA-2Z-A9J6-01             1.0  Primary Tumor   \n",
      "TCGA-2Z-A9J7-01             1.0  Primary Tumor   \n",
      "TCGA-2Z-A9JE-01             1.0  Primary Tumor   \n",
      "\n",
      "                                _primary_disease  \n",
      "TCGA-2A-A8VV-01          prostate adenocarcinoma  \n",
      "TCGA-2Z-A9J3-01  kidney papillary cell carcinoma  \n",
      "TCGA-2Z-A9J6-01  kidney papillary cell carcinoma  \n",
      "TCGA-2Z-A9J7-01  kidney papillary cell carcinoma  \n",
      "TCGA-2Z-A9JE-01  kidney papillary cell carcinoma  \n",
      "\n",
      "Descriptive statistics for pheno_aligned_df:\n",
      "        sample_type_id    sample_type   _primary_disease\n",
      "count       568.000000            568                568\n",
      "unique             NaN              2                 20\n",
      "top                NaN  Primary Tumor  thyroid carcinoma\n",
      "freq               NaN            560                172\n",
      "mean          1.070423            NaN                NaN\n",
      "std           0.589717            NaN                NaN\n",
      "min           1.000000            NaN                NaN\n",
      "25%           1.000000            NaN                NaN\n",
      "50%           1.000000            NaN                NaN\n",
      "75%           1.000000            NaN                NaN\n",
      "max           6.000000            NaN                NaN\n",
      "\n",
      "Missing values in pheno_aligned_df:\n",
      "sample_type_id      0\n",
      "sample_type         0\n",
      "_primary_disease    0\n",
      "dtype: int64\n",
      "\n",
      "--- Detailed EDA of Filtered Data (after removing single-sample classes) ---\n",
      "Filtered Multi-omics Feature Matrix (X_filtered) shape: (565, 451633)\n",
      "First 5 rows of X_filtered:\n",
      "                 100130426_expr  100133144_expr  100134869_expr  10357_expr  \\\n",
      "TCGA-2A-A8VV-01       -0.215095        1.084036        0.805305   -0.139193   \n",
      "TCGA-2Z-A9J3-01       -0.215095        0.224176       -0.364058   -0.818858   \n",
      "TCGA-2Z-A9J6-01       -0.215095        0.615141        1.012674   -0.832402   \n",
      "TCGA-2Z-A9J7-01       -0.215095        0.553621        0.264220   -1.375718   \n",
      "TCGA-2Z-A9JE-01       -0.215095        0.318354        0.256579   -1.149168   \n",
      "\n",
      "                 10431_expr  136542_expr  155060_expr  26823_expr  \\\n",
      "TCGA-2A-A8VV-01    0.304891    -0.041996     0.089764   -1.276634   \n",
      "TCGA-2Z-A9J3-01    0.224305    -0.041996     1.274322   -0.244713   \n",
      "TCGA-2Z-A9J6-01    0.505063    -0.041996     0.345051    0.648401   \n",
      "TCGA-2Z-A9J7-01    1.838354    -0.041996     1.629512    0.045982   \n",
      "TCGA-2Z-A9JE-01    2.173173    -0.041996     0.089764   -1.276634   \n",
      "\n",
      "                 280660_expr  340602_expr  ...  rs7746156_meth  rs798149_meth  \\\n",
      "TCGA-2A-A8VV-01    -0.375007     0.605429  ...       -0.199857       1.181680   \n",
      "TCGA-2Z-A9J3-01    -0.375007    -1.305524  ...       -1.418252      -1.050050   \n",
      "TCGA-2Z-A9J6-01    -0.375007    -1.305524  ...       -0.108891       1.150687   \n",
      "TCGA-2Z-A9J7-01    -0.375007    -0.405900  ...        1.215360       1.155248   \n",
      "TCGA-2Z-A9JE-01    -0.375007    -1.305524  ...        1.227256       1.179041   \n",
      "\n",
      "                 rs845016_meth  rs877309_meth  rs9292570_meth  rs9363764_meth  \\\n",
      "TCGA-2A-A8VV-01      -1.277169      -1.210063       -1.640642        1.096298   \n",
      "TCGA-2Z-A9J3-01      -0.004293       1.376907        0.327942        0.958259   \n",
      "TCGA-2Z-A9J6-01      -1.190556       0.190632       -0.226713        1.028429   \n",
      "TCGA-2Z-A9J7-01      -0.047719       0.137184       -0.303416       -1.552307   \n",
      "TCGA-2Z-A9JE-01       1.411558       0.175603       -0.214041        1.070237   \n",
      "\n",
      "                 rs939290_meth  rs951295_meth  rs966367_meth  rs9839873_meth  \n",
      "TCGA-2A-A8VV-01       1.177536       0.168477       1.428796        0.628206  \n",
      "TCGA-2Z-A9J3-01      -0.106571       0.223820      -0.085130        0.414076  \n",
      "TCGA-2Z-A9J6-01       0.260937       0.169126      -0.099591       -1.269578  \n",
      "TCGA-2Z-A9J7-01       1.173022      -1.353195       1.341533       -0.664372  \n",
      "TCGA-2Z-A9JE-01      -1.513267       0.175400       1.362315        0.581429  \n",
      "\n",
      "[5 rows x 451633 columns]\n",
      "\n",
      "Descriptive statistics for X_filtered (first 50 columns):\n",
      "       100130426_expr  100133144_expr  100134869_expr  10357_expr  10431_expr  \\\n",
      "count      565.000000      565.000000      565.000000  565.000000  565.000000   \n",
      "mean         0.001142       -0.002885       -0.003192   -0.001569   -0.005553   \n",
      "std          1.003417        1.002530        1.002392    1.000346    0.999431   \n",
      "min         -1.468675       -2.782754       -4.616723   -5.944519   -3.604847   \n",
      "25%         -0.215095       -0.362498       -0.524657   -0.711181   -0.619329   \n",
      "50%         -0.215095        0.242135        0.147718    0.110172   -0.060113   \n",
      "75%         -0.215095        0.670103        0.673017    0.741886    0.624291   \n",
      "max         10.244736        1.454948        1.805484    2.237909    4.648484   \n",
      "\n",
      "       136542_expr  155060_expr  26823_expr  280660_expr  340602_expr  ...  \\\n",
      "count   565.000000   565.000000  565.000000   565.000000   565.000000  ...   \n",
      "mean      0.000223    -0.000444    0.002496     0.001991     0.006932  ...   \n",
      "std       1.003535     0.998208    0.999495     1.003165     0.998989  ...   \n",
      "min      -0.041996    -3.901723   -1.276634    -0.375007    -1.463035  ...   \n",
      "25%      -0.041996    -0.683201   -1.276634    -0.375007    -1.305524  ...   \n",
      "50%      -0.041996     0.089764   -0.006256    -0.375007     0.113930  ...   \n",
      "75%      -0.041996     0.656720    0.823297    -0.375007     0.837822  ...   \n",
      "max      23.811762     3.142499    2.538502     5.262384     2.650746  ...   \n",
      "\n",
      "       AADACL3_expr  AADACL4_expr  AADAT_expr  AAGAB_expr   AAK1_expr  \\\n",
      "count    565.000000    565.000000  565.000000  565.000000  565.000000   \n",
      "mean       0.001231      0.002178    0.010257   -0.003031    0.008386   \n",
      "std        1.003397      1.003092    0.978053    1.002452    0.989763   \n",
      "min       -2.298520     -0.410169   -6.049401   -3.400579   -4.410495   \n",
      "25%       -0.231827     -0.410169   -0.502195   -0.671333   -0.515083   \n",
      "50%       -0.231827     -0.410169    0.096456    0.149038   -0.020501   \n",
      "75%       -0.231827     -0.410169    0.743788    0.676138    0.549860   \n",
      "max        9.437835      7.344411    1.853875    2.760437    2.941518   \n",
      "\n",
      "        AAMP_expr  AANAT_expr   AARS_expr  AARS2_expr  AARSD1_expr  \n",
      "count  565.000000  565.000000  565.000000  565.000000   565.000000  \n",
      "mean     0.002940    0.003492   -0.001555    0.000023    -0.002427  \n",
      "std      1.002709    1.001154    1.003061    1.003287     1.002937  \n",
      "min     -5.924493   -3.342208   -3.241946   -2.309778    -3.127451  \n",
      "25%     -0.498008   -0.654145   -0.746177   -0.663429    -0.457623  \n",
      "50%      0.159892    0.102445   -0.124366   -0.089345    -0.061660  \n",
      "75%      0.666424    0.753611    0.841352    0.508069     0.531244  \n",
      "max      3.530907    2.548505    2.493308    6.209412     3.057904  \n",
      "\n",
      "[8 rows x 50 columns]\n",
      "\n",
      "Filtered Target Labels (y_filtered) shape: (565,)\n",
      "Value counts for y_filtered:\n",
      "_primary_disease\n",
      "thyroid carcinoma                        172\n",
      "brain lower grade glioma                 119\n",
      "kidney papillary cell carcinoma          105\n",
      "thymoma                                   35\n",
      "kidney chromophobe                        32\n",
      "sarcoma                                   24\n",
      "adrenocortical cancer                     14\n",
      "prostate adenocarcinoma                   11\n",
      "uterine corpus endometrioid carcinoma      9\n",
      "uveal melanoma                             9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Filtered Phenotype Data (pheno_filtered) shape: (565, 3)\n",
      "First 5 rows of pheno_filtered:\n",
      "                 sample_type_id    sample_type  \\\n",
      "TCGA-2A-A8VV-01             1.0  Primary Tumor   \n",
      "TCGA-2Z-A9J3-01             1.0  Primary Tumor   \n",
      "TCGA-2Z-A9J6-01             1.0  Primary Tumor   \n",
      "TCGA-2Z-A9J7-01             1.0  Primary Tumor   \n",
      "TCGA-2Z-A9JE-01             1.0  Primary Tumor   \n",
      "\n",
      "                                _primary_disease  \n",
      "TCGA-2A-A8VV-01          prostate adenocarcinoma  \n",
      "TCGA-2Z-A9J3-01  kidney papillary cell carcinoma  \n",
      "TCGA-2Z-A9J6-01  kidney papillary cell carcinoma  \n",
      "TCGA-2Z-A9J7-01  kidney papillary cell carcinoma  \n",
      "TCGA-2Z-A9JE-01  kidney papillary cell carcinoma  \n",
      "\n",
      "Descriptive statistics for pheno_filtered:\n",
      "        sample_type_id    sample_type   _primary_disease\n",
      "count       565.000000            565                565\n",
      "unique             NaN              2                 17\n",
      "top                NaN  Primary Tumor  thyroid carcinoma\n",
      "freq               NaN            557                172\n",
      "mean          1.070796            NaN                NaN\n",
      "std           0.591260            NaN                NaN\n",
      "min           1.000000            NaN                NaN\n",
      "25%           1.000000            NaN                NaN\n",
      "50%           1.000000            NaN                NaN\n",
      "75%           1.000000            NaN                NaN\n",
      "max           6.000000            NaN                NaN\n",
      "\n",
      "Encoded 17 cancer types for EDA.\n",
      "\n",
      "PCA reduced data shape: (565, 50)\n",
      "PCA explained variance ratio (first 50 components): 0.54\n",
      "First 5 rows of PCA reduced data (X_pca_df):\n",
      "                         0           1           2           3           4   \\\n",
      "TCGA-2A-A8VV-01   12.712642  172.207885  135.713602  126.072380   40.034767   \n",
      "TCGA-2Z-A9J3-01 -149.392238  207.832657 -202.948218    8.742684  165.532920   \n",
      "TCGA-2Z-A9J6-01 -170.764047  176.292024 -228.524425  -34.973950   88.602273   \n",
      "TCGA-2Z-A9J7-01 -159.382263  199.258795 -191.534133   32.561586  181.309834   \n",
      "TCGA-2Z-A9JE-01 -145.032691  127.526726 -186.828642  -76.526556   93.310582   \n",
      "\n",
      "                        5          6          7           8          9   ...  \\\n",
      "TCGA-2A-A8VV-01  55.936875  96.259914 -66.765418  114.993459  48.579467  ...   \n",
      "TCGA-2Z-A9J3-01 -60.521350  10.214119 -13.728923    5.977378  33.026266  ...   \n",
      "TCGA-2Z-A9J6-01 -65.787780   8.984545  -0.165547   -2.699660 -25.086949  ...   \n",
      "TCGA-2Z-A9J7-01 -64.515431 -11.316178 -15.301878  -17.715782  -5.838939  ...   \n",
      "TCGA-2Z-A9JE-01  19.473567 -33.844531 -41.850343  -24.140525   9.890076  ...   \n",
      "\n",
      "                        40         41         42         43         44  \\\n",
      "TCGA-2A-A8VV-01  11.081777  19.878299 -11.725590  14.084395   8.586518   \n",
      "TCGA-2Z-A9J3-01  18.555620 -14.121106   3.985187  -9.696666  30.679288   \n",
      "TCGA-2Z-A9J6-01   7.043151  19.914837  11.329768  -3.945094  12.785463   \n",
      "TCGA-2Z-A9J7-01  18.185108 -43.925714   6.006393   4.292696  34.015812   \n",
      "TCGA-2Z-A9JE-01  11.017972 -22.560957  -1.783598   8.321493  30.694542   \n",
      "\n",
      "                       45         46         47         48         49  \n",
      "TCGA-2A-A8VV-01 -7.487858   5.311204  15.642800 -16.558838 -17.250610  \n",
      "TCGA-2Z-A9J3-01 -4.975132 -28.911642  16.759143   9.230308  -2.081303  \n",
      "TCGA-2Z-A9J6-01  1.330728  12.711892 -11.274142  28.622442 -11.278865  \n",
      "TCGA-2Z-A9J7-01 -2.893792 -32.344159 -10.680148  -0.219567 -20.855332  \n",
      "TCGA-2Z-A9JE-01 -6.048220 -24.499498  16.675877  24.912671 -23.978597  \n",
      "\n",
      "[5 rows x 50 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shrav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UMAP reduced data shape: (565, 2)\n",
      "UMAP projection plot saved.\n",
      "\n",
      "--- Performing K-Means Clustering (k=3) and Survival Analysis ---\n",
      "K-Means clustering complete. Found 3 clusters.\n",
      "Survival data columns ('_OS_TIME', '_OS_IND') not found in phenotype data.\n",
      "Skipping survival analysis. Please ensure your phenotype file contains these columns or update the column names in the script.\n",
      "\n",
      "EDA and Visualization steps completed.\n",
      "\n",
      "Multi-Omics Workflow (EDA Focus) execution complete.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Multi-Omics Data Analysis Workflow for Jupyter Notebook (EDA Focus)\n",
    "# This script integrates Data Preparation and comprehensive Exploratory Data Analysis (EDA)\n",
    "# and Visualization. All machine learning, GSEA, and misclassification analysis\n",
    "# components have been removed to focus purely on data understanding.\n",
    "#\n",
    "# Before running:\n",
    "# 1. Ensure you have all necessary libraries installed:\n",
    "#    pip install pandas numpy matplotlib seaborn scikit-learn umap-learn mygene scipy\n",
    "# 2. Update the 'raw_expr_file' and 'raw_pheno_file' paths in the 'Data Preparation' section\n",
    "#    to point to your actual raw data files.\n",
    "# ==============================================================================\n",
    "\n",
    "# --- General Imports (common to all modules) ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import mygene # For robust gene ID mapping\n",
    "import time # Import the time module for time.sleep()\n",
    "from scipy import stats # For t-test in Volcano Plot (if used in EDA)\n",
    "\n",
    "# --- Specific Imports for EDA & Visualization ---\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import umap.umap_ as umap\n",
    "\n",
    "\n",
    "# Set a consistent style for all plots for a professional look\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Data Preparation Functions\n",
    "# ==============================================================================\n",
    "\n",
    "def load_and_preprocess_multi_omics_data_advanced(expr_file, maf_file, pheno_file, mart_file, mirna_file, cnv_file, rppa_file, meth_file, target_col):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses expression, mutation, miRNA, CNV, RPPA, Methylation,\n",
    "    and phenotype data, aligning samples and genes/features, and preparing for\n",
    "    multi-omics analysis.\n",
    "    Includes log transformation for expression/miRNA and low-variance feature filtering.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Multi-Omics Data Loading and Preprocessing (Advanced) ---\")\n",
    "\n",
    "    # --- Load Data ---\n",
    "    try:\n",
    "        expr_df = pd.read_csv(expr_file, sep=\"\\t\", index_col=0, on_bad_lines='skip')\n",
    "        maf_df = pd.read_csv(maf_file, sep=\"\\t\", comment='#', low_memory=False, on_bad_lines='skip')\n",
    "        pheno_df = pd.read_csv(pheno_file, sep=\"\\t\", low_memory=False, on_bad_lines='skip')\n",
    "        mirna_df = pd.read_csv(mirna_file, sep=\"\\t\", index_col=0, on_bad_lines='skip')\n",
    "        cnv_df = pd.read_csv(cnv_file, sep=\"\\t\", index_col=0, on_bad_lines='skip')\n",
    "        rppa_df = pd.read_csv(rppa_file, sep=\"\\t\", index_col=0, on_bad_lines='skip')\n",
    "        \n",
    "        print(f\"Attempting to load methylation data in chunks from: {meth_file}\")\n",
    "        CHUNK_SIZE = 50000\n",
    "        TOP_METHYLATION_PROBES_TO_KEEP = 1000 \n",
    "\n",
    "        all_probes_std = pd.Series(dtype=float)\n",
    "        header_df = pd.read_csv(meth_file, sep=\"\\t\", nrows=0, on_bad_lines='skip')\n",
    "        index_col_name = header_df.columns[0]\n",
    "        data_columns_in_file = header_df.columns[1:].tolist()\n",
    "\n",
    "        for i, chunk in enumerate(pd.read_csv(meth_file, sep=\"\\t\", index_col=0, chunksize=CHUNK_SIZE, on_bad_lines='skip')):\n",
    "            chunk_numeric = chunk.select_dtypes(include=np.number)\n",
    "            if not chunk_numeric.empty:\n",
    "                current_chunk_std = chunk_numeric.std()\n",
    "                all_probes_std = all_probes_std.add(current_chunk_std, fill_value=0)\n",
    "            print(f\"  Processed chunk {i+1}, current chunk shape: {chunk.shape}\")\n",
    "\n",
    "        if not all_probes_std.empty:\n",
    "            all_probes_std = all_probes_std[all_probes_std.index.isin(data_columns_in_file)]\n",
    "            top_variable_probes = all_probes_std.nlargest(TOP_METHYLATION_PROBES_TO_KEEP).index.tolist()\n",
    "            print(f\"Identified top {len(top_variable_probes)} most variable methylation probes from {len(all_probes_std)} total probes.\")\n",
    "        else:\n",
    "            top_variable_probes = []\n",
    "            print(\"No numeric methylation probes found to determine variability or no probes in data_columns_in_file.\")\n",
    "\n",
    "        if top_variable_probes:\n",
    "            columns_to_load_by_name = [index_col_name] + top_variable_probes\n",
    "            meth_df = pd.read_csv(meth_file, sep=\"\\t\", index_col=0, usecols=columns_to_load_by_name, on_bad_lines='skip')\n",
    "            print(f\"Methylation data re-loaded with only top {len(top_variable_probes)} variable probes. Final shape: {meth_df.shape}\")\n",
    "        else:\n",
    "            meth_df = pd.DataFrame()\n",
    "            print(\"No top variable methylation probes identified or loaded. Methylation DataFrame is empty.\")\n",
    "\n",
    "        print(\"Raw expression, mutation (MAF), phenotype, miRNA, CNV, RPPA, and Methylation data loaded.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data: {e}. Please ensure all files are in the '{data_dir}' directory.\")\n",
    "        return None, None, None, None, None, None, None, None\n",
    "\n",
    "    print(\"\\n--- Debug: Phenotype DataFrame Columns ---\")\n",
    "    print(pheno_df.columns.tolist())\n",
    "    print(\"------------------------------------------\")\n",
    "\n",
    "    # --- Gene/miRNA ID Mapping Helper Functions ---\n",
    "    mg = mygene.MyGeneInfo()\n",
    "    \n",
    "    def batch_query_mygene_robust(ids, scopes, fields='ensembl.gene', species='human', batch_size=1000):\n",
    "        mapping = {}\n",
    "        successful_queries = 0\n",
    "        not_found_queries_list = []\n",
    "        for i in range(0, len(ids), batch_size):\n",
    "            batch = ids[i:i + batch_size]\n",
    "            try:\n",
    "                res = mg.querymany(batch, scopes=scopes, fields=fields, species=species, returnall=True)\n",
    "                \n",
    "                for nf_item in res.get('notfound', []):\n",
    "                    not_found_queries_list.append(nf_item['query'])\n",
    "\n",
    "                for r in res['out']:\n",
    "                    query_id = str(r['query'])\n",
    "                    ensembl_info = r.get(fields)\n",
    "                    if ensembl_info:\n",
    "                        ensembl_id = None\n",
    "                        if isinstance(ensembl_info, list):\n",
    "                            for item in ensembl_info:\n",
    "                                if isinstance(item, dict) and 'gene' in item:\n",
    "                                    ensembl_id = item['gene']\n",
    "                                    break\n",
    "                        elif isinstance(ensembl_info, dict) and 'gene' in ensembl_info:\n",
    "                            ensembl_id = ensembl_info['gene']\n",
    "                        \n",
    "                        if ensembl_id:\n",
    "                            mapping[query_id] = ensembl_id\n",
    "                            successful_queries += 1\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error in mygene batch query for scopes '{scopes}': {e}\")\n",
    "            time.sleep(0.1)\n",
    "        print(f\"Successfully mapped {successful_queries} out of {len(ids)} terms for scopes '{scopes}'.\")\n",
    "        if not_found_queries_list:\n",
    "            print(f\"First 10 unmapped queries for scopes '{scopes}': {not_found_queries_list[:10]}\")\n",
    "        return mapping\n",
    "\n",
    "    def load_mart_mapping(mart_file_path):\n",
    "        hugo_to_ensembl = {}\n",
    "        entrez_to_ensembl = {}\n",
    "        try:\n",
    "            mart_df = pd.read_csv(mart_file_path, sep=\"\\t\", low_memory=False, on_bad_lines='skip')\n",
    "            print(f\"BioMart mapping file '{os.path.basename(mart_file_path)}' loaded.\")\n",
    "            print(f\"BioMart columns: {mart_df.columns.tolist()}\")\n",
    "\n",
    "            if 'Gene name' in mart_df.columns and 'Gene stable ID' in mart_df.columns:\n",
    "                temp_map_df = mart_df[['Gene name', 'Gene stable ID']].dropna().drop_duplicates()\n",
    "                hugo_to_ensembl = dict(zip(temp_map_df['Gene name'], temp_map_df['Gene stable ID']))\n",
    "                print(f\"Found {len(hugo_to_ensembl)} Hugo Symbol to Ensembl mappings in BioMart file.\")\n",
    "            \n",
    "            if 'NCBI gene ID' in mart_df.columns and 'Gene stable ID' in mart_df.columns:\n",
    "                temp_map_df = mart_df[['NCBI gene ID', 'Gene stable ID']].dropna().drop_duplicates()\n",
    "                entrez_to_ensembl = dict(zip(temp_map_df['NCBI gene ID'].astype(str), temp_map_df['Gene stable ID']))\n",
    "                print(f\"Found {len(entrez_to_ensembl)} Entrez ID to Ensembl mappings in BioMart file.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: BioMart mapping file '{os.path.basename(mart_file_path)}' not found. Skipping BioMart mapping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing BioMart mapping file: {e}\")\n",
    "        return hugo_to_ensembl, entrez_to_ensembl\n",
    "\n",
    "    hugo_to_ensembl_mart, entrez_to_ensembl_mart = load_mart_mapping(mart_file)\n",
    "\n",
    "    # --- Preprocess Gene Expression Data ---\n",
    "    expr_T = expr_df.T\n",
    "    print(f\"Initial Gene Expression data shape (samples x genes): {expr_T.shape}\")\n",
    "    print(f\"First 5 gene expression IDs (before mapping): {expr_T.columns[:5].tolist()}\")\n",
    "\n",
    "    expr_T_ensembl = pd.DataFrame()\n",
    "    is_expr_ensembl = False\n",
    "    if len(expr_T.columns) > 0 and isinstance(expr_T.columns[0], str):\n",
    "        if all(col.startswith('ENSG') and '.' in col for col in expr_T.columns[:min(5, len(expr_T.columns))]):\n",
    "            is_expr_ensembl = True\n",
    "            expr_T_ensembl = expr_T.copy()\n",
    "            expr_T_ensembl.columns = expr_T_ensembl.columns.str.split('.').str[0]\n",
    "            print(\"Gene Expression IDs appear to be Ensembl. Removing version numbers.\")\n",
    "    \n",
    "    if not is_expr_ensembl:\n",
    "        all_expr_genes = list(expr_T.columns.astype(str))\n",
    "        expr_gene_map = {}\n",
    "\n",
    "        if entrez_to_ensembl_mart:\n",
    "            print(\"Attempting BioMart mapping for Gene Expression (Entrez to Ensembl)...\")\n",
    "            expr_gene_map = {k: entrez_to_ensembl_mart.get(k, None) for k in all_expr_genes}\n",
    "            expr_gene_map = {k: v for k, v in expr_gene_map.items() if v is not None}\n",
    "            print(f\"BioMart mapped {len(expr_gene_map)} gene expression IDs.\")\n",
    "\n",
    "        if not expr_gene_map:\n",
    "            print(\"BioMart mapping for Gene Expression failed or not available. Attempting mygene mapping (Entrez/Symbol to Ensembl)...\")\n",
    "            expr_gene_map = batch_query_mygene_robust(all_expr_genes, scopes=['entrezgene', 'symbol'])\n",
    "        \n",
    "        if expr_gene_map:\n",
    "            mapped_cols_data = {}\n",
    "            for original_col, mapped_col in expr_gene_map.items():\n",
    "                if original_col in expr_T.columns:\n",
    "                    mapped_cols_data[mapped_col] = expr_T[original_col]\n",
    "            expr_T_ensembl = pd.DataFrame(mapped_cols_data, index=expr_T.index)\n",
    "            expr_T_ensembl = expr_T_ensembl.loc[:, ~expr_T_ensembl.columns.duplicated()]\n",
    "            print(f\"Gene Expression data after mapping to Ensembl: {expr_T_ensembl.shape}\")\n",
    "        else:\n",
    "            expr_T_ensembl = expr_T.copy()\n",
    "            print(\"Warning: Gene Expression ID mapping failed completely. Proceeding with original gene IDs. This may affect gene-level alignment.\")\n",
    "            print(f\"Gene Expression data shape (original IDs): {expr_T_ensembl.shape}\")\n",
    "\n",
    "    # --- Preprocess Somatic Mutation Data ---\n",
    "    if 'FILTER' in maf_df.columns:\n",
    "        maf_df = maf_df[maf_df['FILTER'] == 'PASS']\n",
    "    maf_df = maf_df[[\"Tumor_Sample_Barcode\", \"Hugo_Symbol\"]].dropna().drop_duplicates()\n",
    "    print(f\"Mutation records after dropping NaNs and duplicates: {maf_df.shape}\")\n",
    "\n",
    "    maf_df_mapped = maf_df.copy()\n",
    "    unique_hugo_symbols = maf_df_mapped['Hugo_Symbol'].unique().tolist()\n",
    "    mutation_gene_map = {}\n",
    "\n",
    "    if hugo_to_ensembl_mart:\n",
    "        print(\"Attempting BioMart mapping for Mutation data (Hugo Symbol to Ensembl)...\")\n",
    "        mutation_gene_map = {k: hugo_to_ensembl_mart.get(k, None) for k in unique_hugo_symbols}\n",
    "        mutation_gene_map = {k: v for k, v in mutation_gene_map.items() if v is not None}\n",
    "        print(f\"BioMart mapped {len(mutation_gene_map)} mutation genes.\")\n",
    "\n",
    "    if not mutation_gene_map:\n",
    "        print(\"BioMart mapping for Mutation failed or not available. Attempting mygene mapping (Hugo Symbol to Ensembl)...\")\n",
    "        mutation_gene_map = batch_query_mygene_robust(unique_hugo_symbols, scopes=['symbol'])\n",
    "\n",
    "    if mutation_gene_map:\n",
    "        maf_df_mapped['Ensembl_ID'] = maf_df_mapped['Hugo_Symbol'].map(mutation_gene_map)\n",
    "        maf_df_mapped = maf_df_mapped.dropna(subset=['Ensembl_ID'])\n",
    "        print(f\"Mutation records after mapping Hugo_Symbol to Ensembl: {maf_df_mapped.shape}\")\n",
    "    else:\n",
    "        maf_df_mapped['Ensembl_ID'] = maf_df_mapped['Hugo_Symbol']\n",
    "        print(\"Warning: Mutation gene ID mapping failed completely. Proceeding with original Hugo Symbols as gene IDs.\")\n",
    "        print(f\"Mutation records shape (original Hugo Symbols): {maf_df_mapped.shape}\")\n",
    "\n",
    "    mutation_matrix = pd.DataFrame()\n",
    "    if not maf_df_mapped.empty:\n",
    "        maf_df_mapped = maf_df_mapped.drop_duplicates(subset=['Tumor_Sample_Barcode', 'Ensembl_ID'])\n",
    "        mutation_matrix = pd.crosstab(maf_df_mapped['Tumor_Sample_Barcode'], maf_df_mapped['Ensembl_ID'])\n",
    "        mutation_matrix = mutation_matrix.clip(upper=1)\n",
    "        print(f\"Binary mutation matrix shape: {mutation_matrix.shape}\")\n",
    "    else:\n",
    "        print(\"Empty mutation matrix created due to no successful gene mapping or no mutation records.\")\n",
    "\n",
    "    # --- Preprocess miRNA Expression Data ---\n",
    "    mirna_T = mirna_df.T\n",
    "    print(f\"Initial miRNA Expression data shape (samples x miRNAs): {mirna_T.shape}\")\n",
    "    print(f\"First 5 miRNA IDs: {mirna_T.columns[:5].tolist()}\")\n",
    "\n",
    "    # --- Preprocess Copy Number Variation (CNV) Data ---\n",
    "    cnv_T = cnv_df.T # Samples as rows, genes as columns\n",
    "    print(f\"Initial CNV data shape (samples x genes): {cnv_T.shape}\")\n",
    "    print(f\"First 5 CNV gene IDs: {cnv_T.columns[:5].tolist()}\")\n",
    "\n",
    "    cnv_T_ensembl = pd.DataFrame()\n",
    "    is_cnv_ensembl = False\n",
    "    if len(cnv_T.columns) > 0 and isinstance(cnv_T.columns[0], str):\n",
    "        if all(col.startswith('ENSG') and '.' in col for col in cnv_T.columns[:min(5, len(cnv_T.columns))]):\n",
    "            is_cnv_ensembl = True\n",
    "            cnv_T_ensembl = cnv_T.copy()\n",
    "            cnv_T_ensembl.columns = cnv_T_ensembl.columns.str.split('.').str[0]\n",
    "            print(\"CNV gene IDs appear to be Ensembl. Removing version numbers.\")\n",
    "    \n",
    "    if not is_cnv_ensembl:\n",
    "        all_cnv_genes = list(cnv_T.columns.astype(str))\n",
    "        cnv_gene_map = {}\n",
    "\n",
    "        if hugo_to_ensembl_mart:\n",
    "            print(\"Attempting BioMart mapping for CNV data (Hugo Symbol to Ensembl)...\")\n",
    "            cnv_gene_map = {k: hugo_to_ensembl_mart.get(k, None) for k in all_cnv_genes}\n",
    "            cnv_gene_map = {k: v for k, v in cnv_gene_map.items() if v is not None}\n",
    "            print(f\"BioMart mapped {len(cnv_gene_map)} CNV genes.\")\n",
    "\n",
    "        if not cnv_gene_map:\n",
    "            print(\"BioMart mapping for CNV failed or not available. Attempting mygene mapping (Symbol to Ensembl)...\")\n",
    "            cnv_gene_map = batch_query_mygene_robust(all_cnv_genes, scopes=['symbol'])\n",
    "        \n",
    "        if cnv_gene_map:\n",
    "            mapped_cols_data = {}\n",
    "            for original_col, mapped_col in cnv_gene_map.items():\n",
    "                if original_col in cnv_T.columns:\n",
    "                    mapped_cols_data[mapped_col] = cnv_T[original_col]\n",
    "            cnv_T_ensembl = pd.DataFrame(mapped_cols_data, index=cnv_T.index)\n",
    "            cnv_T_ensembl = cnv_T_ensembl.loc[:, ~cnv_T_ensembl.columns.duplicated()]\n",
    "            print(f\"CNV data after mapping to Ensembl: {cnv_T_ensembl.shape}\")\n",
    "        else:\n",
    "            cnv_T_ensembl = cnv_T.copy()\n",
    "            print(\"Warning: CNV gene ID mapping failed completely. Proceeding with original CNV gene IDs. This may affect gene-level alignment.\")\n",
    "            print(f\"CNV data shape (original IDs): {cnv_T_ensembl.shape}\")\n",
    "\n",
    "    # NEW: Preprocess RPPA Data\n",
    "    rppa_T = rppa_df.T\n",
    "    print(f\"Initial RPPA data shape (samples x proteins): {rppa_T.shape}\")\n",
    "    print(f\"First 5 RPPA protein IDs: {rppa_T.columns[:5].tolist()}\")\n",
    "\n",
    "    # NEW: Preprocess Methylation Data\n",
    "    meth_T = meth_df.T\n",
    "    print(f\"Initial Methylation data shape (samples x probes): {meth_T.shape}\")\n",
    "    print(f\"First 5 Methylation probe IDs: {meth_T.columns[:5].tolist()}\")\n",
    "\n",
    "    # --- Standardize Sample IDs Across All Omics and Phenotype ---\n",
    "    pheno_sample_id_col = 'sample'\n",
    "    \n",
    "    if pheno_sample_id_col not in pheno_df.columns:\n",
    "        print(f\"Error: The specified sample ID column '{pheno_sample_id_col}' not found in phenotype data.\")\n",
    "        return None, None, None, None, None, None, None, None\n",
    "\n",
    "    # Ensure all sample IDs are consistent (first 15 characters, uppercase)\n",
    "    pheno_df_cleaned = pheno_df.copy()\n",
    "    pheno_df_cleaned[pheno_sample_id_col] = pheno_df_cleaned[pheno_sample_id_col].astype(str).str.slice(0, 15).str.upper()\n",
    "    pheno_df_cleaned = pheno_df_cleaned.loc[~pheno_df_cleaned[pheno_sample_id_col].duplicated(keep='first')]\n",
    "    pheno_df_cleaned = pheno_df_cleaned.set_index(pheno_sample_id_col)\n",
    "\n",
    "    expr_T_ensembl.index = expr_T_ensembl.index.astype(str).str.slice(0, 15).str.upper()\n",
    "    expr_T_ensembl = expr_T_ensembl.groupby(expr_T_ensembl.index).mean()\n",
    "\n",
    "    if not mutation_matrix.empty:\n",
    "        mutation_matrix.index = mutation_matrix.index.astype(str).str.slice(0, 15).str.upper()\n",
    "        mutation_matrix = mutation_matrix.loc[~mutation_matrix.index.duplicated(keep='first')]\n",
    "\n",
    "    mirna_T.index = mirna_T.index.astype(str).str.slice(0, 15).str.upper()\n",
    "    mirna_T = mirna_T.groupby(mirna_T.index).mean()\n",
    "\n",
    "    cnv_T_ensembl.index = cnv_T_ensembl.index.astype(str).str.slice(0, 15).str.upper()\n",
    "    cnv_T_ensembl = cnv_T_ensembl.groupby(cnv_T_ensembl.index).mean()\n",
    "\n",
    "    rppa_T.index = rppa_T.index.astype(str).str.slice(0, 15).str.upper()\n",
    "    rppa_T = rppa_T.groupby(rppa_T.index).mean()\n",
    "\n",
    "    meth_T.index = meth_T.index.astype(str).str.slice(0, 15).str.upper()\n",
    "    meth_T = meth_T.groupby(meth_T.index).mean()\n",
    "\n",
    "\n",
    "    # --- Intersect Samples Across All Omics ---\n",
    "    common_samples = expr_T_ensembl.index.intersection(pheno_df_cleaned.index)\n",
    "    if not mutation_matrix.empty:\n",
    "        common_samples = common_samples.intersection(mutation_matrix.index)\n",
    "    if not mirna_T.empty:\n",
    "        common_samples = common_samples.intersection(mirna_T.index)\n",
    "    if not cnv_T_ensembl.empty:\n",
    "        common_samples = common_samples.intersection(cnv_T_ensembl.index)\n",
    "    if not rppa_T.empty:\n",
    "        common_samples = common_samples.intersection(rppa_T.index)\n",
    "    if not meth_T.empty:\n",
    "        common_samples = common_samples.intersection(meth_T.index)\n",
    "    \n",
    "    print(f\"Common samples across all omics datasets: {len(common_samples)}\")\n",
    "\n",
    "    if len(common_samples) == 0:\n",
    "        print(\"No common samples found across all datasets. Cannot proceed with multi-omics analysis.\")\n",
    "        return None, None, None, None, None, None, None, None\n",
    "\n",
    "    # Subset all dataframes to common samples\n",
    "    expr_aligned = expr_T_ensembl.loc[common_samples]\n",
    "    mut_aligned = mutation_matrix.loc[common_samples] if not mutation_matrix.empty else pd.DataFrame(0, index=common_samples, columns=[])\n",
    "    mirna_aligned = mirna_T.loc[common_samples] if not mirna_T.empty else pd.DataFrame(0, index=common_samples, columns=[])\n",
    "    cnv_aligned = cnv_T_ensembl.loc[common_samples] if not cnv_T_ensembl.empty else pd.DataFrame(0, index=common_samples, columns=[])\n",
    "    rppa_aligned = rppa_T.loc[common_samples] if not rppa_T.empty else pd.DataFrame(0, index=common_samples, columns=[])\n",
    "    meth_aligned = meth_T.loc[common_samples] if not meth_T.empty else pd.DataFrame(0, index=common_samples, columns=[])\n",
    "    pheno_aligned = pheno_df_cleaned.loc[common_samples]\n",
    "\n",
    "    # --- Align Genes/Features (Columns) for Gene-Level Omics ---\n",
    "    expr_cols_are_ensembl = not expr_aligned.empty and expr_aligned.shape[1] > 0 and \\\n",
    "                            all(isinstance(col, str) and col.startswith('ENSG') for col in expr_aligned.columns[:min(5, len(expr_aligned.columns))])\n",
    "    mut_cols_are_ensembl = not mut_aligned.empty and mut_aligned.shape[1] > 0 and \\\n",
    "                           all(isinstance(col, str) and col.startswith('ENSG') for col in mut_aligned.columns[:min(5, len(mut_aligned.columns))])\n",
    "    cnv_cols_are_ensembl = not cnv_aligned.empty and cnv_aligned.shape[1] > 0 and \\\n",
    "                           all(isinstance(col, str) and col.startswith('ENSG') for col in cnv_aligned.columns[:min(5, len(cnv_aligned.columns))])\n",
    "    \n",
    "    if expr_cols_are_ensembl and mut_cols_are_ensembl and cnv_cols_are_ensembl:\n",
    "        print(\"All gene-level omics (Expression, Mutation, CNV) are Ensembl IDs. Attempting gene-level alignment.\")\n",
    "        all_gene_features_union = expr_aligned.columns.union(mut_aligned.columns).union(cnv_aligned.columns)\n",
    "        \n",
    "        expr_aligned = expr_aligned.reindex(columns=all_gene_features_union, fill_value=0)\n",
    "        mut_aligned = mut_aligned.reindex(columns=all_gene_features_union, fill_value=0)\n",
    "        cnv_aligned = cnv_aligned.reindex(columns=all_gene_features_union, fill_value=0)\n",
    "    else:\n",
    "        print(\"Gene-level alignment skipped (either not all Ensembl IDs or one/more gene-level omics data are empty/unmapped). Features will be concatenated as-is.\")\n",
    "            \n",
    "    print(f\"Aligned gene expression data shape: {expr_aligned.shape}\")\n",
    "    print(f\"Aligned mutation data shape: {mut_aligned.shape}\")\n",
    "    print(f\"Aligned miRNA expression data shape: {mirna_aligned.shape}\")\n",
    "    print(f\"Aligned CNV data shape: {cnv_aligned.shape}\")\n",
    "    print(f\"Aligned RPPA data shape: {rppa_aligned.shape}\")\n",
    "    print(f\"Aligned Methylation data shape: {meth_aligned.shape}\")\n",
    "\n",
    "\n",
    "    # --- Impute Missing Values and Scale Data for Each Omics ---\n",
    "    \n",
    "    # Helper function for filtering low variance and processing\n",
    "    def process_omics_data(df_aligned, omics_name, apply_log_transform=False):\n",
    "        if df_aligned.empty or df_aligned.shape[1] == 0:\n",
    "            print(f\"No {omics_name} columns to process. Skipping imputation and scaling.\")\n",
    "            return pd.DataFrame(index=df_aligned.index), df_aligned # Return empty scaled, and original aligned\n",
    "\n",
    "        # Drop columns that are entirely NaN before imputation\n",
    "        df_filtered = df_aligned.dropna(axis=1, how='all')\n",
    "        if df_filtered.empty:\n",
    "            print(f\"{omics_name} data became empty after dropping all-NaN columns. Skipping imputation and scaling.\")\n",
    "            return pd.DataFrame(index=df_aligned.index), df_aligned\n",
    "\n",
    "        # Apply log transformation if specified (for expression/miRNA)\n",
    "        if apply_log_transform:\n",
    "            # Add a small constant (1) to avoid log(0) issues\n",
    "            df_filtered = np.log2(df_filtered + 1)\n",
    "            print(f\"{omics_name} data log2 transformed.\")\n",
    "\n",
    "        # Remove features with zero variance (after log transform if applied)\n",
    "        # This is a simple form of noise elimination\n",
    "        initial_cols = df_filtered.shape[1]\n",
    "        df_filtered = df_filtered.loc[:, df_filtered.std() > 1e-6] # Keep columns with std > a very small number\n",
    "        if df_filtered.shape[1] < initial_cols:\n",
    "            print(f\"Removed {initial_cols - df_filtered.shape[1]} zero-variance features from {omics_name} data.\")\n",
    "        \n",
    "        if df_filtered.empty:\n",
    "            print(f\"{omics_name} data became empty after dropping zero-variance columns. Skipping imputation and scaling.\")\n",
    "            return pd.DataFrame(index=df_aligned.index), df_aligned\n",
    "\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        df_imputed = pd.DataFrame(imputer.fit_transform(df_filtered),\n",
    "                                  columns=df_filtered.columns,\n",
    "                                  index=df_filtered.index)\n",
    "        print(f\"{omics_name} data imputed. Missing values: {df_imputed.isnull().sum().sum()}\")\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        df_scaled = pd.DataFrame(scaler.fit_transform(df_imputed),\n",
    "                                 columns=df_imputed.columns,\n",
    "                                 index=df_imputed.index)\n",
    "        return df_scaled, df_aligned # Return scaled data and original aligned (potentially log-transformed and filtered)\n",
    "\n",
    "    expr_scaled, expr_original_aligned_processed = process_omics_data(expr_aligned, \"Gene Expression\", apply_log_transform=True)\n",
    "    mut_scaled, _ = process_omics_data(mut_aligned, \"Mutation\") # Mutation is binary, no log transform\n",
    "    mirna_scaled, mirna_original_aligned_processed = process_omics_data(mirna_aligned, \"miRNA Expression\", apply_log_transform=True)\n",
    "    cnv_scaled, cnv_original_aligned_processed = process_omics_data(cnv_aligned, \"CNV\")\n",
    "    rppa_scaled, rppa_original_aligned_processed = process_omics_data(rppa_aligned, \"RPPA\")\n",
    "    meth_scaled, meth_original_aligned_processed = process_omics_data(meth_aligned, \"Methylation\")\n",
    "    \n",
    "    print(\"All omics data imputed and scaled (if columns available).\")\n",
    "\n",
    "    # --- Concatenate All Scaled Multi-Omics Data ---\n",
    "    final_omics_dfs = []\n",
    "    \n",
    "    if not expr_scaled.empty:\n",
    "        if not (expr_cols_are_ensembl and mut_cols_are_ensembl and cnv_cols_are_ensembl):\n",
    "            expr_scaled.columns = [f\"{col}_expr\" for col in expr_scaled.columns]\n",
    "        final_omics_dfs.append(expr_scaled)\n",
    "    \n",
    "    if not mut_scaled.empty:\n",
    "        if not (expr_cols_are_ensembl and mut_cols_are_ensembl and cnv_cols_are_ensembl):\n",
    "            mut_scaled.columns = [f\"{col}_mut\" for col in mut_scaled.columns]\n",
    "        final_omics_dfs.append(mut_scaled)\n",
    "\n",
    "    if not mirna_scaled.empty:\n",
    "        mirna_scaled.columns = [f\"{col}_mirna\" for col in mirna_scaled.columns]\n",
    "        final_omics_dfs.append(mirna_scaled)\n",
    "\n",
    "    if not cnv_scaled.empty:\n",
    "        if not (expr_cols_are_ensembl and mut_cols_are_ensembl and cnv_cols_are_ensembl):\n",
    "            cnv_scaled.columns = [f\"{col}_cnv\" for col in cnv_scaled.columns]\n",
    "        final_omics_dfs.append(cnv_scaled)\n",
    "\n",
    "    if not rppa_scaled.empty:\n",
    "        rppa_scaled.columns = [f\"{col}_rppa\" for col in rppa_scaled.columns]\n",
    "        final_omics_dfs.append(rppa_scaled)\n",
    "\n",
    "    if not meth_scaled.empty:\n",
    "        meth_scaled.columns = [f\"{col}_meth\" for col in meth_scaled.columns]\n",
    "        final_omics_dfs.append(meth_scaled)\n",
    "\n",
    "    if final_omics_dfs:\n",
    "        X_multiomics = pd.concat(final_omics_dfs, axis=1)\n",
    "        # Ensure no duplicate columns after concatenation, which can happen if original IDs overlap\n",
    "        X_multiomics = X_multiomics.loc[:, ~X_multiomics.columns.duplicated()]\n",
    "    else:\n",
    "        X_multiomics = pd.DataFrame(index=common_samples)\n",
    "        print(\"Warning: All omics dataframes are empty after preprocessing. Multi-omics matrix is empty.\")\n",
    "\n",
    "    y_labels = pheno_aligned[target_col]\n",
    "\n",
    "    print(f\"Final Multi-omics feature matrix shape: {X_multiomics.shape}\")\n",
    "    print(f\"Target labels shape: {y_labels.shape}\")\n",
    "\n",
    "    return X_multiomics, y_labels, pheno_aligned, expr_original_aligned_processed, mirna_original_aligned_processed, cnv_original_aligned_processed, rppa_original_aligned_processed, meth_original_aligned_processed\n",
    "\n",
    "def export_data_to_pkl(dataframe, output_path, name=\"DataFrame\"):\n",
    "    \"\"\"\n",
    "    Exports a DataFrame to a .pkl file.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame or pd.Series): The DataFrame or Series to export.\n",
    "        output_path (str): The path to save the .pkl file.\n",
    "        name (str): A descriptive name for the data being saved.\n",
    "    \"\"\"\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Created output directory: {output_dir}\")\n",
    "\n",
    "    try:\n",
    "        dataframe.to_pickle(output_path)\n",
    "        print(f\"{name} successfully exported to: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting {name} to .pkl: {e}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. EDA & Visualization Functions\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_summary_statistics(dataframe, name=\"Data\"):\n",
    "    \"\"\"\n",
    "    Generates and prints summary statistics for a given DataFrame.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The DataFrame for which to generate statistics.\n",
    "        name (str): A descriptive name for the DataFrame (e.g., \"Gene Expression\", \"Phenotype\").\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- EDA: Summary Statistics for {name} ---\")\n",
    "    print(dataframe.describe())\n",
    "    print(f\"\\nMissing values in {name}:\\n{dataframe.isnull().sum().sum()} total missing values.\")\n",
    "    if dataframe.isnull().sum().sum() > 0:\n",
    "        print(f\"Missing values per column:\\n{dataframe.isnull().sum()[dataframe.isnull().sum() > 0]}\")\n",
    "    print(f\"\\nDataFrame Info for {name}:\")\n",
    "    dataframe.info()\n",
    "    print(\"-\" * (25 + len(name)))\n",
    "\n",
    "def plot_tumor_type_distribution(phenotype_df, tumor_type_column='_primary_disease', output_path=None):\n",
    "    \"\"\"\n",
    "    Plots the distribution of tumor types from the phenotype DataFrame.\n",
    "\n",
    "    Args:\n",
    "        phenotype_df (pd.DataFrame): The phenotype DataFrame.\n",
    "        tumor_type_column (str): The name of the column containing tumor type information.\n",
    "        output_path (str, optional): Path to save the plot. If None, displays the plot.\n",
    "    \"\"\"\n",
    "    if tumor_type_column not in phenotype_df.columns:\n",
    "        print(f\"Error: '{tumor_type_column}' not found in phenotype DataFrame. Cannot plot tumor type distribution.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- EDA: Plotting Tumor Type Distribution ---\")\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.countplot(y=phenotype_df[tumor_type_column], order=phenotype_df[tumor_type_column].value_counts().index, palette='viridis')\n",
    "    plt.title(f'Distribution of {tumor_type_column.replace(\"_\", \" \").title()}', fontsize=16, weight='bold')\n",
    "    plt.xlabel('Number of Samples', fontsize=12)\n",
    "    plt.ylabel(tumor_type_column.replace(\"_\", \" \").title(), fontsize=12)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_path:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            print(f\"Created output directory: {output_dir}\")\n",
    "        plt.savefig(output_path, dpi=300)\n",
    "        print(f\"Tumor type distribution plot saved to: {output_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def perform_pca(data_df, n_components=2):\n",
    "    \"\"\"\n",
    "    Performs Principal Component Analysis (PCA) on the given data.\n",
    "\n",
    "    Args:\n",
    "        data_df (pd.DataFrame): DataFrame (samples x features).\n",
    "        n_components (int): Number of principal components to compute.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - pca_result (pd.DataFrame): DataFrame with PCA components.\n",
    "            - pca_model (PCA): The fitted PCA model.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- EDA: Performing PCA with {n_components} components ---\")\n",
    "    # Handle potential NaN values by filling them (e.g., with mean or 0)\n",
    "    data_df_filled = data_df.fillna(data_df.mean())\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    principal_components = pca.fit_transform(data_df_filled)\n",
    "    pca_result = pd.DataFrame(data=principal_components,\n",
    "                              columns=[f'PC{i+1}' for i in range(n_components)],\n",
    "                              index=data_df.index)\n",
    "    print(f\"Explained variance ratio by components: {pca.explained_variance_ratio_}\")\n",
    "    print(f\"Cumulative explained variance: {np.sum(pca.explained_variance_ratio_)}\")\n",
    "    return pca_result, pca\n",
    "\n",
    "def plot_pca(pca_result_df, phenotype_df, color_column='_primary_disease', output_path=None):\n",
    "    \"\"\"\n",
    "    Plots the PCA results, colored by a specified phenotype column.\n",
    "\n",
    "    Args:\n",
    "        pca_result_df (pd.DataFrame): DataFrame with PCA components.\n",
    "        phenotype_df (pd.DataFrame): Matched phenotype DataFrame.\n",
    "        color_column (str): The column in phenotype_df to use for coloring the plot.\n",
    "        output_path (str, optional): Path to save the plot. If None, displays the plot.\n",
    "    \"\"\"\n",
    "    if color_column not in phenotype_df.columns:\n",
    "        print(f\"Error: '{color_column}' not found in phenotype DataFrame. Cannot color PCA plot.\")\n",
    "        return\n",
    "\n",
    "    # Merge PCA results with phenotype data for coloring\n",
    "    plot_df = pca_result_df.merge(phenotype_df[[color_column]], left_index=True, right_index=True)\n",
    "\n",
    "    print(f\"\\n--- EDA: Plotting PCA results, colored by '{color_column}' ---\")\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(x='PC1', y='PC2', hue=color_column, data=plot_df,\n",
    "                    palette='tab20', s=70, alpha=0.8, edgecolor='w', linewidth=0.5)\n",
    "    plt.title(f'PCA of Multi-Omics Data (Colored by {color_column.replace(\"_\", \" \").title()})', fontsize=16, weight='bold')\n",
    "    plt.xlabel(f'Principal Component 1', fontsize=12)\n",
    "    plt.ylabel(f'Principal Component 2', fontsize=12)\n",
    "    plt.legend(title=color_column.replace(\"_\", \" \").title(), bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to prevent legend overlap\n",
    "\n",
    "    if output_path:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            print(f\"Created output directory: {output_dir}\")\n",
    "        plt.savefig(output_path, dpi=300)\n",
    "        print(f\"PCA plot saved to: {output_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def perform_umap(data_df, n_components=2, random_state=42):\n",
    "    \"\"\"\n",
    "    Performs UMAP dimensionality reduction on the given data.\n",
    "\n",
    "    Args:\n",
    "        data_df (pd.DataFrame): DataFrame (samples x features).\n",
    "        n_components (int): Number of dimensions for the UMAP embedding.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with UMAP components.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- EDA: Performing UMAP with {n_components} components ---\")\n",
    "    # Handle potential NaN values by filling them (e.g., with mean or 0)\n",
    "    data_df_filled = data_df.fillna(data_df.mean())\n",
    "\n",
    "    reducer = umap.UMAP(n_components=n_components, random_state=random_state)\n",
    "    umap_embedding = reducer.fit_transform(data_df_filled)\n",
    "    umap_result = pd.DataFrame(data=umap_embedding,\n",
    "                               columns=[f'UMAP{i+1}' for i in range(n_components)],\n",
    "                               index=data_df.index)\n",
    "    return umap_result\n",
    "\n",
    "def plot_umap(umap_result_df, phenotype_df, color_column='_primary_disease', output_path=None):\n",
    "    \"\"\"\n",
    "    Plots the UMAP results, colored by a specified phenotype column.\n",
    "\n",
    "    Args:\n",
    "        umap_result_df (pd.DataFrame): DataFrame with UMAP components.\n",
    "        phenotype_df (pd.DataFrame): Matched phenotype DataFrame.\n",
    "        color_column (str): The column in phenotype_df to use for coloring the plot.\n",
    "        output_path (str, optional): Path to save the plot. If None, displays the plot.\n",
    "    \"\"\"\n",
    "    if color_column not in phenotype_df.columns:\n",
    "        print(f\"Error: '{color_column}' not found in phenotype DataFrame. Cannot color UMAP plot.\")\n",
    "        return\n",
    "\n",
    "    # Merge UMAP results with phenotype data for coloring\n",
    "    plot_df = umap_result_df.merge(phenotype_df[[color_column]], left_index=True, right_index=True)\n",
    "\n",
    "    print(f\"\\n--- EDA: Plotting UMAP results, colored by '{color_column}' ---\")\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(x='UMAP1', y='UMAP2', hue=color_column, data=plot_df,\n",
    "                    palette='tab20', s=70, alpha=0.8, edgecolor='w', linewidth=0.5)\n",
    "    plt.title(f'UMAP of Multi-Omics Data (Colored by {color_column.replace(\"_\", \" \").title()})', fontsize=16, weight='bold')\n",
    "    plt.xlabel(f'UMAP Component 1', fontsize=12)\n",
    "    plt.ylabel(f'UMAP Component 2', fontsize=12)\n",
    "    plt.legend(title=color_column.replace(\"_\", \" \").title(), bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "\n",
    "    if output_path:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            print(f\"Created output directory: {output_dir}\")\n",
    "        plt.savefig(output_path, dpi=300)\n",
    "        print(f\"UMAP plot saved to: {output_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def perform_clustering_and_survival_analysis(X_data, y_labels, pheno_df, output_dir, n_clusters=3):\n",
    "    \"\"\"\n",
    "    Performs K-Means clustering on the data and then conducts Kaplan-Meier survival\n",
    "    analysis for the identified clusters.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Performing K-Means Clustering (k={n_clusters}) and Survival Analysis ---\")\n",
    "\n",
    "    # Check for empty numpy array using .size\n",
    "    if X_data.size == 0 or X_data.shape[0] < n_clusters:\n",
    "        print(f\"Insufficient data for clustering with {n_clusters} clusters. Skipping.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10) # n_init for robust centroid initialization\n",
    "        cluster_labels = kmeans.fit_predict(X_data)\n",
    "        print(f\"K-Means clustering complete. Found {n_clusters} clusters.\")\n",
    "\n",
    "        # Add cluster labels to phenotype data\n",
    "        pheno_with_clusters = pheno_df.copy()\n",
    "        # Ensure the index of pheno_with_clusters matches X_data for Series creation\n",
    "        # If X_data is a numpy array, its index is not directly available. Use pheno_df.index.\n",
    "        pheno_with_clusters['Cluster'] = pd.Series(cluster_labels, index=pheno_df.index)\n",
    "\n",
    "\n",
    "        # Prepare survival data\n",
    "        # Common TCGA survival columns are '_OS_TIME' (event indicator) and '_OS_IND' (time in days)\n",
    "        survival_time_col = '_OS_TIME' # Overall Survival Time in days\n",
    "        survival_event_col = '_OS_IND' # Overall Survival Indicator (1=dead, 0=alive/censored)\n",
    "\n",
    "        # Check if the required survival columns exist in the phenotype data\n",
    "        if survival_time_col not in pheno_with_clusters.columns or survival_event_col not in pheno_with_clusters.columns:\n",
    "            print(f\"Survival data columns ('{survival_time_col}', '{survival_event_col}') not found in phenotype data.\")\n",
    "            print(\"Skipping survival analysis. Please ensure your phenotype file contains these columns or update the column names in the script.\")\n",
    "            return\n",
    "\n",
    "        # Convert survival status to boolean (True for event, False for censored)\n",
    "        # Ensure that 'OS_IND' is treated as 1 for event, 0 for censored.\n",
    "        # Sometimes it might be 'Death'/'Alive' or other strings, so convert to int first.\n",
    "        pheno_with_clusters['Event'] = pheno_with_clusters[survival_event_col].astype(int) == 1\n",
    "        pheno_with_clusters['Time'] = pheno_with_clusters[survival_time_col].astype(float)\n",
    "\n",
    "        # Drop rows with NaN in survival data\n",
    "        pheno_with_clusters = pheno_with_clusters.dropna(subset=['Time', 'Event', 'Cluster'])\n",
    "        \n",
    "        if pheno_with_clusters.empty:\n",
    "            print(\"No valid survival data after cleaning. Skipping survival analysis.\")\n",
    "            return\n",
    "\n",
    "        # Kaplan-Meier Plotting\n",
    "        kmf = KaplanMeierFitter()\n",
    "        plt.figure(figsize=(10, 7))\n",
    "\n",
    "        cluster_groups = pheno_with_clusters.groupby('Cluster')\n",
    "        for name, group in cluster_groups:\n",
    "            if len(group) > 1: # Need at least 2 samples to plot a curve\n",
    "                kmf.fit(group['Time'], event_observed=group['Event'], label=f'Cluster {name}')\n",
    "                kmf.plot_survival_function(ax=plt.gca())\n",
    "            else:\n",
    "                print(f\"Cluster {name} has only {len(group)} sample(s), skipping KM plot for this cluster.\")\n",
    "\n",
    "        plt.title(f\"Kaplan-Meier Survival Curves by K-Means Clusters (k={n_clusters})\", fontsize=16, weight='bold', color='darkblue')\n",
    "        plt.xlabel(\"Time (days)\", fontsize=12, color='dimgray')\n",
    "        plt.ylabel(\"Survival Probability\", fontsize=12, color='dimgray')\n",
    "        plt.grid(True, linestyle=':', alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f\"survival_curves_kmeans_k{n_clusters}.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Kaplan-Meier Survival Curves plot saved for {n_clusters} clusters.\")\n",
    "\n",
    "        # Log-rank test for all pairs of clusters\n",
    "        print(\"\\n--- Log-Rank Test Results (P-values between clusters) ---\")\n",
    "        cluster_ids = sorted(pheno_with_clusters['Cluster'].unique())\n",
    "        if len(cluster_ids) >= 2:\n",
    "            for i in range(len(cluster_ids)):\n",
    "                for j in range(i + 1, len(cluster_ids)):\n",
    "                    cluster1_data = pheno_with_clusters[pheno_with_clusters['Cluster'] == cluster_ids[i]]\n",
    "                    cluster2_data = pheno_with_clusters[pheno_with_clusters['Cluster'] == cluster_ids[j]]\n",
    "\n",
    "                    if len(cluster1_data) > 1 and len(cluster2_data) > 1:\n",
    "                        results = logrank_test(cluster1_data['Time'], cluster2_data['Time'],\n",
    "                                               event_observed_A=cluster1_data['Event'],\n",
    "                                               event_observed_B=cluster2_data['Event'])\n",
    "                        print(f\"Cluster {cluster_ids[i]} vs Cluster {cluster_ids[j]}: p-value = {results.p_value:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"Skipping log-rank test for Cluster {cluster_ids[i]} vs Cluster {cluster_ids[j]} due to insufficient samples.\")\n",
    "        else:\n",
    "            print(\"Fewer than 2 clusters with sufficient samples for log-rank test.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during clustering or survival analysis: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Main Execution Block for Multi-Omics Workflow (EDA Focus)\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Multi-Omics Workflow (EDA Focus)...\")\n",
    "\n",
    "    # --- Configuration ---\n",
    "    # IMPORTANT: Update these paths to your actual raw data files\n",
    "    data_dir = r\"C:\\Users\\shrav\\Desktop\\PYTHON\\Cancer\\Pan Cancer Analysis\"\n",
    "    raw_expr_file = os.path.join(data_dir, \"EB++AdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.xena\")\n",
    "    raw_maf_file = os.path.join(data_dir, \"mc3.v0.2.8.PUBLIC.maf\")\n",
    "    raw_pheno_file = os.path.join(data_dir, \"TCGA_phenotype_denseDataOnlyDownload.tsv\")\n",
    "    raw_mart_file = os.path.join(data_dir, \"mart_export.txt\") # BioMart mapping file\n",
    "    raw_mirna_file = os.path.join(data_dir, \"pancanMiRs_EBadjOnProtocolPlatformWithoutRepsWithUnCorrectMiRs_08_04_16.xena\")\n",
    "    raw_cnv_file = os.path.join(data_dir, \"Gistic2_CopyNumber_Gistic2_all_thresholded.by_genes\")\n",
    "    raw_rppa_file = os.path.join(data_dir, \"TCGA-RPPA-pancan-clean.xena\")\n",
    "    raw_meth_file = os.path.join(data_dir, \"jhu-usc.edu_PANCAN_HumanMethylation450.betaValue_whitelisted.tsv.synapse_download_5096262.xena\")\n",
    "    \n",
    "    # Output directories for processed data and plots\n",
    "    output_dir = \"multi_omics_results_advanced\" # This is the main output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Define the target column for cancer type in phenotype data\n",
    "    target_column = '_primary_disease'\n",
    "\n",
    "    # ==========================================================================\n",
    "    # PHASE 1: DATA PREPARATION (Advanced Multi-Omics Loading and Preprocessing)\n",
    "    # ==========================================================================\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EXECUTING PHASE 1: DATA PREPARATION (Advanced Multi-Omics)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    X_multiomics_features = None\n",
    "    y_multiomics_labels = None\n",
    "    pheno_aligned_df = None\n",
    "    expr_original_aligned_processed = None\n",
    "    mirna_original_aligned_processed = None\n",
    "    cnv_original_aligned_processed = None\n",
    "    rppa_original_aligned_processed = None\n",
    "    meth_original_aligned_processed = None\n",
    "\n",
    "    try:\n",
    "        # Check if all raw data files exist\n",
    "        required_files = {\n",
    "            raw_expr_file: \"Gene Expression\",\n",
    "            raw_maf_file: \"Somatic Mutation\",\n",
    "            raw_pheno_file: \"Phenotype\",\n",
    "            raw_mart_file: \"BioMart Mapping\",\n",
    "            raw_mirna_file: \"miRNA Expression\",\n",
    "            raw_cnv_file: \"Copy Number Variation\",\n",
    "            raw_rppa_file: \"RPPA Expression\",\n",
    "            raw_meth_file: \"Methylation Beta Values\"\n",
    "        }\n",
    "        all_files_exist = True\n",
    "        for f_path, f_name in required_files.items():\n",
    "            if not os.path.exists(f_path):\n",
    "                print(f\"Error: Required file not found for {f_name}: {f_path}\")\n",
    "                all_files_exist = False\n",
    "        \n",
    "        if all_files_exist:\n",
    "            (X_multiomics_features, y_multiomics_labels, pheno_aligned_df,\n",
    "             expr_original_aligned_processed, mirna_original_aligned_processed,\n",
    "             cnv_original_aligned_processed, rppa_original_aligned_processed,\n",
    "             meth_original_aligned_processed) = load_and_preprocess_multi_omics_data_advanced(\n",
    "                raw_expr_file, raw_maf_file, raw_pheno_file, raw_mart_file,\n",
    "                raw_mirna_file, raw_cnv_file, raw_rppa_file, raw_meth_file, target_column\n",
    "            )\n",
    "\n",
    "            if X_multiomics_features is not None and not X_multiomics_features.empty:\n",
    "                # Save the processed data for Phase 8\n",
    "                export_data_to_pkl(X_multiomics_features, os.path.join(output_dir, 'X_multiomics_features.pkl'), 'X_multiomics_features')\n",
    "                export_data_to_pkl(y_multiomics_labels, os.path.join(output_dir, 'y_multiomics_labels.pkl'), 'y_multiomics_labels')\n",
    "                export_data_to_pkl(pheno_aligned_df, os.path.join(output_dir, 'pheno_aligned_df.pkl'), 'pheno_aligned_df')\n",
    "                print(\"\\nProcessed multi-omics data saved to .pkl files for subsequent phases.\")\n",
    "            else:\n",
    "                print(\"Multi-omics data processing resulted in empty features. Skipping saving.\")\n",
    "        else:\n",
    "            print(\"\\nSkipping multi-omics data preparation due to missing raw input files.\")\n",
    "            print(\"Please ensure all required raw data files are in the specified 'data_dir'.\")\n",
    "            # Generate dummy data for EDA demonstration if raw files are missing\n",
    "            print(\"\\nGenerating dummy data for EDA demonstration...\")\n",
    "            np.random.seed(42)\n",
    "            num_samples = 500\n",
    "            num_features = 1000 # A reasonable number for multi-omics\n",
    "            \n",
    "            X_multiomics_features = pd.DataFrame(np.random.rand(num_samples, num_features),\n",
    "                                                 index=[f'sample_{i}' for i in range(num_samples)],\n",
    "                                                 columns=[f'feature_{j}' for j in range(num_features)])\n",
    "            \n",
    "            cancer_types = ['BRCA', 'LUAD', 'COAD', 'KIRC', 'LIHC', 'STAD', 'THCA', 'OV', 'LGG', 'SKCM']\n",
    "            y_multiomics_labels = pd.Series(np.random.choice(cancer_types, num_samples),\n",
    "                                            index=X_multiomics_features.index)\n",
    "            \n",
    "            pheno_aligned_df = pd.DataFrame({\n",
    "                '_primary_disease': y_multiomics_labels,\n",
    "                'gender': np.random.choice(['Male', 'Female'], num_samples),\n",
    "                'age_at_diagnosis': np.random.randint(30, 80, num_samples),\n",
    "                '_OS_TIME': np.random.randint(100, 3000, num_samples), # Dummy survival time\n",
    "                '_OS_IND': np.random.choice([0, 1], num_samples, p=[0.7, 0.3]) # Dummy survival indicator\n",
    "            }, index=X_multiomics_features.index)\n",
    "            \n",
    "            print(\"Dummy multi-omics data generated for EDA.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Advanced Data Preparation: {e}\")\n",
    "        print(\"Exiting workflow as data preparation is crucial.\")\n",
    "        exit()\n",
    "\n",
    "    # ==========================================================================\n",
    "    # PHASE 2: EDA & VISUALIZATION\n",
    "    # ==========================================================================\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EXECUTING PHASE 2: EDA & VISUALIZATION\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Check if data was successfully loaded or generated before proceeding with EDA\n",
    "    if X_multiomics_features is not None and not X_multiomics_features.empty and \\\n",
    "       y_multiomics_labels is not None and not y_multiomics_labels.empty and \\\n",
    "       pheno_aligned_df is not None and not pheno_aligned_df.empty:\n",
    "        try:\n",
    "            # --- Extensive EDA and Data Inspection ---\n",
    "            print(\"\\n--- Detailed EDA of Loaded Multi-Omics Data ---\")\n",
    "\n",
    "            print(\"\\nOriginal Multi-omics Feature Matrix (X_multiomics_features):\")\n",
    "            print(f\"Shape: {X_multiomics_features.shape}\")\n",
    "            print(\"First 5 rows:\")\n",
    "            print(X_multiomics_features.head())\n",
    "            print(\"\\nDescriptive statistics for X_multiomics_features (first 50 columns):\")\n",
    "            print(X_multiomics_features.iloc[:, :50].describe()) # Show for a subset of columns\n",
    "            print(\"\\nMissing values in X_multiomics_features:\")\n",
    "            print(X_multiomics_features.isnull().sum().sum())\n",
    "            \n",
    "            print(\"\\nOriginal Target Labels (y_multiomics_labels):\")\n",
    "            print(f\"Shape: {y_multiomics_labels.shape}\")\n",
    "            print(\"Value counts:\")\n",
    "            print(y_multiomics_labels.value_counts().head(10)) # Show top 10 cancer types\n",
    "            \n",
    "            print(\"\\nAligned Phenotype Data (pheno_aligned_df):\")\n",
    "            print(f\"Shape: {pheno_aligned_df.shape}\")\n",
    "            print(\"First 5 rows:\")\n",
    "            print(pheno_aligned_df.head())\n",
    "            print(\"\\nDescriptive statistics for pheno_aligned_df:\")\n",
    "            print(pheno_aligned_df.describe(include='all')) # Include non-numeric columns\n",
    "            print(\"\\nMissing values in pheno_aligned_df:\")\n",
    "            print(pheno_aligned_df.isnull().sum())\n",
    "\n",
    "            # Filter out classes with only one sample before encoding\n",
    "            class_counts = y_multiomics_labels.value_counts()\n",
    "            classes_to_keep = class_counts[class_counts >= 2].index\n",
    "            \n",
    "            if len(classes_to_keep) < 2:\n",
    "                print(\"After filtering, fewer than 2 cancer types remain with at least 2 samples. Skipping further analysis.\")\n",
    "            else:\n",
    "                filtered_samples_mask = y_multiomics_labels.isin(classes_to_keep)\n",
    "                X_filtered = X_multiomics_features[filtered_samples_mask]\n",
    "                y_filtered = y_multiomics_labels[filtered_samples_mask]\n",
    "                pheno_filtered = pheno_aligned_df[filtered_samples_mask] # Keep phenotype aligned for survival analysis\n",
    "\n",
    "                # Remove duplicate columns from X_filtered before analysis\n",
    "                initial_cols_X_filtered = X_filtered.shape[1]\n",
    "                X_filtered = X_filtered.loc[:, ~X_filtered.columns.duplicated()]\n",
    "                if X_filtered.shape[1] < initial_cols_X_filtered:\n",
    "                    print(f\"Removed {initial_cols_X_filtered - X_filtered.shape[1]} duplicate columns from X_filtered.\")\n",
    "\n",
    "                print(\"\\n--- Detailed EDA of Filtered Data (after removing single-sample classes) ---\")\n",
    "                print(f\"Filtered Multi-omics Feature Matrix (X_filtered) shape: {X_filtered.shape}\")\n",
    "                print(\"First 5 rows of X_filtered:\")\n",
    "                print(X_filtered.head())\n",
    "                print(\"\\nDescriptive statistics for X_filtered (first 50 columns):\")\n",
    "                print(X_filtered.iloc[:, :50].describe())\n",
    "\n",
    "                print(f\"\\nFiltered Target Labels (y_filtered) shape: {y_filtered.shape}\")\n",
    "                print(\"Value counts for y_filtered:\")\n",
    "                print(y_filtered.value_counts().head(10))\n",
    "\n",
    "                print(f\"\\nFiltered Phenotype Data (pheno_filtered) shape: {pheno_filtered.shape}\")\n",
    "                print(\"First 5 rows of pheno_filtered:\")\n",
    "                print(pheno_filtered.head())\n",
    "                print(\"\\nDescriptive statistics for pheno_filtered:\")\n",
    "                print(pheno_filtered.describe(include='all'))\n",
    "\n",
    "\n",
    "                le = LabelEncoder()\n",
    "                y_encoded = le.fit_transform(y_filtered)\n",
    "                class_names = le.classes_\n",
    "                print(f\"\\nEncoded {len(class_names)} cancer types for EDA.\")\n",
    "\n",
    "                # --- Dimensionality Reduction: PCA ---\n",
    "                n_components_pca = min(50, X_filtered.shape[1])\n",
    "                if X_filtered.shape[1] > 1 and n_components_pca > 1:\n",
    "                    pca = PCA(n_components=n_components_pca, random_state=42)\n",
    "                    X_pca = pca.fit_transform(X_filtered)\n",
    "                    # Ensure X_pca has an index for consistency with pheno_filtered\n",
    "                    X_pca_df = pd.DataFrame(X_pca, index=X_filtered.index)\n",
    "                    print(f\"\\nPCA reduced data shape: {X_pca.shape}\")\n",
    "                    print(f\"PCA explained variance ratio (first {n_components_pca} components): {np.sum(pca.explained_variance_ratio_):.2f}\")\n",
    "                    print(\"First 5 rows of PCA reduced data (X_pca_df):\")\n",
    "                    print(X_pca_df.head())\n",
    "                else:\n",
    "                    print(\"Not enough features for PCA. Skipping PCA.\")\n",
    "                    X_pca = X_filtered.values # Keep as numpy array if PCA skipped\n",
    "                    X_pca_df = X_filtered # Keep as DataFrame if PCA skipped\n",
    "\n",
    "                # --- Dimensionality Reduction: UMAP ---\n",
    "                if X_pca.shape[1] >= 2: # Use X_pca (numpy array) directly for UMAP\n",
    "                    reducer = umap.UMAP(n_components=2, random_state=42, metric='euclidean')\n",
    "                    X_umap = reducer.fit_transform(X_pca)\n",
    "                    print(f\"\\nUMAP reduced data shape: {X_umap.shape}\")\n",
    "\n",
    "                    plt.figure(figsize=(12, 10))\n",
    "                    sns.scatterplot(x=X_umap[:, 0], y=X_umap[:, 1], hue=y_filtered, palette='tab20',\n",
    "                                    s=80, alpha=0.85, edgecolor='black', linewidth=0.7)\n",
    "                    plt.title(\"UMAP Projection of Multi-Omics Data (Integrated Features)\", fontsize=18, weight='bold', color='darkblue')\n",
    "                    plt.xlabel(\"UMAP Component 1\", fontsize=14, color='dimgray')\n",
    "                    plt.ylabel(\"UMAP Component 2\", fontsize=14, color='dimgray')\n",
    "                    plt.xticks(fontsize=12)\n",
    "                    plt.yticks(fontsize=12)\n",
    "                    plt.legend(title=\"Cancer Type\", bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, fontsize='small', title_fontsize=12)\n",
    "                    plt.grid(True, linestyle=':', alpha=0.5)\n",
    "                    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "                    plt.savefig(os.path.join(output_dir, \"multiomics_umap_projection_advanced.png\"), dpi=300, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "                    print(\"UMAP projection plot saved.\")\n",
    "                else:\n",
    "                    print(\"Not enough dimensions after PCA for UMAP. Skipping UMAP plot.\")\n",
    "\n",
    "                # Perform Clustering and Survival Analysis\n",
    "                # Pass the PCA-reduced data (NumPy array) and original phenotype DataFrame\n",
    "                perform_clustering_and_survival_analysis(X_pca, y_filtered, pheno_filtered, output_dir, n_clusters=3) # You can adjust n_clusters\n",
    "\n",
    "                print(\"\\nEDA and Visualization steps completed.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during EDA & Visualization: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(\"Multi-omics data was not loaded or generated successfully. Skipping EDA & Visualization steps.\")\n",
    "\n",
    "    print(\"\\nMulti-Omics Workflow (EDA Focus) execution complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7f04ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Phase 1.8: Multi-Omics Machine Learning and Advanced Analysis\n",
    "# This script performs dimensionality reduction (PCA, UMAP), trains multiple\n",
    "# machine learning models for cancer type classification, and includes clustering\n",
    "# with survival analysis. It is designed to be run AFTER data preparation (Phase 7).\n",
    "#\n",
    "# Before running:\n",
    "# 1. Ensure you have necessary libraries installed:\n",
    "#    pip install pandas numpy matplotlib seaborn scikit-learn umap-learn lightgbm lifelines\n",
    "# 2. This script EXPLICITLY LOADS 'X_multiomics_features', 'y_multiomics_labels',\n",
    "#    and 'pheno_aligned_df' from the 'multi_omics_results_advanced' directory.\n",
    "#    You MUST run the \"Phase 7: Multi-Omics Data Preparation & EDA\" Canvas first\n",
    "#    to generate and save these files.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b9a9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import umap.umap_ as umap # For UMAP visualization\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC # Support Vector Classifier\n",
    "from sklearn.neural_network import MLPClassifier # Multi-layer Perceptron (simple NN)\n",
    "import lightgbm as lgb # Light Gradient Boosting Machine\n",
    "from sklearn.cluster import KMeans # For clustering\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# For survival analysis\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "\n",
    "# Set a consistent style for all plots for a professional look\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# --- Configuration ---\n",
    "# This output directory should match where the data prep script saves its output\n",
    "output_dir = \"multi_omics_results_advanced\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# Helper Functions\n",
    "# ==============================================================================\n",
    "\n",
    "def perform_clustering_and_survival_analysis(X_data, y_labels, pheno_df, output_dir, n_clusters=3):\n",
    "    \"\"\"\n",
    "    Performs K-Means clustering on the data and then conducts Kaplan-Meier survival\n",
    "    analysis for the identified clusters.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Performing K-Means Clustering (k={n_clusters}) and Survival Analysis ---\")\n",
    "\n",
    "    # Check for empty numpy array using .size\n",
    "    if X_data.size == 0 or X_data.shape[0] < n_clusters:\n",
    "        print(f\"Insufficient data for clustering with {n_clusters} clusters. Skipping.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10) # n_init for robust centroid initialization\n",
    "        cluster_labels = kmeans.fit_predict(X_data)\n",
    "        print(f\"K-Means clustering complete. Found {n_clusters} clusters.\")\n",
    "\n",
    "        # Add cluster labels to phenotype data\n",
    "        pheno_with_clusters = pheno_df.copy()\n",
    "        # Ensure the index of pheno_with_clusters matches X_data for Series creation\n",
    "        # If X_data is a numpy array, its index is not directly available. Use pheno_df.index.\n",
    "        pheno_with_clusters['Cluster'] = pd.Series(cluster_labels, index=pheno_df.index)\n",
    "\n",
    "\n",
    "        # Prepare survival data\n",
    "        # Common TCGA survival columns are '_OS_TIME' (event indicator) and '_OS_IND' (time in days)\n",
    "        survival_time_col = '_OS_TIME' # Overall Survival Time in days\n",
    "        survival_event_col = '_OS_IND' # Overall Survival Indicator (1=dead, 0=alive/censored)\n",
    "\n",
    "        # Check if the required survival columns exist in the phenotype data\n",
    "        if survival_time_col not in pheno_with_clusters.columns or survival_event_col not in pheno_with_clusters.columns:\n",
    "            print(f\"Survival data columns ('{survival_time_col}', '{survival_event_col}') not found in phenotype data.\")\n",
    "            print(\"Skipping survival analysis. Please ensure your phenotype file contains these columns or update the column names in the script.\")\n",
    "            return\n",
    "\n",
    "        # Convert survival status to boolean (True for event, False for censored)\n",
    "        # Ensure that 'OS_IND' is treated as 1 for event, 0 for censored.\n",
    "        # Sometimes it might be 'Death'/'Alive' or other strings, so convert to int first.\n",
    "        pheno_with_clusters['Event'] = pheno_with_clusters[survival_event_col].astype(int) == 1\n",
    "        pheno_with_clusters['Time'] = pheno_with_clusters[survival_time_col].astype(float)\n",
    "\n",
    "        # Drop rows with NaN in survival data\n",
    "        pheno_with_clusters = pheno_with_clusters.dropna(subset=['Time', 'Event', 'Cluster'])\n",
    "        \n",
    "        if pheno_with_clusters.empty:\n",
    "            print(\"No valid survival data after cleaning. Skipping survival analysis.\")\n",
    "            return\n",
    "\n",
    "        # Kaplan-Meier Plotting\n",
    "        kmf = KaplanMeierFitter()\n",
    "        plt.figure(figsize=(10, 7))\n",
    "\n",
    "        cluster_groups = pheno_with_clusters.groupby('Cluster')\n",
    "        for name, group in cluster_groups:\n",
    "            if len(group) > 1: # Need at least 2 samples to plot a curve\n",
    "                kmf.fit(group['Time'], event_observed=group['Event'], label=f'Cluster {name}')\n",
    "                kmf.plot_survival_function(ax=plt.gca())\n",
    "            else:\n",
    "                print(f\"Cluster {name} has only {len(group)} sample(s), skipping KM plot for this cluster.\")\n",
    "\n",
    "        plt.title(f\"Kaplan-Meier Survival Curves by K-Means Clusters (k={n_clusters})\", fontsize=16, weight='bold', color='darkblue')\n",
    "        plt.xlabel(\"Time (days)\", fontsize=12, color='dimgray')\n",
    "        plt.ylabel(\"Survival Probability\", fontsize=12, color='dimgray')\n",
    "        plt.grid(True, linestyle=':', alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f\"survival_curves_kmeans_k{n_clusters}.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Kaplan-Meier Survival Curves plot saved for {n_clusters} clusters.\")\n",
    "\n",
    "        # Log-rank test for all pairs of clusters\n",
    "        print(\"\\n--- Log-Rank Test Results (P-values between clusters) ---\")\n",
    "        cluster_ids = sorted(pheno_with_clusters['Cluster'].unique())\n",
    "        if len(cluster_ids) >= 2:\n",
    "            for i in range(len(cluster_ids)):\n",
    "                for j in range(i + 1, len(cluster_ids)):\n",
    "                    cluster1_data = pheno_with_clusters[pheno_with_clusters['Cluster'] == cluster_ids[i]]\n",
    "                    cluster2_data = pheno_with_clusters[pheno_with_clusters['Cluster'] == cluster_ids[j]]\n",
    "\n",
    "                    if len(cluster1_data) > 1 and len(cluster2_data) > 1:\n",
    "                        results = logrank_test(cluster1_data['Time'], cluster2_data['Time'],\n",
    "                                               event_observed_A=cluster1_data['Event'],\n",
    "                                               event_observed_B=cluster2_data['Event'])\n",
    "                        print(f\"Cluster {cluster_ids[i]} vs Cluster {cluster_ids[j]}: p-value = {results.p_value:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"Skipping log-rank test for Cluster {cluster_ids[i]} vs Cluster {cluster_ids[j]} due to insufficient samples.\")\n",
    "        else:\n",
    "            print(\"Fewer than 2 clusters with sufficient samples for log-rank test.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during clustering or survival analysis: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Main Execution Block for Multi-Omics ML and Advanced Analysis\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Multi-Omics Machine Learning and Advanced Analysis Phase...\")\n",
    "\n",
    "    # Initialize variables to None or empty before the try block\n",
    "    X_multiomics_features = None\n",
    "    y_multiomics_labels = None\n",
    "    pheno_aligned_df = None\n",
    "    data_loaded_successfully = False\n",
    "\n",
    "    # --- Load preprocessed data from files ---\n",
    "    # These files are expected to be generated by the \"Phase 7: Data Prep\" Canvas.\n",
    "    try:\n",
    "        print(f\"Attempting to load preprocessed data from '{output_dir}'...\")\n",
    "        X_multiomics_features = pd.read_pickle(os.path.join(output_dir, 'X_multiomics_features.pkl'))\n",
    "        y_multiomics_labels = pd.read_pickle(os.path.join(output_dir, 'y_multiomics_labels.pkl'))\n",
    "        pheno_aligned_df = pd.read_pickle(os.path.join(output_dir, 'pheno_aligned_df.pkl'))\n",
    "        print(\"Preprocessed data loaded successfully.\")\n",
    "        data_loaded_successfully = True\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data: {e}. One or more required .pkl files were not found.\")\n",
    "        print(f\"Please ensure you have run the 'Phase 7: Multi-Omics Data Preparation & EDA' Canvas first to generate these files in '{output_dir}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during data loading: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # Proceed with analysis only if data was successfully loaded\n",
    "    if data_loaded_successfully and \\\n",
    "       X_multiomics_features is not None and not X_multiomics_features.empty and \\\n",
    "       y_multiomics_labels is not None and not y_multiomics_labels.empty and \\\n",
    "       pheno_aligned_df is not None and not pheno_aligned_df.empty:\n",
    "\n",
    "        # Filter out classes with only one sample before encoding and splitting\n",
    "        class_counts = y_multiomics_labels.value_counts()\n",
    "        classes_to_keep = class_counts[class_counts >= 2].index\n",
    "        \n",
    "        if len(classes_to_keep) < 2:\n",
    "            print(\"After filtering, fewer than 2 cancer types remain with at least 2 samples. Skipping ML models.\")\n",
    "        else:\n",
    "            filtered_samples_mask = y_multiomics_labels.isin(classes_to_keep)\n",
    "            X_filtered = X_multiomics_features[filtered_samples_mask]\n",
    "            y_filtered = y_multiomics_labels[filtered_samples_mask]\n",
    "            pheno_filtered = pheno_aligned_df[filtered_samples_mask] # Keep phenotype aligned for survival analysis\n",
    "\n",
    "            # Remove duplicate columns from X_filtered before training\n",
    "            initial_cols_X_filtered = X_filtered.shape[1]\n",
    "            X_filtered = X_filtered.loc[:, ~X_filtered.columns.duplicated()]\n",
    "            if X_filtered.shape[1] < initial_cols_X_filtered:\n",
    "                print(f\"Removed {initial_cols_X_filtered - X_filtered.shape[1]} duplicate columns from X_filtered.\")\n",
    "\n",
    "\n",
    "            print(f\"Original samples: {len(y_multiomics_labels)}, Samples after filtering single-member classes: {len(y_filtered)}\")\n",
    "            print(f\"Original unique classes: {y_filtered.nunique()}, Classes after filtering: {y_filtered.nunique()}\")\n",
    "\n",
    "            le = LabelEncoder()\n",
    "            y_encoded = le.fit_transform(y_filtered)\n",
    "            class_names = le.classes_\n",
    "            print(f\"Encoded {len(class_names)} cancer types for ML.\")\n",
    "\n",
    "            # --- Dimensionality Reduction: PCA ---\n",
    "            n_components_pca = min(50, X_filtered.shape[1])\n",
    "            if X_filtered.shape[1] > 1 and n_components_pca > 1:\n",
    "                pca = PCA(n_components=n_components_pca, random_state=42)\n",
    "                X_pca = pca.fit_transform(X_filtered)\n",
    "                # Ensure X_pca has an index for consistency with pheno_filtered\n",
    "                X_pca_df = pd.DataFrame(X_pca, index=X_filtered.index)\n",
    "                print(f\"\\nPCA reduced data shape: {X_pca.shape}\")\n",
    "                print(f\"PCA explained variance ratio (first {n_components_pca} components): {np.sum(pca.explained_variance_ratio_):.2f}\")\n",
    "            else:\n",
    "                print(\"Not enough features for PCA. Skipping PCA.\")\n",
    "                X_pca = X_filtered.values # Keep as numpy array if PCA skipped\n",
    "                X_pca_df = X_filtered # Keep as DataFrame if PCA skipped\n",
    "\n",
    "            # --- Dimensionality Reduction: UMAP ---\n",
    "            if X_pca.shape[1] >= 2: # Use X_pca (numpy array) directly for UMAP\n",
    "                reducer = umap.UMAP(n_components=2, random_state=42, metric='euclidean')\n",
    "                X_umap = reducer.fit_transform(X_pca)\n",
    "                print(f\"\\nUMAP reduced data shape: {X_umap.shape}\")\n",
    "\n",
    "                plt.figure(figsize=(12, 10))\n",
    "                sns.scatterplot(x=X_umap[:, 0], y=X_umap[:, 1], hue=y_filtered, palette='tab20',\n",
    "                                s=80, alpha=0.85, edgecolor='black', linewidth=0.7)\n",
    "                plt.title(\"UMAP Projection of Multi-Omics Data (Integrated Features)\", fontsize=18, weight='bold', color='darkblue')\n",
    "                plt.xlabel(\"UMAP Component 1\", fontsize=14, color='dimgray')\n",
    "                plt.ylabel(\"UMAP Component 2\", fontsize=14, color='dimgray')\n",
    "                plt.xticks(fontsize=12)\n",
    "                plt.yticks(fontsize=12)\n",
    "                plt.legend(title=\"Cancer Type\", bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, fontsize='small', title_fontsize=12)\n",
    "                plt.grid(True, linestyle=':', alpha=0.5)\n",
    "                plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "                plt.savefig(os.path.join(output_dir, \"multiomics_umap_projection_advanced.png\"), dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                print(\"UMAP projection plot saved.\")\n",
    "            else:\n",
    "                print(\"Not enough dimensions after PCA for UMAP. Skipping UMAP plot.\")\n",
    "\n",
    "            # Perform Clustering and Survival Analysis\n",
    "            # Pass the PCA-reduced data (NumPy array) and original phenotype DataFrame\n",
    "            perform_clustering_and_survival_analysis(X_pca, y_filtered, pheno_filtered, output_dir, n_clusters=3) # You can adjust n_clusters\n",
    "\n",
    "\n",
    "            # --- Machine Learning: Train and Evaluate Multiple Classifiers ---\n",
    "            if X_filtered.shape[0] > 1 and X_filtered.shape[1] > 0 and len(np.unique(y_encoded)) > 1:\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X_filtered, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
    "                )\n",
    "                print(f\"\\nTraining data shape: {X_train.shape}, Test data shape: {X_test.shape}\")\n",
    "\n",
    "                classifiers = {\n",
    "                    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1),\n",
    "                    \"Support Vector Machine (SVC)\": SVC(random_state=42, probability=True),\n",
    "                    \"LightGBM Classifier\": lgb.LGBMClassifier(random_state=42, n_jobs=-1),\n",
    "                    \"MLP Classifier\": MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42, verbose=True)\n",
    "                }\n",
    "\n",
    "                for name, clf in classifiers.items():\n",
    "                    print(f\"\\n--- Training {name} ---\")\n",
    "                    try:\n",
    "                        clf.fit(X_train, y_train)\n",
    "                        y_pred = clf.predict(X_test)\n",
    "\n",
    "                        # Get unique labels present in y_test and y_pred\n",
    "                        unique_labels_in_test_pred = np.unique(np.concatenate((y_test, y_pred)))\n",
    "                        # Filter class_names to match the unique labels present\n",
    "                        display_class_names = [class_names[i] for i in unique_labels_in_test_pred]\n",
    "\n",
    "                        print(f\"📊 Classification Report for {name}:\")\n",
    "                        print(classification_report(y_test, y_pred, labels=unique_labels_in_test_pred, target_names=display_class_names, zero_division=0))\n",
    "\n",
    "                        cm = confusion_matrix(y_test, y_pred, normalize='true', labels=unique_labels_in_test_pred)\n",
    "                        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_class_names)\n",
    "\n",
    "                        fig, ax = plt.subplots(figsize=(16, 16))\n",
    "                        disp.plot(cmap='Blues', ax=ax, colorbar=False, xticks_rotation='vertical')\n",
    "                        plt.title(f\"Normalized Confusion Matrix ({name})\", fontsize=18, weight='bold', color='darkblue')\n",
    "                        plt.xlabel(\"Predicted Label\", fontsize=14, color='dimgray')\n",
    "                        plt.ylabel(\"True Label\", fontsize=14, color='dimgray')\n",
    "                        plt.xticks(fontsize=10)\n",
    "                        plt.yticks(fontsize=10)\n",
    "                        plt.tight_layout()\n",
    "                        plt.savefig(os.path.join(output_dir, f\"multiomics_confusion_matrix_{name.replace(' ', '_').replace('(', '').replace(')', '')}.png\"), dpi=300, bbox_inches='tight')\n",
    "                        plt.close()\n",
    "                        print(f\"Normalized Confusion Matrix plot for {name} saved.\")\n",
    "\n",
    "                        if name == \"MLP Classifier\" and hasattr(clf, 'loss_curve_'):\n",
    "                            plt.figure(figsize=(10, 6))\n",
    "                            plt.plot(clf.loss_curve_)\n",
    "                            plt.title(\"MLP Classifier Training Loss Curve\", fontsize=18, weight='bold', color='darkblue')\n",
    "                            plt.xlabel(\"Iteration\", fontsize=14, color='dimgray')\n",
    "                            plt.ylabel(\"Loss\", fontsize=14, color='dimgray')\n",
    "                            plt.grid(True, linestyle='--', alpha=0.6)\n",
    "                            plt.tight_layout()\n",
    "                            plt.savefig(os.path.join(output_dir, \"mlp_classifier_loss_curve.png\"), dpi=300, bbox_inches='tight')\n",
    "                            plt.close()\n",
    "                            print(\"MLP Classifier loss curve plot saved.\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error training or evaluating {name}: {e}\")\n",
    "                        import traceback\n",
    "                        traceback.print_exc()\n",
    "            else:\n",
    "                print(\"Insufficient data or classes for Machine Learning. Skipping model training and evaluation.\")\n",
    "    else:\n",
    "        print(\"Multi-omics data was not loaded successfully. Skipping ML analysis steps.\")\n",
    "\n",
    "    print(\"\\nMulti-Omics Machine Learning & Advanced Analysis Phase complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
